[{"title":"JVM导出dump分析","path":"/posts/d23c492a/","content":"获取 JVM 的 dump 的两种方式。 通过配置 JVM 参数自动导出在 JVM 启动参数加增加两个参数 1234#出现 OOME 时生成堆 dump: -XX:+HeapDumpOnOutOfMemoryError#生成堆文件地址：-XX:HeapDumpPath=/opt/jvmlogs/ -XX:HeapDumpPath 是指定 dump 输出的目录 手动通过 Jmap 命令导出第二种是在程序可能发生异常时手动导出 1jmap -dump:format=b,file=/var/logs/heap.hprof $PID format&#x3D;b 表示输出二进制格式 file 指定输出的文件目录 PID 表示指定 JVM 进程的 PID 第一种方式是需要等待当前JVM出现问题后才能生成dmp文件，实时性不高，第二种方式在执行时，JVM是暂停服务的，所以对线上的运行会产生影响，所以建议第一种方式。 dump 分析dump 的具体分析可能使用工具进行，比如Jprofiler 或者 Eclipse 的 Memory Analyzer Tool 工具，包括 IDEA 中打开 dump 文件也能展示分析Eclipse Memory Analyzer Tool 展示效果 IDEA 展示效果 超大dump分析一般导出的dump文件都比较大，在本地分析可能受限于自身电脑性能与内存的原因无法完成。比如dump文件可能有5、6G，也可能有20、30G的大小。这种情况下我们可以直接在Linux服务器上分析，需要使用Memory Analyzer Tools这个工具。下载地址：https://www.eclipse.org/mat/downloads.php历史版本下载地址：https://eclipse.dev/mat/previousReleases.php下载的时候需要看下自己Linux的版本，下载匹配的版本即可。 12[root@KSSHUAT01450 /data]# uname -mx86_64 另外就是需要下载与自己JDK匹配的版本，否则启动报错： 1234567891011121314151617181920212223242526[root@KSSHUAT01450 /data/mat]# ./ParseHeapDump.sh heap.hprof org.eclipse.mat.api:suspects org.eclipse.mat.api:overview org.eclipse.mat.api:top_componentsUnrecognized option: --add-exports=java.base/jdk.internal.org.objectweb.asm=ALL-UNNAMEDError: Could not create the Java Virtual Machine.Error: A fatal exception has occurred. Program will exit.MemoryAnalyzer:JVM terminated. Exit code=1/opt/jdk8//bin/java--add-exports=java.base/jdk.internal.org.objectweb.asm=ALL-UNNAMED-Xmx2048m-jar /data/mat//plugins/org.eclipse.equinox.launcher_1.6.600.v20231106-1826.jar-os linux-ws gtk-arch x86_64-launcher /data/mat/MemoryAnalyzer-name MemoryAnalyzer--launcher.library /data/mat/plugins/org.eclipse.equinox.launcher.gtk.linux.x86_64_1.2.800.v20231003-1442/eclipse_11802.so-startup /data/mat/plugins/org.eclipse.equinox.launcher_1.6.600.v20231106-1826.jar--launcher.overrideVmargs-exitdata 3-consolelog-application org.eclipse.mat.api.parse heap.hprof org.eclipse.mat.api:suspects org.eclipse.mat.api:overview org.eclipse.mat.api:top_components-vm /opt/jdk8//bin/java-vmargs--add-exports=java.base/jdk.internal.org.objectweb.asm=ALL-UNNAMED-Xmx2048m-jar /data/mat/plugins/org.eclipse.equinox.launcher_1.6.600.v20231106-1826.jar 如果是JDK8建议去历史版本中下载Memory Analyzer 1.10.0 Release版本，测试可以正常启动。下载下来是一个Zip压缩包，直接上传到Linux服务器上进行解压。 1[root@KSSHUAT01450 /data]# unzip MemoryAnalyzer-1.10.0.20200225-linux.gtk.x86_64.zip 解压完成会得到一个mat目录，在该目录下执行命令 1[root@KSSHUAT01450 /data]# ./ParseHeapDump.sh heap.hprof org.eclipse.mat.api:suspects org.eclipse.mat.api:overview org.eclipse.mat.api:top_components 命令中的heap.hprof 则是我们自己的dump文件，我是把它移动到了Mat目录当中。如果内存不足可能会报错：java.lang.OutOfMemoryError: Java heap space需要调整一下Mat工具的堆内存配置，编辑mat目录下的 MemoryAnalyzer.ini ，调整 -Xmx的参数即可 12345678[root@KSSHUAT01450 /data]# vim MemoryAnalyzer.ini -startupplugins/org.eclipse.equinox.launcher_1.5.0.v20180512-1130.jar--launcher.libraryplugins/org.eclipse.equinox.launcher.gtk.linux.x86_64_1.1.700.v20180518-1200-vmargs-Xmx10240m~","tags":["JVM"],"categories":["Java"]},{"title":"排查CPU过高两种方法","path":"/posts/1e92dd4a/","content":"使用 Jstack 命令先使用 Top 找出占用 CPU 最高的 Java 进程通过 top 命令可以清晰的看到占用 CPU 的进程信息，占用最高的 Java 进程的 PID 是 5232 再通过 Top 命令找出占用 Cpu 最高的线程上一个我们已经查到了占用 CPU 最高的进程信息以及它的 PID，接着我们就可以使用 top -Hp PID 命令从进程中找到占用 CPU 最高的线程是哪个具体命令： 1&gt;top -Hp 5232 展示界面与上面差不多，但前面的 ID 是线程的 ID，我们记下占用 CPU 最高的线程 ID 号 使用 Jstack 命令导出线程快照接着我们使用 Jstack PID将进程的线程栈导出来，命令如： 1&gt;jstack 5232 &gt; thread_jstack.log :::info注意这里的 5232 是我们第一步中查询出来的进程 PID，而不是第二步中的线程 ID，后面的 thread_jstack.log 是导出的文件中，可以指定目录与其它名称::: 查询线程栈日志最后我们就可以查询占用 CPU 最高的线程的线程栈日志了。首先把 top -Hp PID 命令中查询出来的占用 CPU 最高的线程栈 ID 转为十六进制数然后使用这个十六进制数在 thread_jstack.log日志中搜索即可，会匹配到日志中 nid 这里的值。日志中会详细的把线程名称、状态以及对应的代码打印出来，这样就能够知道是哪块的代码占用了大量 CPU，从而有针对性的排查问题了 使用 Arthas 工具使用 Arthas 工具会比使用 Jstack 命令更加方便快捷，主要是使用 Thread命令，官网有详细的文档Arthas下面从官网文档中摘抄最常用的两个命令记录下 一键展示当前最忙的 N 个线程并打印堆栈1234567891011121314151617181920212223[arthas@27764]$ thread -n 3&quot;C2 CompilerThread1&quot; [Internal] cpuUsage=95.87% deltaTime=257ms time=47762ms&quot;http-nio-8412-exec-7&quot; Id=88 cpuUsage=59.46% deltaTime=159ms time=17460ms TIMED_WAITING at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) at java.net.SocketInputStream.read(SocketInputStream.java:170) at java.net.SocketInputStream.read(SocketInputStream.java:141) at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101) at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144) at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174) at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3011) at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3472) at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3462) at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3905) at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530) //省略详细堆栈日志&quot;C2 CompilerThread6&quot; [Internal] cpuUsage=42.31% deltaTime=113ms time=53087ms thread -b, 找出当前阻塞其他线程的线程有时候我们发现应用卡住了， 通常是由于某个线程拿住了某个锁， 并且其他线程都在等待这把锁造成的。 为了排查这类问题， arthas 提供了thread -b， 一键找出那个罪魁祸首。 123456789101112131415161718192021222324252627282930313233343536$ thread -b&quot;http-bio-8080-exec-4&quot; Id=27 TIMED_WAITING at java.lang.Thread.sleep(Native Method) at test.arthas.TestThreadBlocking.doGet(TestThreadBlocking.java:22) - locked java.lang.Object@725be470 &lt;---- but blocks 4 other threads! at javax.servlet.http.HttpServlet.service(HttpServlet.java:624) at javax.servlet.http.HttpServlet.service(HttpServlet.java:731) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) at test.filter.TestDurexFilter.doFilter(TestDurexFilter.java:46) at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241) at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208) at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220) at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122) at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:505) at com.taobao.tomcat.valves.ContextLoadFilterValve$FilterChainAdapter.doFilter(ContextLoadFilterValve.java:191) at com.taobao.eagleeye.EagleEyeFilter.doFilter(EagleEyeFilter.java:81) at com.taobao.tomcat.valves.ContextLoadFilterValve.invoke(ContextLoadFilterValve.java:150) at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:170) at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103) at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116) at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:429) at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1085) at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:625) at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:318) - locked org.apache.tomcat.util.net.SocketWrapper@7127ee12 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:745) Number of locked synchronizers = 1 - java.util.concurrent.ThreadPoolExecutor$Worker@31a6493e :::warning注意：目前只支持找出 synchronized 关键字阻塞住的线程， 如果是java.util.concurrent.Lock， 目前还不支持。:::","tags":["JVM"],"categories":["Java"]},{"title":"Mysql 索引失效场景","path":"/posts/33cb7ec2/","content":"索引在哪些场景下会失效，通过测试来验证一下。新建一张表，初始化了 30 万条数据进去。 123456789CREATE TABLE employees ( emp_no INT NOT NULL, birth_date DATE NOT NULL, first_name VARCHAR(14) NOT NULL, last_name VARCHAR(16) NOT NULL, gender ENUM (&#x27;M&#x27;,&#x27;F&#x27;) NOT NULL, hire_date DATE NOT NULL, PRIMARY KEY (emp_no)); 首先我们给这张表添加一个索引，就使用 first_name 字段好了 1ALTER TABLE employees ADD INDEX employees_n1 ( first_name ); 当我们使用 first_name 字段去查询时，索引是生效的 12345678mysql&gt; EXPLAIN SELECT * FROM employees WHERE first_name = &#x27;Mary&#x27;;+----+-------------+-----------+------------+------+---------------+--------------+---------+-------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+------+---------------+--------------+---------+-------+------+----------+-------+| 1 | SIMPLE | employees | NULL | ref | employees_n1 | employees_n1 | 16 | const | 224 | 100.00 | NULL |+----+-------------+-----------+------------+------+---------------+--------------+---------+-------+------+----------+-------+1 row in set, 1 warning (0.00 sec) 那在什么场景下才会失效呢？ 以下是可能导致索引失效的原因： 隐式或显式类型转换first_name 是一个 Varchar 类型的字段，我们使用的值 123 是整数类型，Mysql 会尝试将 first_name 字段中的值转成数字类型，从而导致该索引字段失效 123456mysql&gt; EXPLAIN SELECT * FROM employees WHERE first_name = 123;+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+| 1 | SIMPLE | employees | NULL | ALL | employees_n1 | NULL | NULL | NULL | 299556 | 10.00 | Using where |+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+ 对索引字段进行运算下面例子中对 first_name 字段进行了运算导致索引失效 123456mysql&gt; EXPLAIN SELECT * FROM employees WHERE LEFT(first_name,1) = &#x27;Mary&#x27;;+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+| 1 | SIMPLE | employees | NULL | ALL | NULL | NULL | NULL | NULL | 299556 | 100.00 | Using where |+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+ LIKE 左模糊匹配下面例子中分别对索引字段 first_name 使用 Like 进行匹配，可以发现当出现左模糊匹配时，索引失效。 1234567891011121314151617181920212223mysql&gt; EXPLAIN SELECT * FROM employees WHERE first_name LIKE &#x27;%Mary%&#x27;;+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+| 1 | SIMPLE | employees | NULL | ALL | NULL | NULL | NULL | NULL | 299556 | 11.11 | Using where |+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+1 row in set, 1 warning (0.00 sec)mysql&gt; EXPLAIN SELECT * FROM employees WHERE first_name LIKE &#x27;%Mary&#x27;;+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+| 1 | SIMPLE | employees | NULL | ALL | NULL | NULL | NULL | NULL | 299556 | 11.11 | Using where |+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+1 row in set, 1 warning (0.00 sec)mysql&gt; EXPLAIN SELECT * FROM employees WHERE first_name LIKE &#x27;Mary%&#x27;;+----+-------------+-----------+------------+-------+---------------+--------------+---------+------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+-------+---------------+--------------+---------+------+------+----------+-----------------------+| 1 | SIMPLE | employees | NULL | range | employees_n1 | employees_n1 | 16 | NULL | 224 | 100.00 | Using index condition |+----+-------------+-----------+------------+-------+---------------+--------------+---------+------+------+----------+-----------------------+ OR 连接非索引字段使用索引字段查询的同时使用 OR 连接一个非索引字段时，会导致该索引字段失效。但如果改成使用 AND 则不影响 1234567891011121314mysql&gt; EXPLAIN SELECT * FROM employees WHERE first_name = &#x27;Mary&#x27; OR last_name = &#x27;Sluis&#x27;;+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+| 1 | SIMPLE | employees | NULL | ALL | employees_n1 | NULL | NULL | NULL | 299556 | 10.07 | Using where |+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+mysql&gt; EXPLAIN SELECT * FROM employees WHERE first_name = &#x27;Mary&#x27; AND last_name = &#x27;Sluis&#x27;;+----+-------------+-----------+------------+------+---------------+--------------+---------+-------+------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+------+---------------+--------------+---------+-------+------+----------+-------------+| 1 | SIMPLE | employees | NULL | ref | employees_n1 | employees_n1 | 16 | const | 224 | 10.00 | Using where |+----+-------------+-----------+------------+------+---------------+--------------+---------+-------+------+----------+-------------+ 最左匹配原则在使用组合索引时，条件中必须出现组合索引中的第一个字段，该索引才会生效下面我们删除原来的索引，新建一个组合索引 12345678mysql&gt; ALTER TABLE employees DROP INDEX employees_n1;Query OK, 0 rows affected (0.01 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; ALTER TABLE employees ADD INDEX employees_n1 ( first_name, last_name, gender );Query OK, 0 rows affected (1.35 sec)Records: 0 Duplicates: 0 Warnings: 0 下面的测试语句中：第一条语句的条件包含了三个字段，分别都是组合索引中的字段，因此索引是生效的。 1234567mysql&gt; EXPLAIN SELECT * FROM employees WHERE first_name = &#x27;Mary&#x27; AND last_name = &#x27;Sluis&#x27; AND gender = &#x27;M&#x27;;+----+-------------+-----------+------------+------+---------------+--------------+---------+-------------------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+------+---------------+--------------+---------+-------------------+------+----------+-------+| 1 | SIMPLE | employees | NULL | ref | employees_n1 | employees_n1 | 35 | const,const,const | 1 | 100.00 | NULL |+----+-------------+-----------+------------+------+---------------+--------------+---------+-------------------+------+----------+-------+1 row in set, 1 warning (0.00 sec) 第二条语句中，把组合索引中的第一个字段去除了，此时索引没有生效。 12345678mysql&gt; EXPLAIN SELECT * FROM employees WHERE last_name = &#x27;Sluis&#x27; AND gender = &#x27;M&#x27;;+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+| 1 | SIMPLE | employees | NULL | ALL | NULL | NULL | NULL | NULL | 299556 | 5.00 | Using where |+----+-------------+-----------+------------+------+---------------+------+---------+------+--------+----------+-------------+ 第三条语句中，把条件的位置更换了，没有按组合索引中的字段顺序来查询，此时索引也是生效的。 12345678mysql&gt; EXPLAIN SELECT * FROM employees WHERE last_name = &#x27;Sluis&#x27; AND first_name = &#x27;Mary&#x27; AND gender = &#x27;M&#x27;;+----+-------------+-----------+------------+------+---------------+--------------+---------+-------------------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+------+---------------+--------------+---------+-------------------+------+----------+-------+| 1 | SIMPLE | employees | NULL | ref | employees_n1 | employees_n1 | 35 | const,const,const | 1 | 100.00 | NULL |+----+-------------+-----------+------------+------+---------------+--------------+---------+-------------------+------+----------+-------+ 第四条语句中，把组合索引中间的字段去除，只加了 first_name 与 gender 两个条件，虽然看起来也走了索引，但实际上 gender 条件索引是失效的，只有 first_name 字段进行了索引 1234567mysql&gt; EXPLAIN SELECT * FROM employees WHERE first_name = &#x27;Mary&#x27; AND gender = &#x27;M&#x27;;+----+-------------+-----------+------------+------+---------------+--------------+---------+-------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+------+---------------+--------------+---------+-------+------+----------+-----------------------+| 1 | SIMPLE | employees | NULL | ref | employees_n1 | employees_n1 | 16 | const | 224 | 50.00 | Using index condition |+----+-------------+-----------+------------+------+---------------+--------------+---------+-------+------+----------+-----------------------+ 因此组合索引优化可以考虑： 最好包含所有索引字段 条件中包含索引中的第一个字段 尽量不要跳过中间的字段 比较运算后面的索引失效为了方便进行比较运算，重新建一下索引 1234567mysql&gt; ALTER TABLE employees DROP INDEX employees_n1;Query OK, 0 rows affected (0.01 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; ALTER TABLE employees ADD INDEX employees_n1 ( first_name, birth_date, last_name );Query OK, 0 rows affected (1.26 sec)Records: 0 Duplicates: 0 Warnings: 0 下面例子中：第一条语句是三个条件都走了索引第二个语句把中间字段使用了大小比较运算符，导致后面的字段索引失效。 1234567891011121314mysql&gt; EXPLAIN SELECT * FROM employees WHERE first_name = &#x27;Mary&#x27; AND birth_date = &#x27;1958-02-26&#x27; AND last_name = &#x27;Sluis&#x27;;+----+-------------+-----------+------------+------+---------------+--------------+---------+-------------------+------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+------+---------------+--------------+---------+-------------------+------+----------+-------+| 1 | SIMPLE | employees | NULL | ref | employees_n1 | employees_n1 | 37 | const,const,const | 1 | 100.00 | NULL |+----+-------------+-----------+------------+------+---------------+--------------+---------+-------------------+------+----------+-------+1 row in set, 1 warning (0.00 sec)mysql&gt; EXPLAIN SELECT * FROM employees WHERE first_name = &#x27;Mary&#x27; AND birth_date &gt; &#x27;1958-02-26&#x27; AND last_name = &#x27;Sluis&#x27;;+----+-------------+-----------+------------+-------+---------------+--------------+---------+------+------+----------+-----------------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-----------+------------+-------+---------------+--------------+---------+------+------+----------+-----------------------+| 1 | SIMPLE | employees | NULL | range | employees_n1 | employees_n1 | 19 | NULL | 111 | 10.00 | Using index condition |+----+-------------+-----------+------------+-------+---------------+--------------+---------+------+------+----------+-----------------------+","tags":["Mysql"],"categories":["Java"]},{"title":"Spring事务监听器","path":"/posts/b5979740/","content":"有时我们需要在某一方法事务提交后再执行一些操作，这些操作可能涉及到提交的数据，如要查询上一步提交后的数据这种 情况，这时我们希望能实现在上一步事务提交后再执行后面的操作。Spring 中可以使用 ApplicationEvent 事件监听来实现事务监听。 新建事件类12345678910111213141516/**● @Description 待办任务状态推送事件 ● @Date 2021/01/05 15:41 ● @Author jiangwei.huang@hand-china.com*/public class TodoPushEvent extends ApplicationEvent &#123; private List processTaskInfos; public TodoPushEvent(Object source, List processTaskInfos) &#123; super(source); this.processTaskInfos = processTaskInfos; &#125; public List getProcessTaskInfos() &#123; return processTaskInfos; &#125;&#125; 新建事件监听器12345678910111213141516171819202122232425262728293031/**● @Description 待办任务状态推送监听器 ● @Date 2021/01/05 15:42 ● @Author jiangwei.huang@hand-china.com*/@Component @Slf4j public class TodoPushListener &#123; @Autowired private IYnjtTaskSendService iYnjtTaskSendService; private static final Logger LOGGER = LoggerFactory.getLogger(TodoPushListener.class); @TransactionalEventListener(phase = TransactionPhase.AFTER_COMMIT) public void onApplicationEvent(TodoPushEvent todoPushEvent)&#123; List processTaskInfos = todoPushEvent.getProcessTaskInfos(); Task taskEntity = (Task) todoPushEvent.getSource(); //排除当前待办任务 List taskInfos = processTaskInfos.stream().filter(processTaskInfo -&gt; !StringUtils.equals(processTaskInfo.getId(),taskEntity.getId()) ) .collect(Collectors.toList()); //推送当前已完成的待办 YnjtTaskDTO ynjtTaskDTO = new YnjtTaskDTO().setTaskId(Long.parseLong(taskEntity.getId())).setProcInstId(Long.parseLong(taskEntity.getProcessInstanceId())).setAction(&quot;COMPLETE&quot;); iYnjtTaskSendService.sendApprovedTask(Collections.singletonList(ynjtTaskDTO)); try &#123; //推送待办 iYnjtTaskSendService.sendTasks(null, Long.parseLong(taskEntity.getTenantId()), taskEntity.getProcessInstanceId(), &quot;P&quot;, taskInfos); &#125;catch (Exception e)&#123; LOGGER.error(taskEntity.getProcessInstanceId() + &quot;审批后生成下一节点的待办推送失败：&quot; + e.getMessage()); &#125; &#125; &#125; onApplicationEvent方法中的参数为上面新建的事件类。 加上 @TransactionalEventListener 注解，TransactionPhase.AFTER_COMMIT 表示在上个事务提交之前再执行。 事件调用在其它事务方法中调用即可，可实现当前方法事务提前后再 执行该事件监听的执行内容。 因此调用的方法必须是有事务的，即使用@Transactional进行注解了，这样事务监听才会生效 12//当前事务提交后推送待办状态到IAMapplicationEventPublisher.publishEvent(new TodoPushEvent(taskEntity,processTaskInfos));","tags":["Spring"],"categories":["Spring"]},{"title":"SpringBoot 日志格式优化","path":"/posts/be2345fe/","content":"Spring Boot默认集成的日志打印格式中没有行号，但有时排查问题希望能直接定位到行号。我们可以将默认的格式进行更改。 默认格式配置在这个路径下： 1/org/springframework/boot/spring-boot/2.0.6.RELEASE/spring-boot-2.0.6.RELEASE.jar!/org/springframework/boot/logging/logback/defaults.xml 更改只需要改application.yml文件即可。 12345678910logging: level: org.hzero.workflow: $&#123;LOG_LEVEL:debug&#125; org.activiti: $&#123;LOG_LEVEL:debug&#125; org.apache.servicecomb: debug org.srm.workflow: $&#123;LOG_LEVEL:debug&#125; pattern: console: &quot;%clr(%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;)&#123;faint&#125; %clr($&#123;LOG_LEVEL_PATTERN:-%5p&#125;) %clr($&#123;PID:- &#125;)&#123;magenta&#125; %clr(---)&#123;faint&#125; %clr([%t])&#123;faint&#125; %clr(%-40.40logger&#123;39&#125;)&#123;cyan&#125;[lineno:%line] %clr(:)&#123;faint&#125; %m%n$&#123;LOG_EXCEPTION_CONVERSION_WORD:%wEx&#125;&quot; file: &quot;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; $&#123;LOG_LEVEL_PATTERN:-%5p&#125; $&#123;PID:- &#125; --- [%t] %-40.40logger&#123;39&#125;[lineno:%line]: %m%n$&#123;LOG_EXCEPTION_CONVERSION_WORD:%wEx&#125;&quot; 我们只需要在默认的基础上，在适当位置加上 %line 即可，效果如下: ![image-20211008110041558](&#x2F;Users&#x2F;hjw&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20211008110041558.png)","tags":["logback"],"categories":["日志管理"]},{"title":"Spring Cloud重试机制导致接口多次调用问题","path":"/posts/c501b3d2/","content":"最近在项目上遇到一个奇怪的问题： 我们有两个服务，服务A调用服务B发送邮件，但结果是重复发送了4封邮件，内容还是一样的。一开始以为是邮件服务器出问题，但通过本地调试发现发送邮件的方法是被重复调用了。 至于为什么会重复调用，这得先了解一下Spring Cloud 的重试机制。 Spring Cloud 整合了 Spring Retry实现重试逻辑。 可以通过Ribbon、Feign、Zuul中进行配置。 服务A中有如下Ribbon配置： 1234567891011ribbon: # 客户端读取超时时间，超时时间要小于Hystrix的超时时间，否则重试机制就无意义了 ReadTimeout: $&#123;RIBBON_READ_TIMEOUT:30000&#125; # 客户端连接超时时间 ConnectTimeout: $&#123;RIBBON_CONNECT_TIMEOUT:3000&#125; # 访问实例失败(超时)，允许自动重试，设置重试次数，失败后会更换实例访问，请一定确保接口的幂等性，否则重试可能导致数据异常。 OkToRetryOnAllOperations: true # 最大重试次数(不包含首次调用) MaxAutoRetries: 1 # 切换实例后最大重试次数 MaxAutoRetriesNextServer: 1 可以看到 OkToRetryOnAllOperations 属性控制了超时时是否自动重试所有请求，当为true时会重试所有请求，包含POST请求，可能会对服务器资源产生影响，如我遇到的重复发送多封邮件的问题。 因此，我将服务A 的 ReadTimeOut更改为 40000，重新部署测试重复调用的情况消失。 ribbon.retryableStatusCodes 属性列出希望客户端重试的响应代码。 Feign的重试功能被默认配置为永不启用 Zuul可以通过zuul.retryable 来控制是否开启重试。可以通过zuul.routes.routename.retryable来为逐个路由禁用重试功能","tags":["Rabbion"],"categories":["微服务"]},{"title":"一千行 MySQL 学习笔记-转载","path":"/posts/b399a0b0/","content":"转载至：https://shockerli.net/post/1000-line-mysql-note/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989990991992993994995996997998999100010011002100310041005100610071008100910101011101210131014101510161017101810191020102110221023102410251026102710281029103010311032103310341035103610371038103910401041104210431044/* Windows服务 */-- 启动MySQL net start mysql-- 创建Windows服务 sc create mysql binPath= mysqld_bin_path(注意：等号与值之间有空格)/* 连接与断开服务器 */mysql -h 地址 -P 端口 -u 用户名 -p 密码SHOW PROCESSLIST -- 显示哪些线程正在运行SHOW VARIABLES -- 显示系统变量信息/* 数据库操作 */ -------------------- 查看当前数据库 SELECT DATABASE();-- 显示当前时间、用户名、数据库版本 SELECT now(), user(), version();-- 创建库 CREATE DATABASE[ IF NOT EXISTS] 数据库名 数据库选项 数据库选项： CHARACTER SET charset_name COLLATE collation_name-- 查看已有库 SHOW DATABASES[ LIKE &#x27;PATTERN&#x27;]-- 查看当前库信息 SHOW CREATE DATABASE 数据库名-- 修改库的选项信息 ALTER DATABASE 库名 选项信息-- 删除库 DROP DATABASE[ IF EXISTS] 数据库名 同时删除该数据库相关的目录及其目录内容/* 表的操作 */ -------------------- 创建表 CREATE [TEMPORARY] TABLE[ IF NOT EXISTS] [库名.]表名 ( 表的结构定义 )[ 表选项] 每个字段必须有数据类型 最后一个字段后不能有逗号 TEMPORARY 临时表，会话结束时表自动消失 对于字段的定义： 字段名 数据类型 [NOT NULL | NULL] [DEFAULT default_value] [AUTO_INCREMENT] [UNIQUE [KEY] | [PRIMARY] KEY] [COMMENT &#x27;string&#x27;]-- 表选项 -- 字符集 CHARSET = charset_name 如果表没有设定，则使用数据库字符集 -- 存储引擎 ENGINE = engine_name 表在管理数据时采用的不同的数据结构，结构不同会导致处理方式、提供的特性操作等不同 常见的引擎：InnoDB MyISAM Memory/Heap BDB Merge Example CSV MaxDB Archive 不同的引擎在保存表的结构和数据时采用不同的方式 MyISAM表文件含义：.frm表定义，.MYD表数据，.MYI表索引 InnoDB表文件含义：.frm表定义，表空间数据和日志文件 SHOW ENGINES -- 显示存储引擎的状态信息 SHOW ENGINE 引擎名 &#123;LOGS|STATUS&#125; -- 显示存储引擎的日志或状态信息 -- 自增起始数 AUTO_INCREMENT = 行数 -- 数据文件目录 DATA DIRECTORY = &#x27;目录&#x27; -- 索引文件目录 INDEX DIRECTORY = &#x27;目录&#x27; -- 表注释 COMMENT = &#x27;string&#x27; -- 分区选项 PARTITION BY ... (详细见手册)-- 查看所有表 SHOW TABLES[ LIKE &#x27;pattern&#x27;] SHOW TABLES FROM 表名-- 查看表机构 SHOW CREATE TABLE 表名 （信息更详细） DESC 表名 / DESCRIBE 表名 / EXPLAIN 表名 / SHOW COLUMNS FROM 表名 [LIKE &#x27;PATTERN&#x27;] SHOW TABLE STATUS [FROM db_name] [LIKE &#x27;pattern&#x27;]-- 修改表 -- 修改表本身的选项 ALTER TABLE 表名 表的选项 eg: ALTER TABLE 表名 ENGINE=MYISAM; -- 对表进行重命名 RENAME TABLE 原表名 TO 新表名 RENAME TABLE 原表名 TO 库名.表名 （可将表移动到另一个数据库） -- RENAME可以交换两个表名 -- 修改表的字段机构（13.1.2. ALTER TABLE语法） ALTER TABLE 表名 操作名 -- 操作名 ADD[ COLUMN] 字段定义 -- 增加字段 AFTER 字段名 -- 表示增加在该字段名后面 FIRST -- 表示增加在第一个 ADD PRIMARY KEY(字段名) -- 创建主键 ADD UNIQUE [索引名] (字段名)-- 创建唯一索引 ADD INDEX [索引名] (字段名) -- 创建普通索引 DROP[ COLUMN] 字段名 -- 删除字段 MODIFY[ COLUMN] 字段名 字段属性 -- 支持对字段属性进行修改，不能修改字段名(所有原有属性也需写上) CHANGE[ COLUMN] 原字段名 新字段名 字段属性 -- 支持对字段名修改 DROP PRIMARY KEY -- 删除主键(删除主键前需删除其AUTO_INCREMENT属性) DROP INDEX 索引名 -- 删除索引 DROP FOREIGN KEY 外键 -- 删除外键-- 删除表 DROP TABLE[ IF EXISTS] 表名 ...-- 清空表数据 TRUNCATE [TABLE] 表名-- 复制表结构 CREATE TABLE 表名 LIKE 要复制的表名-- 复制表结构和数据 CREATE TABLE 表名 [AS] SELECT * FROM 要复制的表名-- 检查表是否有错误 CHECK TABLE tbl_name [, tbl_name] ... [option] ...-- 优化表 OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ...-- 修复表 REPAIR [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... [QUICK] [EXTENDED] [USE_FRM]-- 分析表 ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] .../* 数据操作 */ -------------------- 增 INSERT [INTO] 表名 [(字段列表)] VALUES (值列表)[, (值列表), ...] -- 如果要插入的值列表包含所有字段并且顺序一致，则可以省略字段列表。 -- 可同时插入多条数据记录！ REPLACE 与 INSERT 完全一样，可互换。 INSERT [INTO] 表名 SET 字段名=值[, 字段名=值, ...]-- 查 SELECT 字段列表 FROM 表名[ 其他子句] -- 可来自多个表的多个字段 -- 其他子句可以不使用 -- 字段列表可以用*代替，表示所有字段-- 删 DELETE FROM 表名[ 删除条件子句] 没有条件子句，则会删除全部-- 改 UPDATE 表名 SET 字段名=新值[, 字段名=新值] [更新条件]/* 字符集编码 */ -------------------- MySQL、数据库、表、字段均可设置编码-- 数据编码与客户端编码不需一致SHOW VARIABLES LIKE &#x27;character_set_%&#x27; -- 查看所有字符集编码项 character_set_client 客户端向服务器发送数据时使用的编码 character_set_results 服务器端将结果返回给客户端所使用的编码 character_set_connection 连接层编码SET 变量名 = 变量值 SET character_set_client = gbk; SET character_set_results = gbk; SET character_set_connection = gbk;SET NAMES GBK; -- 相当于完成以上三个设置-- 校对集 校对集用以排序 SHOW CHARACTER SET [LIKE &#x27;pattern&#x27;]/SHOW CHARSET [LIKE &#x27;pattern&#x27;] 查看所有字符集 SHOW COLLATION [LIKE &#x27;pattern&#x27;] 查看所有校对集 CHARSET 字符集编码 设置字符集编码 COLLATE 校对集编码 设置校对集编码/* 数据类型（列类型） */ ------------------1. 数值类型-- a. 整型 ---------- 类型 字节 范围（有符号位） tinyint 1字节 -128 ~ 127 无符号位：0 ~ 255 smallint 2字节 -32768 ~ 32767 mediumint 3字节 -8388608 ~ 8388607 int 4字节 bigint 8字节 int(M) M表示总位数 - 默认存在符号位，unsigned 属性修改 - 显示宽度，如果某个数不够定义字段时设置的位数，则前面以0补填，zerofill 属性修改 例：int(5) 插入一个数&#x27;123&#x27;，补填后为&#x27;00123&#x27; - 在满足要求的情况下，越小越好。 - 1表示bool值真，0表示bool值假。MySQL没有布尔类型，通过整型0和1表示。常用tinyint(1)表示布尔型。-- b. 浮点型 ---------- 类型 字节 范围 float(单精度) 4字节 double(双精度) 8字节 浮点型既支持符号位 unsigned 属性，也支持显示宽度 zerofill 属性。 不同于整型，前后均会补填0. 定义浮点型时，需指定总位数和小数位数。 float(M, D) double(M, D) M表示总位数，D表示小数位数。 M和D的大小会决定浮点数的范围。不同于整型的固定范围。 M既表示总位数（不包括小数点和正负号），也表示显示宽度（所有显示符号均包括）。 支持科学计数法表示。 浮点数表示近似值。-- c. 定点数 ---------- decimal -- 可变长度 decimal(M, D) M也表示总位数，D表示小数位数。 保存一个精确的数值，不会发生数据的改变，不同于浮点数的四舍五入。 将浮点数转换为字符串来保存，每9位数字保存为4个字节。2. 字符串类型-- a. char, varchar ---------- char 定长字符串，速度快，但浪费空间 varchar 变长字符串，速度慢，但节省空间 M表示能存储的最大长度，此长度是字符数，非字节数。 不同的编码，所占用的空间不同。 char,最多255个字符，与编码无关。 varchar,最多65535字符，与编码有关。 一条有效记录最大不能超过65535个字节。 utf8 最大为21844个字符，gbk 最大为32766个字符，latin1 最大为65532个字符 varchar 是变长的，需要利用存储空间保存 varchar 的长度，如果数据小于255个字节，则采用一个字节来保存长度，反之需要两个字节来保存。 varchar 的最大有效长度由最大行大小和使用的字符集确定。 最大有效长度是65532字节，因为在varchar存字符串时，第一个字节是空的，不存在任何数据，然后还需两个字节来存放字符串的长度，所以有效长度是64432-1-2=65532字节。 例：若一个表定义为 CREATE TABLE tb(c1 int, c2 char(30), c3 varchar(N)) charset=utf8; 问N的最大值是多少？ 答：(65535-1-2-4-30*3)/3-- b. blob, text ---------- blob 二进制字符串（字节字符串） tinyblob, blob, mediumblob, longblob text 非二进制字符串（字符字符串） tinytext, text, mediumtext, longtext text 在定义时，不需要定义长度，也不会计算总长度。 text 类型在定义时，不可给default值-- c. binary, varbinary ---------- 类似于char和varchar，用于保存二进制字符串，也就是保存字节字符串而非字符字符串。 char, varchar, text 对应 binary, varbinary, blob.3. 日期时间类型 一般用整型保存时间戳，因为PHP可以很方便的将时间戳进行格式化。 datetime 8字节 日期及时间 1000-01-01 00:00:00 到 9999-12-31 23:59:59 date 3字节 日期 1000-01-01 到 9999-12-31 timestamp 4字节 时间戳 19700101000000 到 2038-01-19 03:14:07 time 3字节 时间 -838:59:59 到 838:59:59 year 1字节 年份 1901 - 2155datetime YYYY-MM-DD hh:mm:sstimestamp YY-MM-DD hh:mm:ss YYYYMMDDhhmmss YYMMDDhhmmss YYYYMMDDhhmmss YYMMDDhhmmssdate YYYY-MM-DD YY-MM-DD YYYYMMDD YYMMDD YYYYMMDD YYMMDDtime hh:mm:ss hhmmss hhmmssyear YYYY YY YYYY YY4. 枚举和集合-- 枚举(enum) ----------enum(val1, val2, val3...) 在已知的值中进行单选。最大数量为65535. 枚举值在保存时，以2个字节的整型(smallint)保存。每个枚举值，按保存的位置顺序，从1开始逐一递增。 表现为字符串类型，存储却是整型。 NULL值的索引是NULL。 空字符串错误值的索引值是0。-- 集合（set） ----------set(val1, val2, val3...) create table tab ( gender set(&#x27;男&#x27;, &#x27;女&#x27;, &#x27;无&#x27;) ); insert into tab values (&#x27;男, 女&#x27;); 最多可以有64个不同的成员。以bigint存储，共8个字节。采取位运算的形式。 当创建表时，SET成员值的尾部空格将自动被删除。/* 选择类型 */-- PHP角度1. 功能满足2. 存储空间尽量小，处理效率更高3. 考虑兼容问题-- IP存储 ----------1. 只需存储，可用字符串2. 如果需计算，查找等，可存储为4个字节的无符号int，即unsigned 1) PHP函数转换 ip2long可转换为整型，但会出现携带符号问题。需格式化为无符号的整型。 利用sprintf函数格式化字符串 sprintf(&quot;%u&quot;, ip2long(&#x27;192.168.3.134&#x27;)); 然后用long2ip将整型转回IP字符串 2) MySQL函数转换(无符号整型，UNSIGNED) INET_ATON(&#x27;127.0.0.1&#x27;) 将IP转为整型 INET_NTOA(2130706433) 将整型转为IP/* 列属性（列约束） */ ------------------1. PRIMARY 主键 - 能唯一标识记录的字段，可以作为主键。 - 一个表只能有一个主键。 - 主键具有唯一性。 - 声明字段时，用 primary key 标识。 也可以在字段列表之后声明 例：create table tab ( id int, stu varchar(10), primary key (id)); - 主键字段的值不能为null。 - 主键可以由多个字段共同组成。此时需要在字段列表后声明的方法。 例：create table tab ( id int, stu varchar(10), age int, primary key (stu, age));2. UNIQUE 唯一索引（唯一约束） 使得某字段的值也不能重复。3. NULL 约束 null不是数据类型，是列的一个属性。 表示当前列是否可以为null，表示什么都没有。 null, 允许为空。默认。 not null, 不允许为空。 insert into tab values (null, &#x27;val&#x27;); -- 此时表示将第一个字段的值设为null, 取决于该字段是否允许为null4. DEFAULT 默认值属性 当前字段的默认值。 insert into tab values (default, &#x27;val&#x27;); -- 此时表示强制使用默认值。 create table tab ( add_time timestamp default current_timestamp ); -- 表示将当前时间的时间戳设为默认值。 current_date, current_time5. AUTO_INCREMENT 自动增长约束 自动增长必须为索引（主键或unique） 只能存在一个字段为自动增长。 默认为1开始自动增长。可以通过表属性 auto_increment = x进行设置，或 alter table tbl auto_increment = x;6. COMMENT 注释 例：create table tab ( id int ) comment &#x27;注释内容&#x27;;7. FOREIGN KEY 外键约束 用于限制主表与从表数据完整性。 alter table t1 add constraint `t1_t2_fk` foreign key (t1_id) references t2(id); -- 将表t1的t1_id外键关联到表t2的id字段。 -- 每个外键都有一个名字，可以通过 constraint 指定 存在外键的表，称之为从表（子表），外键指向的表，称之为主表（父表）。 作用：保持数据一致性，完整性，主要目的是控制存储在外键表（从表）中的数据。 MySQL中，可以对InnoDB引擎使用外键约束： 语法： foreign key (外键字段） references 主表名 (关联字段) [主表记录删除时的动作] [主表记录更新时的动作] 此时需要检测一个从表的外键需要约束为主表的已存在的值。外键在没有关联的情况下，可以设置为null.前提是该外键列，没有not null。 可以不指定主表记录更改或更新时的动作，那么此时主表的操作被拒绝。 如果指定了 on update 或 on delete：在删除或更新时，有如下几个操作可以选择： 1. cascade，级联操作。主表数据被更新（主键值更新），从表也被更新（外键值更新）。主表记录被删除，从表相关记录也被删除。 2. set null，设置为null。主表数据被更新（主键值更新），从表的外键被设置为null。主表记录被删除，从表相关记录外键被设置成null。但注意，要求该外键列，没有not null属性约束。 3. restrict，拒绝父表删除和更新。 注意，外键只被InnoDB存储引擎所支持。其他引擎是不支持的。/* 建表规范 */ ------------------ -- Normal Format, NF - 每个表保存一个实体信息 - 每个具有一个ID字段作为主键 - ID主键 + 原子表 -- 1NF, 第一范式 字段不能再分，就满足第一范式。 -- 2NF, 第二范式 满足第一范式的前提下，不能出现部分依赖。 消除符合主键就可以避免部分依赖。增加单列关键字。 -- 3NF, 第三范式 满足第二范式的前提下，不能出现传递依赖。 某个字段依赖于主键，而有其他字段依赖于该字段。这就是传递依赖。 将一个实体信息的数据放在一个表内实现。/* SELECT */ ------------------SELECT [ALL|DISTINCT] select_expr FROM -&gt; WHERE -&gt; GROUP BY [合计函数] -&gt; HAVING -&gt; ORDER BY -&gt; LIMITa. select_expr -- 可以用 * 表示所有字段。 select * from tb; -- 可以使用表达式（计算公式、函数调用、字段也是个表达式） select stu, 29+25, now() from tb; -- 可以为每个列使用别名。适用于简化列标识，避免多个列标识符重复。 - 使用 as 关键字，也可省略 as. select stu+10 as add10 from tb;b. FROM 子句 用于标识查询来源。 -- 可以为表起别名。使用as关键字。 SELECT * FROM tb1 AS tt, tb2 AS bb; -- from子句后，可以同时出现多个表。 -- 多个表会横向叠加到一起，而数据会形成一个笛卡尔积。 SELECT * FROM tb1, tb2; -- 向优化符提示如何选择索引 USE INDEX、IGNORE INDEX、FORCE INDEX SELECT * FROM table1 USE INDEX (key1,key2) WHERE key1=1 AND key2=2 AND key3=3; SELECT * FROM table1 IGNORE INDEX (key3) WHERE key1=1 AND key2=2 AND key3=3;c. WHERE 子句 -- 从from获得的数据源中进行筛选。 -- 整型1表示真，0表示假。 -- 表达式由运算符和运算数组成。 -- 运算数：变量（字段）、值、函数返回值 -- 运算符： =, &lt;=&gt;, &lt;&gt;, !=, &lt;=, &lt;, &gt;=, &gt;, !, &amp;&amp;, ||, in (not) null, (not) like, (not) in, (not) between and, is (not), and, or, not, xor is/is not 加上ture/false/unknown，检验某个值的真假 &lt;=&gt;与&lt;&gt;功能相同，&lt;=&gt;可用于null比较d. GROUP BY 子句, 分组子句 GROUP BY 字段/别名 [排序方式] 分组后会进行排序。升序：ASC，降序：DESC 以下[合计函数]需配合 GROUP BY 使用： count 返回不同的非NULL值数目 count(*)、count(字段) sum 求和 max 求最大值 min 求最小值 avg 求平均值 group_concat 返回带有来自一个组的连接的非NULL值的字符串结果。组内字符串连接。e. HAVING 子句，条件子句 与 where 功能、用法相同，执行时机不同。 where 在开始时执行检测数据，对原数据进行过滤。 having 对筛选出的结果再次进行过滤。 having 字段必须是查询出来的，where 字段必须是数据表存在的。 where 不可以使用字段的别名，having 可以。因为执行WHERE代码时，可能尚未确定列值。 where 不可以使用合计函数。一般需用合计函数才会用 having SQL标准要求HAVING必须引用GROUP BY子句中的列或用于合计函数中的列。f. ORDER BY 子句，排序子句 order by 排序字段/别名 排序方式 [,排序字段/别名 排序方式]... 升序：ASC，降序：DESC 支持多个字段的排序。g. LIMIT 子句，限制结果数量子句 仅对处理好的结果进行数量限制。将处理好的结果的看作是一个集合，按照记录出现的顺序，索引从0开始。 limit 起始位置, 获取条数 省略第一个参数，表示从索引0开始。limit 获取条数h. DISTINCT, ALL 选项 distinct 去除重复记录 默认为 all, 全部记录/* UNION */ ------------------ 将多个select查询的结果组合成一个结果集合。 SELECT ... UNION [ALL|DISTINCT] SELECT ... 默认 DISTINCT 方式，即所有返回的行都是唯一的 建议，对每个SELECT查询加上小括号包裹。 ORDER BY 排序时，需加上 LIMIT 进行结合。 需要各select查询的字段数量一样。 每个select查询的字段列表(数量、类型)应一致，因为结果中的字段名以第一条select语句为准。/* 子查询 */ ------------------ - 子查询需用括号包裹。-- from型 from后要求是一个表，必须给子查询结果取个别名。 - 简化每个查询内的条件。 - from型需将结果生成一个临时表格，可用以原表的锁定的释放。 - 子查询返回一个表，表型子查询。 select * from (select * from tb where id&gt;0) as subfrom where id&gt;1;-- where型 - 子查询返回一个值，标量子查询。 - 不需要给子查询取别名。 - where子查询内的表，不能直接用以更新。 select * from tb where money = (select max(money) from tb); -- 列子查询 如果子查询结果返回的是一列。 使用 in 或 not in 完成查询 exists 和 not exists 条件 如果子查询返回数据，则返回1或0。常用于判断条件。 select column1 from t1 where exists (select * from t2); -- 行子查询 查询条件是一个行。 select * from t1 where (id, gender) in (select id, gender from t2); 行构造符：(col1, col2, ...) 或 ROW(col1, col2, ...) 行构造符通常用于与对能返回两个或两个以上列的子查询进行比较。 -- 特殊运算符 != all() 相当于 not in = some() 相当于 in。any 是 some 的别名 != some() 不等同于 not in，不等于其中某一个。 all, some 可以配合其他运算符一起使用。/* 连接查询(join) */ ------------------ 将多个表的字段进行连接，可以指定连接条件。-- 内连接(inner join) - 默认就是内连接，可省略inner。 - 只有数据存在时才能发送连接。即连接结果不能出现空行。 on 表示连接条件。其条件表达式与where类似。也可以省略条件（表示条件永远为真） 也可用where表示连接条件。 还有 using, 但需字段名相同。 using(字段名) -- 交叉连接 cross join 即，没有条件的内连接。 select * from tb1 cross join tb2;-- 外连接(outer join) - 如果数据不存在，也会出现在连接结果中。 -- 左外连接 left join 如果数据不存在，左表记录会出现，而右表为null填充 -- 右外连接 right join 如果数据不存在，右表记录会出现，而左表为null填充-- 自然连接(natural join) 自动判断连接条件完成连接。 相当于省略了using，会自动查找相同字段名。 natural join natural left join natural right joinselect info.id, info.name, info.stu_num, extra_info.hobby, extra_info.sex from info, extra_info where info.stu_num = extra_info.stu_id;/* 导入导出 */ ------------------select * into outfile 文件地址 [控制格式] from 表名; -- 导出表数据load data [local] infile 文件地址 [replace|ignore] into table 表名 [控制格式]; -- 导入数据 生成的数据默认的分隔符是制表符 local未指定，则数据文件必须在服务器上 replace 和 ignore 关键词控制对现有的唯一键记录的重复的处理-- 控制格式fields 控制字段格式默认：fields terminated by &#x27;\\t&#x27; enclosed by &#x27;&#x27; escaped by &#x27;\\\\&#x27; terminated by &#x27;string&#x27; -- 终止 enclosed by &#x27;char&#x27; -- 包裹 escaped by &#x27;char&#x27; -- 转义 -- 示例： SELECT a,b,a+b INTO OUTFILE &#x27;/tmp/result.text&#x27; FIELDS TERMINATED BY &#x27;,&#x27; OPTIONALLY ENCLOSED BY &#x27;&quot;&#x27; LINES TERMINATED BY &#x27; &#x27; FROM test_table;lines 控制行格式默认：lines terminated by &#x27; &#x27; terminated by &#x27;string&#x27; -- 终止/* INSERT */ ------------------select语句获得的数据可以用insert插入。可以省略对列的指定，要求 values () 括号内，提供给了按照列顺序出现的所有字段的值。 或者使用set语法。 INSERT INTO tbl_name SET field=value,...；可以一次性使用多个值，采用(), (), ();的形式。 INSERT INTO tbl_name VALUES (), (), ();可以在列值指定时，使用表达式。 INSERT INTO tbl_name VALUES (field_value, 10+10, now());可以使用一个特殊值 DEFAULT，表示该列使用默认值。 INSERT INTO tbl_name VALUES (field_value, DEFAULT);可以通过一个查询的结果，作为需要插入的值。 INSERT INTO tbl_name SELECT ...;可以指定在插入的值出现主键（或唯一索引）冲突时，更新其他非主键列的信息。 INSERT INTO tbl_name VALUES/SET/SELECT ON DUPLICATE KEY UPDATE 字段=值, …;/* DELETE */ ------------------DELETE FROM tbl_name [WHERE where_definition] [ORDER BY ...] [LIMIT row_count]按照条件删除。where指定删除的最多记录数。limit可以通过排序条件删除。order by + limit支持多表删除，使用类似连接语法。delete from 需要删除数据多表1，表2 using 表连接操作 条件。/* TRUNCATE */ ------------------TRUNCATE [TABLE] tbl_name清空数据删除重建表区别：1，truncate 是删除表再创建，delete 是逐条删除2，truncate 重置auto_increment的值。而delete不会3，truncate 不知道删除了几条，而delete知道。4，当被用于带分区的表时，truncate 会保留分区/* 备份与还原 */ ------------------备份，将数据的结构与表内数据保存起来。利用 mysqldump 指令完成。-- 导出mysqldump [options] db_name [tables]mysqldump [options] ---database DB1 [DB2 DB3...]mysqldump [options] --all--database1. 导出一张表 mysqldump -u用户名 -p密码 库名 表名 &gt; 文件名(D:/a.sql)2. 导出多张表 mysqldump -u用户名 -p密码 库名 表1 表2 表3 &gt; 文件名(D:/a.sql)3. 导出所有表 mysqldump -u用户名 -p密码 库名 &gt; 文件名(D:/a.sql)4. 导出一个库 mysqldump -u用户名 -p密码 --lock-all-tables --database 库名 &gt; 文件名(D:/a.sql)可以-w携带WHERE条件-- 导入1. 在登录mysql的情况下： source 备份文件2. 在不登录的情况下 mysql -u用户名 -p密码 库名 &lt; 备份文件/* 视图 */ ------------------什么是视图： 视图是一个虚拟表，其内容由查询定义。同真实的表一样，视图包含一系列带有名称的列和行数据。但是，视图并不在数据库中以存储的数据值集形式存在。行和列数据来自由定义视图的查询所引用的表，并且在引用视图时动态生成。 视图具有表结构文件，但不存在数据文件。 对其中所引用的基础表来说，视图的作用类似于筛选。定义视图的筛选可以来自当前或其它数据库的一个或多个表，或者其它视图。通过视图进行查询没有任何限制，通过它们进行数据修改时的限制也很少。 视图是存储在数据库中的查询的sql语句，它主要出于两种原因：安全原因，视图可以隐藏一些数据，如：社会保险基金表，可以用视图只显示姓名，地址，而不显示社会保险号和工资数等，另一原因是可使复杂的查询易于理解和使用。-- 创建视图CREATE [OR REPLACE] [ALGORITHM = &#123;UNDEFINED | MERGE | TEMPTABLE&#125;] VIEW view_name [(column_list)] AS select_statement - 视图名必须唯一，同时不能与表重名。 - 视图可以使用select语句查询到的列名，也可以自己指定相应的列名。 - 可以指定视图执行的算法，通过ALGORITHM指定。 - column_list如果存在，则数目必须等于SELECT语句检索的列数-- 查看结构 SHOW CREATE VIEW view_name-- 删除视图 - 删除视图后，数据依然存在。 - 可同时删除多个视图。 DROP VIEW [IF EXISTS] view_name ...-- 修改视图结构 - 一般不修改视图，因为不是所有的更新视图都会映射到表上。 ALTER VIEW view_name [(column_list)] AS select_statement-- 视图作用 1. 简化业务逻辑 2. 对客户端隐藏真实的表结构-- 视图算法(ALGORITHM) MERGE 合并 将视图的查询语句，与外部查询需要先合并再执行！ TEMPTABLE 临时表 将视图执行完毕后，形成临时表，再做外层查询！ UNDEFINED 未定义(默认)，指的是MySQL自主去选择相应的算法。/* 事务(transaction) */ ------------------事务是指逻辑上的一组操作，组成这组操作的各个单元，要不全成功要不全失败。 - 支持连续SQL的集体成功或集体撤销。 - 事务是数据库在数据晚自习方面的一个功能。 - 需要利用 InnoDB 或 BDB 存储引擎，对自动提交的特性支持完成。 - InnoDB被称为事务安全型引擎。-- 事务开启 START TRANSACTION; 或者 BEGIN; 开启事务后，所有被执行的SQL语句均被认作当前事务内的SQL语句。-- 事务提交 COMMIT;-- 事务回滚 ROLLBACK; 如果部分操作发生问题，映射到事务开启前。-- 事务的特性 1. 原子性（Atomicity） 事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。 2. 一致性（Consistency） 事务前后数据的完整性必须保持一致。 - 事务开始和结束时，外部数据一致 - 在整个事务过程中，操作是连续的 3. 隔离性（Isolation） 多个用户并发访问数据库时，一个用户的事务不能被其它用户的事物所干扰，多个并发事务之间的数据要相互隔离。 4. 持久性（Durability） 一个事务一旦被提交，它对数据库中的数据改变就是永久性的。-- 事务的实现 1. 要求是事务支持的表类型 2. 执行一组相关的操作前开启事务 3. 整组操作完成后，都成功，则提交；如果存在失败，选择回滚，则会回到事务开始的备份点。-- 事务的原理 利用InnoDB的自动提交(autocommit)特性完成。 普通的MySQL执行语句后，当前的数据提交操作均可被其他客户端可见。 而事务是暂时关闭“自动提交”机制，需要commit提交持久化数据操作。-- 注意 1. 数据定义语言（DDL）语句不能被回滚，比如创建或取消数据库的语句，和创建、取消或更改表或存储的子程序的语句。 2. 事务不能被嵌套-- 保存点 SAVEPOINT 保存点名称 -- 设置一个事务保存点 ROLLBACK TO SAVEPOINT 保存点名称 -- 回滚到保存点 RELEASE SAVEPOINT 保存点名称 -- 删除保存点-- InnoDB自动提交特性设置 SET autocommit = 0|1; 0表示关闭自动提交，1表示开启自动提交。 - 如果关闭了，那普通操作的结果对其他客户端也不可见，需要commit提交后才能持久化数据操作。 - 也可以关闭自动提交来开启事务。但与START TRANSACTION不同的是， SET autocommit是永久改变服务器的设置，直到下次再次修改该设置。(针对当前连接) 而START TRANSACTION记录开启前的状态，而一旦事务提交或回滚后就需要再次开启事务。(针对当前事务)/* 锁表 */表锁定只用于防止其它客户端进行不正当地读取和写入MyISAM 支持表锁，InnoDB 支持行锁-- 锁定 LOCK TABLES tbl_name [AS alias]-- 解锁 UNLOCK TABLES/* 触发器 */ ------------------ 触发程序是与表有关的命名数据库对象，当该表出现特定事件时，将激活该对象 监听：记录的增加、修改、删除。-- 创建触发器CREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt 参数： trigger_time是触发程序的动作时间。它可以是 before 或 after，以指明触发程序是在激活它的语句之前或之后触发。 trigger_event指明了激活触发程序的语句的类型 INSERT：将新行插入表时激活触发程序 UPDATE：更改某一行时激活触发程序 DELETE：从表中删除某一行时激活触发程序 tbl_name：监听的表，必须是永久性的表，不能将触发程序与TEMPORARY表或视图关联起来。 trigger_stmt：当触发程序激活时执行的语句。执行多个语句，可使用BEGIN...END复合语句结构-- 删除DROP TRIGGER [schema_name.]trigger_name可以使用old和new代替旧的和新的数据 更新操作，更新前是old，更新后是new. 删除操作，只有old. 增加操作，只有new.-- 注意 1. 对于具有相同触发程序动作时间和事件的给定表，不能有两个触发程序。-- 字符连接函数concat(str1,str2,...])concat_ws(separator,str1,str2,...)-- 分支语句if 条件 then 执行语句elseif 条件 then 执行语句else 执行语句end if;-- 修改最外层语句结束符delimiter 自定义结束符号 SQL语句自定义结束符号delimiter ; -- 修改回原来的分号-- 语句块包裹begin 语句块end-- 特殊的执行1. 只要添加记录，就会触发程序。2. Insert into on duplicate key update 语法会触发： 如果没有重复记录，会触发 before insert, after insert; 如果有重复记录并更新，会触发 before insert, before update, after update; 如果有重复记录但是没有发生更新，则触发 before insert, before update3. Replace 语法 如果有记录，则执行 before insert, before delete, after delete, after insert/* SQL编程 */ --------------------// 局部变量 ------------ 变量声明 declare var_name[,...] type [default value] 这个语句被用来声明局部变量。要给变量提供一个默认值，请包含一个default子句。值可以被指定为一个表达式，不需要为一个常数。如果没有default子句，初始值为null。-- 赋值 使用 set 和 select into 语句为变量赋值。 - 注意：在函数内是可以使用全局变量（用户自定义的变量）--// 全局变量 ------------ 定义、赋值set 语句可以定义并为变量赋值。set @var = value;也可以使用select into语句为变量初始化并赋值。这样要求select语句只能返回一行，但是可以是多个字段，就意味着同时为多个变量进行赋值，变量的数量需要与查询的列数一致。还可以把赋值语句看作一个表达式，通过select执行完成。此时为了避免=被当作关系运算符看待，使用:=代替。（set语句可以使用= 和 :=）。select @var:=20;select @v1:=id, @v2=name from t1 limit 1;select * from tbl_name where @var:=30;select into 可以将表中查询获得的数据赋给变量。 -| select max(height) into @max_height from tb;-- 自定义变量名为了避免select语句中，用户自定义的变量与系统标识符（通常是字段名）冲突，用户自定义变量在变量名前使用@作为开始符号。@var=10; - 变量被定义后，在整个会话周期都有效（登录到退出）--// 控制结构 ------------ if语句if search_condition then statement_list [elseif search_condition then statement_list]...[else statement_list]end if;-- case语句CASE value WHEN [compare-value] THEN result[WHEN [compare-value] THEN result ...][ELSE result]END-- while循环[begin_label:] while search_condition do statement_listend while [end_label];- 如果需要在循环内提前终止 while循环，则需要使用标签；标签需要成对出现。 -- 退出循环 退出整个循环 leave 退出当前循环 iterate 通过退出的标签决定退出哪个循环--// 内置函数 ------------ 数值函数abs(x) -- 绝对值 abs(-10.9) = 10format(x, d) -- 格式化千分位数值 format(1234567.456, 2) = 1,234,567.46ceil(x) -- 向上取整 ceil(10.1) = 11floor(x) -- 向下取整 floor (10.1) = 10round(x) -- 四舍五入去整mod(m, n) -- m%n m mod n 求余 10%3=1pi() -- 获得圆周率pow(m, n) -- m^nsqrt(x) -- 算术平方根rand() -- 随机数truncate(x, d) -- 截取d位小数-- 时间日期函数now(), current_timestamp(); -- 当前日期时间current_date(); -- 当前日期current_time(); -- 当前时间date(&#x27;yyyy-mm-dd hh:ii:ss&#x27;); -- 获取日期部分time(&#x27;yyyy-mm-dd hh:ii:ss&#x27;); -- 获取时间部分date_format(&#x27;yyyy-mm-dd hh:ii:ss&#x27;, &#x27;%d %y %a %d %m %b %j&#x27;); -- 格式化时间unix_timestamp(); -- 获得unix时间戳from_unixtime(); -- 从时间戳获得时间-- 字符串函数length(string) -- string长度，字节char_length(string) -- string的字符个数substring(str, position [,length]) -- 从str的position开始,取length个字符replace(str ,search_str ,replace_str) -- 在str中用replace_str替换search_strinstr(string ,substring) -- 返回substring首次在string中出现的位置concat(string [,...]) -- 连接字串charset(str) -- 返回字串字符集lcase(string) -- 转换成小写left(string, length) -- 从string2中的左边起取length个字符load_file(file_name) -- 从文件读取内容locate(substring, string [,start_position]) -- 同instr,但可指定开始位置lpad(string, length, pad) -- 重复用pad加在string开头,直到字串长度为lengthltrim(string) -- 去除前端空格repeat(string, count) -- 重复count次rpad(string, length, pad) --在str后用pad补充,直到长度为lengthrtrim(string) -- 去除后端空格strcmp(string1 ,string2) -- 逐字符比较两字串大小-- 流程函数case when [condition] then result [when [condition] then result ...] [else result] end 多分支if(expr1,expr2,expr3) 双分支。-- 聚合函数count()sum();max();min();avg();group_concat()-- 其他常用函数md5();default();--// 存储函数，自定义函数 ------------ 新建 CREATE FUNCTION function_name (参数列表) RETURNS 返回值类型 函数体 - 函数名，应该合法的标识符，并且不应该与已有的关键字冲突。 - 一个函数应该属于某个数据库，可以使用db_name.funciton_name的形式执行当前函数所属数据库，否则为当前数据库。 - 参数部分，由&quot;参数名&quot;和&quot;参数类型&quot;组成。多个参数用逗号隔开。 - 函数体由多条可用的mysql语句，流程控制，变量声明等语句构成。 - 多条语句应该使用 begin...end 语句块包含。 - 一定要有 return 返回值语句。-- 删除 DROP FUNCTION [IF EXISTS] function_name;-- 查看 SHOW FUNCTION STATUS LIKE &#x27;partten&#x27; SHOW CREATE FUNCTION function_name;-- 修改 ALTER FUNCTION function_name 函数选项--// 存储过程，自定义功能 ------------ 定义存储存储过程 是一段代码（过程），存储在数据库中的sql组成。一个存储过程通常用于完成一段业务逻辑，例如报名，交班费，订单入库等。而一个函数通常专注与某个功能，视为其他程序服务的，需要在其他语句中调用函数才可以，而存储过程不能被其他调用，是自己执行 通过call执行。-- 创建CREATE PROCEDURE sp_name (参数列表) 过程体参数列表：不同于函数的参数列表，需要指明参数类型IN，表示输入型OUT，表示输出型INOUT，表示混合型注意，没有返回值。/* 存储过程 */ ------------------存储过程是一段可执行性代码的集合。相比函数，更偏向于业务逻辑。调用：CALL 过程名-- 注意- 没有返回值。- 只能单独调用，不可夹杂在其他语句中-- 参数IN|OUT|INOUT 参数名 数据类型IN 输入：在调用过程中，将数据输入到过程体内部的参数OUT 输出：在调用过程中，将过程体处理完的结果返回到客户端INOUT 输入输出：既可输入，也可输出-- 语法CREATE PROCEDURE 过程名 (参数列表)BEGIN 过程体END/* 用户和权限管理 */ -------------------- root密码重置1. 停止MySQL服务2. [Linux] /usr/local/mysql/bin/safe_mysqld --skip-grant-tables &amp; [Windows] mysqld --skip-grant-tables3. use mysql;4. UPDATE `user` SET PASSWORD=PASSWORD(&quot;密码&quot;) WHERE `user` = &quot;root&quot;;5. FLUSH PRIVILEGES;用户信息表：mysql.user-- 刷新权限FLUSH PRIVILEGES;-- 增加用户CREATE USER 用户名 IDENTIFIED BY [PASSWORD] 密码(字符串) - 必须拥有mysql数据库的全局CREATE USER权限，或拥有INSERT权限。 - 只能创建用户，不能赋予权限。 - 用户名，注意引号：如 &#x27;user_name&#x27;@&#x27;192.168.1.1&#x27; - 密码也需引号，纯数字密码也要加引号 - 要在纯文本中指定密码，需忽略PASSWORD关键词。要把密码指定为由PASSWORD()函数返回的混编值，需包含关键字PASSWORD-- 重命名用户RENAME USER old_user TO new_user-- 设置密码SET PASSWORD = PASSWORD(&#x27;密码&#x27;) -- 为当前用户设置密码SET PASSWORD FOR 用户名 = PASSWORD(&#x27;密码&#x27;) -- 为指定用户设置密码-- 删除用户DROP USER 用户名-- 分配权限/添加用户GRANT 权限列表 ON 表名 TO 用户名 [IDENTIFIED BY [PASSWORD] &#x27;password&#x27;] - all privileges 表示所有权限 - *.* 表示所有库的所有表 - 库名.表名 表示某库下面的某表 GRANT ALL PRIVILEGES ON `pms`.* TO &#x27;pms&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;pms0817&#x27;;-- 查看权限SHOW GRANTS FOR 用户名 -- 查看当前用户权限 SHOW GRANTS; 或 SHOW GRANTS FOR CURRENT_USER; 或 SHOW GRANTS FOR CURRENT_USER();-- 撤消权限REVOKE 权限列表 ON 表名 FROM 用户名REVOKE ALL PRIVILEGES, GRANT OPTION FROM 用户名 -- 撤销所有权限-- 权限层级-- 要使用GRANT或REVOKE，您必须拥有GRANT OPTION权限，并且您必须用于您正在授予或撤销的权限。全局层级：全局权限适用于一个给定服务器中的所有数据库，mysql.user GRANT ALL ON *.*和 REVOKE ALL ON *.*只授予和撤销全局权限。数据库层级：数据库权限适用于一个给定数据库中的所有目标，mysql.db, mysql.host GRANT ALL ON db_name.*和REVOKE ALL ON db_name.*只授予和撤销数据库权限。表层级：表权限适用于一个给定表中的所有列，mysql.talbes_priv GRANT ALL ON db_name.tbl_name和REVOKE ALL ON db_name.tbl_name只授予和撤销表权限。列层级：列权限适用于一个给定表中的单一列，mysql.columns_priv 当使用REVOKE时，您必须指定与被授权列相同的列。-- 权限列表ALL [PRIVILEGES] -- 设置除GRANT OPTION之外的所有简单权限ALTER -- 允许使用ALTER TABLEALTER ROUTINE -- 更改或取消已存储的子程序CREATE -- 允许使用CREATE TABLECREATE ROUTINE -- 创建已存储的子程序CREATE TEMPORARY TABLES -- 允许使用CREATE TEMPORARY TABLECREATE USER -- 允许使用CREATE USER, DROP USER, RENAME USER和REVOKE ALL PRIVILEGES。CREATE VIEW -- 允许使用CREATE VIEWDELETE -- 允许使用DELETEDROP -- 允许使用DROP TABLEEXECUTE -- 允许用户运行已存储的子程序FILE -- 允许使用SELECT...INTO OUTFILE和LOAD DATA INFILEINDEX -- 允许使用CREATE INDEX和DROP INDEXINSERT -- 允许使用INSERTLOCK TABLES -- 允许对您拥有SELECT权限的表使用LOCK TABLESPROCESS -- 允许使用SHOW FULL PROCESSLISTREFERENCES -- 未被实施RELOAD -- 允许使用FLUSHREPLICATION CLIENT -- 允许用户询问从属服务器或主服务器的地址REPLICATION SLAVE -- 用于复制型从属服务器（从主服务器中读取二进制日志事件）SELECT -- 允许使用SELECTSHOW DATABASES -- 显示所有数据库SHOW VIEW -- 允许使用SHOW CREATE VIEWSHUTDOWN -- 允许使用mysqladmin shutdownSUPER -- 允许使用CHANGE MASTER, KILL, PURGE MASTER LOGS和SET GLOBAL语句，mysqladmin debug命令；允许您连接（一次），即使已达到max_connections。UPDATE -- 允许使用UPDATEUSAGE -- “无权限”的同义词GRANT OPTION -- 允许授予权限/* 表维护 */-- 分析和存储表的关键字分布ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE 表名 ...-- 检查一个或多个表是否有错误CHECK TABLE tbl_name [, tbl_name] ... [option] ...option = &#123;QUICK | FAST | MEDIUM | EXTENDED | CHANGED&#125;-- 整理数据文件的碎片OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] .../* 杂项 */ ------------------1. 可用反引号（`）为标识符（库名、表名、字段名、索引、别名）包裹，以避免与关键字重名！中文也可以作为标识符！2. 每个库目录存在一个保存当前数据库的选项文件db.opt。3. 注释： 单行注释 # 注释内容 多行注释 /* 注释内容 */ 单行注释 -- 注释内容 (标准SQL注释风格，要求双破折号后加一空格符（空格、TAB、换行等）)4. 模式通配符： _ 任意单个字符 % 任意多个字符，甚至包括零字符 单引号需要进行转义 \\&#x27;5. CMD命令行内的语句结束符可以为 &quot;;&quot;, &quot;\\G&quot;, &quot;\\g&quot;，仅影响显示结果。其他地方还是用分号结束。delimiter 可修改当前对话的语句结束符。6. SQL对大小写不敏感7. 清除已有语句：\\c","tags":["Mysql"],"categories":["mysql"]},{"title":"JVM调优","path":"/posts/1653795f/","content":"JVM相关知识记录 JVM内存结构 每启动一个线程，JVM就会在栈空间栈分配对应的 线程栈, 比如 1MB 的空间（-Xss1m）。 线程栈也叫做Java方法栈。 如果使用了JNI方法，则会分配一个单独的本地方法栈(Native Stack). 线程执行过程中，一般会有多个方法组成调用栈(Stack Trace), 比如A调用B，B调用C。。。每执行到一个方法，就会创建对应的 栈帧(Frame). 栈帧只是一个逻辑上的概念，具体的大小，在一个方法编写完成后基本上就能确定。 比如返回值需要有一个空间存放吧，每个局部变量都需要对应的地址空间，此外还有操作数栈，以及方法指针(标识这个栈帧对应的是哪个类的哪个方法,指向常量池中的字符串常量）。 Java程序除了栈内存之外，最主要内存区域就是堆内存了。 堆内存是所有线程共用的内存空间，理论上大家都可以访问里面的内容。 但JVM的具体实现一般会有各种优化。 比如将逻辑上的Java堆,划分为堆(Heap)和非堆(Non-Heap)两个部分. 这种划分的依据在于，我们编写的Java代码，基本上只能使用Heap这部分空间，发生内存分配和回收的主要区域也在这部分，所以有一种说法，这里的Heap也叫GC管理的堆(GC Heap)。 GC理论中有一个重要的思想，叫做分代。 经过研究发现，程序中分配的对象，要么用过就扔，要么就能存活很久很久。 JVM将Heap内存分为年轻代（Young generation）和老年代（Old generation, 也叫 Tenured）两部分。 年轻代还划分为3个内存池，新生代(Eden space)和存活区(Survivor space), 存活区在大部分GC算法中有2个(S0, S1)，S0和S1总有一个是空的,但一般较小，也不浪费多少空间。 具体实现对新生代还有优化，那就是TLAB(Thread Local Allocation Buffer), 给每个线程先划定一小片空间，你创建的对象先在这里分配，满了再换。这能极大降低并发资源锁定的开销。 Non-Heap本质上还是Heap，只是一般不归GC管理，里面划分为3个内存池。 Metaspace, 以前叫持久代(永久代, Permanent generation), Java8换了个名字叫 Metaspace. Java8将方法区移动到了Meta区里面，而方法又是class的一部分。。。和CCS交叉了? CCS, Compressed Class Space, 存放class信息的，和 Metaspace 有交叉。 Code Cache, 存放 JIT 编译器编译后的本地机器代码。 JVM的内存结构大致如此。 以上来源自：https://github.com/cncounter/translation/blob/master/tiemao_2019/22_chat_jvm_troubleshoot/README.md 基础工具jps全称 Java Virtual Machine Process Status Too 。列出目标系统上已检测到的Java虚拟机（JVM） 如没有指定远程主机，则显示当前主机当前用户下的Java应用的PID与标识符 概要 jps\t[参数] [hostid] 参数 -q\t不输出类名、Jar名和传入main方法的参数 -m\t输出传入main方法的参数 -l\t显示应用程序main类的完整程序包名称或应用程序JAR文件的完整路径名 -v\t显示传递给JVM的参数 示例 在本地启动了一个register服务 123456➜ hsrm-register git:(master) jps -l19334 org.jetbrains.jps.cmdline.Launcher19335 org.hsrm.register.RegisterApplication19304 org.jetbrains.idea.maven.server.RemoteMavenServer36537 nutstore.client.gui.NutstoreGUI19658 sun.tools.jps.Jps jstack打印Java进程，核心文件或远程调试服务器的Java线程堆栈跟踪。根据堆栈信息可以帮助我们定位到具体的代码 概要 jstack [参数] pid ​\t为其打印堆栈跟踪的进程ID。该进程必须是Java进程。要获取机器上运行的Java进程的列表，请使用jps 参数 -F\t当jstack[ -l] pid没有响应时，强制进行堆栈转储 -l 打印有关锁的其他信息 -m\t不仅会输出Java堆栈信息，还会输出C&#x2F;C++堆栈信息（比如Native方法","tags":["JVM"],"categories":["Java"]},{"title":"无法创建本地线程分析","path":"/posts/b161c9c2/","content":"最近项目的系统突然出现崩溃，无法响应请求。使用运维账号登陆服务器也无法登陆，提示 12fork: retry: No child processes fork: Resource temporarily unavailable 通过root账号登陆上去查看应用日志，报错如下： 1java.lang.OutOfMemoryError: unable to create new native thread OOM造成无法创建线程。于是先把部分服务停了，运维账号则可正常登陆。 分析原因由于之前也没有遇到过类似问题，通过网上查找，分析了多种原因，记录如下。 应用内存分配过多由于该项目是一个微服务构架，总共有近20个微服务，但只用了两台64G内存的服务器做集群，内存确实是很紧张。我统计了所有服务分配的堆内存，即 -Xmx 的值，一共分配了大概45G的样子。因此决定重新分配各微服务的堆内存大小。 1、将不必要的微服务停止 2、将不使用量不大的部分服务减少堆内存的分配的大小 3、将之前分配过大的堆内存减小 调整之后所有启用的微服务分配的总堆内存大概减小到了32G，之后重启各服务，以为万事大吉了，但过了几天再次出现无法创建线程的问题了。。。 用户可用线程数限制这次怀疑是用户最大的可用线程数受限了，通过 ulimit -u 命令可查询当前用户最大可使用的线程数。经查询运维账号可使用的线程数为 65535 这个数量完全是够用了。 我们可以通过这个命令统计出某个进程所使用的线程数，所有服务的线程数总合完全没有超过 65536 1ps Hh p &lt;pid&gt; | wc -l 因此可排除这个原因 线程数过高经常上面再次分析没有找到原因，但通过查看各服务线程数，发现某个服务线程数很高，停止或重启该服务，则free内存多了很多，但过几天后随着该服务线程数越来越高，free 内存也越来越少。 因此决定通过分析该服务线程数过高看能否找出问题。 1、首先可以通过jps命令查看当前用户下的所有java应用进程，得到各服务的pid 2、通过 ps Hh p &lt;pid&gt; | wc -l 查询每个pid，找出线程数最多的一个服务 3、通过top Hp &lt;pid&gt; 可以动态展示该进程下的线程情况。找出最占CPU的线程，记录该线程的pid 4、通过 printf “%x” &lt;pid&gt; 将上面的线程pid转化为十六进制数 5、通过Jstack 查找该线程的堆栈信息 1jstack &lt;进程pid&gt; | grep &lt;线程的十六进制数形式&gt; jstack -l 表示显示锁信息 grep -C 5\t表示展示上下5行信息 通过对堆栈的分析，我们可能获得一些信息，如该线程是什么状态，被什么锁了等。 但很遗憾，通过对堆栈分析，我也没有找出具体的问题。 Linux缓存通过使用free -m 命令，发现虽然free内存很少，但buff&#x2F;cache占用很大 按理来说 buff&#x2F;cache也是空闲内存，如果系统内存不足，应该是会自动清缓存的。很显然系统并没有清除缓存，所有导致系统可用的空闲内存不足。 首先分析缓存占用大的问题。应用系统中有许多附件需要上传或下载，并且对minio做了群集配置，这可能是造成cache过大的原因。 1、手动清除缓存 Linux中可以手动清除缓存，通过 修改/proc/sys/vm/drop_caches 文件来触发。 这个文件默认值为0 值为1时：可以释放pagecache缓存 值为2时：可以释放pagecache和inode缓存 值为3时：可以释放pagecache, dentries和inodes缓存 因此，可以通过修改这个文件的值来触发系统清理缓存。 1234sync #通知系统将缓存及时写入echo 1 &gt;&gt; /proc/sys/vm/drop_cachesecho 2 &gt;&gt; /proc/sys/vm/drop_cachesecho 3 &gt;&gt; /proc/sys/vm/drop_caches 执行完成后再使用 free -m 命令，会发现free内存多出来几十G，这下内存暂时是足够了。 但如果后面缓存继续占用越来越大，这种情况还是有可能发生，因此最终还是需要找出系统为什么不会自动清理缓存的原因才能从根本上解决这个问题。","tags":["JVM"],"categories":["Java"]},{"title":"注解结合切面实现键值转换","path":"/posts/cd85f6d4/","content":"不积跬步，无以至千里。不积小流，无以成江海。 通常我们有这种需求，某一个字段是由前台维护的：编码 &lt;&#x3D;&#x3D;&gt; 含义，类似一个键值对。含义是可能会变化的，但编码是固定的，一般我们都会在数据库中直接存储编码，但数据返回给前台显示时需要展示为含义。当然我们可以在从数据库查询时去join到该值的含义一起返回，这里提供另一种方式来实现这个功能。 实现思路主要是使用注解加切面。在数据返回前通过切面把被注解的字段替换成该值的含义。 这里使用性别字段，键值关系为: M - 男 W - 女 新建注解新建一个注解用于标识字段需要被切面处理 1234@Target(ElementType.FIELD)@Retention(RetentionPolicy.RUNTIME)public @interface SexValue &#123;&#125; 新建第二个注解，作为一个切点。 12345@Target(value = ElementType.METHOD )@Retention(value = RetentionPolicy.RUNTIME)public @interface ProcessResult &#123;&#125; 注解参数含义： @interface : 表示定义一个注解 @Target 表示该注解可以用于什么地方，可能的ElementType参数有： CONSTRUCTOR：构造器的声明 FIELD：域声明（包括enum实例） LOCAL_VARIABLE：局部变量声明 PACKAGE：包声明 PARAMETER：参数声明 TYPE：类、接口（包括注解类型）或enum声明 @Retention 表示需要在什么级别保存该注解信息。可选的RetentionPolicy参数包括： SOURCE：注解将被编译器丢弃 CLASS：注解在class文件中可用，但会被VM丢弃 RUNTIME：VM将在运行期间保留注解，因此可以通过反射机制读取注解的信息 @Document 将注解包含在Javadoc中 @Inherited 允许子类继承父类中的注解 Entity实体类如下，在 sex 字段上添加注解 1234567891011121314151617181920212223242526272829303132public class Person &#123; private Long id; private String name; private Integer age; @SexValue private String sex; private String address; private String phoneNum; private LocalDateTime createdDate; private LocalDateTime lastUpdateDate; private Long objectVersionNumber; public void createBefore()&#123; this.createdDate = LocalDateTime.now(); this.lastUpdateDate = LocalDateTime.now(); this.objectVersionNumber = 1L; &#125; public void updateBefore(Long objectVersionNumber)&#123; this.lastUpdateDate = LocalDateTime.now(); this.objectVersionNumber = objectVersionNumber + 1L; &#125;&#125; Controller在Controller 中，把需要翻译的方法加上注解。 1234567891011121314151617181920212223242526272829303132333435@RestController@RequestMapping(&quot;/v1/hjwjw/person&quot;)public class PersonController &#123; private IPersonService personService; public PersonController(IPersonService personService) &#123; this.personService = personService; &#125; @GetMapping @ProcessResult public ResponseEntity&lt;List&lt;Person&gt;&gt; query()&#123; return ResponseEntity.ok(personService.queryPerson()); &#125; @PostMapping @ProcessResult public ResponseEntity&lt;Person&gt; createPerson(@RequestBody Person personVO)&#123; return ResponseEntity.ok(personService.createPerson(personVO)); &#125; @PutMapping @ProcessResult public ResponseEntity&lt;Person&gt; updatePerson(@RequestBody Person personVO)&#123; return ResponseEntity.ok(personService.updatePerson(personVO)); &#125; @DeleteMapping(&quot;/&#123;personId&#125;&quot;) public ResponseEntity delPerson(@PathVariable(&quot;personId&quot;) Long personId)&#123; personService.delPerson(personId); return ResponseEntity.ok(HttpStatus.OK); &#125;&#125; 新建切面新建一个切面类，在Controller中加了 @ProcessResult 注解的方法，在返回前会进入切面进行处理。 返回的Object需要判断是否为集合，并把其父类字段都需要遍历查找是否有添加@SexValue注解。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768@Component@Aspectpublic class SexAspect &#123; private static final Logger LOGGER = LoggerFactory.getLogger(SexAspect.class); @AfterReturning(value = &quot;@annotation(processResult)&quot;,returning = &quot;result&quot;) public Object aftreReturning(JoinPoint joinPoint, ProcessResult processResult,Object result) throws IllegalAccessException &#123; LOGGER.info(joinPoint.toString()); LOGGER.info(&quot;&lt;===================Aspect======================&gt;&quot;); LOGGER.info(result.toString()); if (result == null)&#123; return null; &#125; if (result instanceof ResponseEntity)&#123; Object body = ((ResponseEntity&lt;?&gt;) result).getBody(); if (body == null)&#123; return null; &#125; if (body instanceof Collection )&#123; for (Object obj : (Collection&lt;?&gt;) body)&#123; //处理 processObj(obj); &#125; &#125;else &#123; //处理 processObj(body); &#125; &#125;else if (result instanceof Collection)&#123; //处理 for (Object obj : (Collection&lt;?&gt;) result)&#123; //处理 processObj(obj); &#125; &#125;else &#123; processObj(result); &#125; return result; &#125; private void processObj(Object obj) throws IllegalAccessException &#123; //取出Obj所有 Field，以及父类 Field List&lt;Field&gt; fieldList = new ArrayList&lt;&gt;(); Class&lt;?&gt; tempClass = obj.getClass(); while (tempClass != null)&#123; fieldList.addAll(Arrays.asList(tempClass.getDeclaredFields())); tempClass = tempClass.getSuperclass(); &#125; Field[] fields = new Field[fieldList.size()]; fieldList.toArray(fields); //遍历Field,查找添加 @SexValue 注解的字段 并 做翻译 for (Field field : fields)&#123; if (field.isAnnotationPresent(SexValue.class))&#123; field.setAccessible(true); String fieldValue = String.valueOf(field.get(obj)); LOGGER.info(&quot;fieldValue:&#123;&#125;&quot;,fieldValue); if (&quot;M&quot;.equals(fieldValue))&#123; field.set(obj,&quot;男&quot;); &#125;else &#123; field.set(obj,&quot;女&quot;); &#125; &#125; &#125; &#125;&#125; 这里只是做了简单的值转换。至于如何获取到对应的含义，建议把配置的值集缓存到Redis，这样在切面处理时可以根据编码从 Redis 中直接取出含义进行替换。","tags":["annotation"]},{"title":"StringUtils工具类split方法对比","path":"/posts/89d04356/","content":"不积跬步，无以至千里。不积小流，无以成江海。 我们常用的StringUtils工具类org.apache.commons.lang3.StringUtils 与 org.springframework.util.StringUtils 他们提供的Split方法有差别，导致今天程序出现问题时费了点时间才发现问题。特意写测试对比了两个方法，以免以后再掉坑里。 实现方式对比首先是 org.apache.commons.lang3.StringUtils#split 的实现 1234567891011121314151617181920212223242526272829/** * &lt;p&gt;Splits the provided text into an array, separators specified. * This is an alternative to using StringTokenizer.&lt;/p&gt; * * &lt;p&gt;The separator is not included in the returned String array. * Adjacent separators are treated as one separator. * For more control over the split use the StrTokenizer class.&lt;/p&gt; * * &lt;p&gt;A &#123;@code null&#125; input String returns &#123;@code null&#125;. * A &#123;@code null&#125; separatorChars splits on whitespace.&lt;/p&gt; * * &lt;pre&gt; * StringUtils.split(null, *) = null * StringUtils.split(&quot;&quot;, *) = [] * StringUtils.split(&quot;abc def&quot;, null) = [&quot;abc&quot;, &quot;def&quot;] * StringUtils.split(&quot;abc def&quot;, &quot; &quot;) = [&quot;abc&quot;, &quot;def&quot;] * StringUtils.split(&quot;abc def&quot;, &quot; &quot;) = [&quot;abc&quot;, &quot;def&quot;] * StringUtils.split(&quot;ab:cd:ef&quot;, &quot;:&quot;) = [&quot;ab&quot;, &quot;cd&quot;, &quot;ef&quot;] * &lt;/pre&gt; * * @param str the String to parse, may be null * @param separatorChars the characters used as the delimiters, * &#123;@code null&#125; splits on whitespace * @return an array of parsed Strings, &#123;@code null&#125; if null String input */public static String[] split(final String str, final String separatorChars) &#123; return splitWorker(str, separatorChars, -1, false);&#125; 在上面的注释中已经给出了一些示例 org.springframework.util.StringUtils#split的实现 123456789101112131415161718192021222324252627282930313233343536/** * Split a &#123;@code String&#125; at the first occurrence of the delimiter. * Does not include the delimiter in the result. * @param toSplit the string to split (potentially &#123;@code null&#125; or empty) * @param delimiter to split the string up with (potentially &#123;@code null&#125; or empty) * @return a two element array with index 0 being before the delimiter, and * index 1 being after the delimiter (neither element includes the delimiter); * or &#123;@code null&#125; if the delimiter wasn&#x27;t found in the given input &#123;@code String&#125; */@Nullablepublic static String[] split(@Nullable String toSplit, @Nullable String delimiter) &#123; if (!hasLength(toSplit) || !hasLength(delimiter)) &#123; return null; &#125; int offset = toSplit.indexOf(delimiter); if (offset &lt; 0) &#123; return null; &#125; String beforeDelimiter = toSplit.substring(0, offset); String afterDelimiter = toSplit.substring(offset + delimiter.length()); return new String[] &#123;beforeDelimiter, afterDelimiter&#125;;&#125;/** * Check that the given &#123;@code String&#125; is neither &#123;@code null&#125; nor of length 0. * &lt;p&gt;Note: this method returns &#123;@code true&#125; for a &#123;@code String&#125; that * purely consists of whitespace. * @param str the &#123;@code String&#125; to check (may be &#123;@code null&#125;) * @return &#123;@code true&#125; if the &#123;@code String&#125; is not &#123;@code null&#125; and has length * @see #hasLength(CharSequence) * @see #hasText(String) */public static boolean hasLength(@Nullable String str) &#123; return (str != null &amp;&amp; !str.isEmpty());&#125; 测试对比下面通过跑测试对比如下： 参数1 参数2 org.apache.commons.lang3.StringUtils#split org.springframework.util.StringUtils#split null null null null “1,2” “,” String[1]&#x3D;{“1,2”} null null “,” Null null “1,2,3” “,” String[3]&#x3D;{“1”,”2”,”3”} String[3]&#x3D;{“1”,”2”,”3”} “1,2,3” “:” String[1]&#x3D;{“1,2,3”} null “” “,” String[0]&#x3D;{} null “1” “,” String[1]&#x3D;{“1”} null 通过对比，org.apache.commons.lang3.StringUtils#split 优势明显，不会随便就抛出空指针异常，因此今后最好使用 org.apache.commons.lang3.StringUtils#split这个方法，可以避免一些无端的异常。","tags":["StringUtils"],"categories":["Java"]},{"title":"Java8 全新的日期与时间 API","path":"/posts/ac5ad770/","content":"不积跬步，无以至千里。不积小流，无以成江海。 Java8 全新的日期与时间 API 主要如下： LocalDate 表示年月日的日期 LocalTime 表示时分秒的时间 LocalDateTime 则包含以上 Instant 获取秒数 Instant.now() 创建对象 Instant.getEpochSecond() 获取秒数 Instant.toEpochMilli() 获取毫秒数 为什么需要 LocalDateTime Date 可读性差，我们经常需要使用 SimpleDateFormat 来格式化，使其按我们常用的方式显示。如果不格式化，则显示如下样式： 1Tue sep 10 09:34:04 CST 2019 当我们使用 SimpleDateFormat 的时候，又会出现其它问题。SimpleDateFormat 是线程不安全的。它的实现代码中使用了 calendar 共享变量，并且没有做线程安全控制。在多线程中使用时，会使 calendar.setTime 方法设置的值不准确。 对于以上问题可以有解决方法：1、每个线程使用时都创建一个 SimpleDateFormat 对象，但这样开销太多。2、对使用 format 与 parse 方法的地方加锁，但导致线程阻塞性能差。3、使用 ThreadLcoal 保证每个线程最多只创建一次 SimpleDateFormat 对象，这种是比较好的方法。 但总的来说，Date在处理时间上比较麻烦，而且其 getYear,getMonth 等方法已经被弃用了 新 API 特性LocalDate、LocalTime、LocalDateTime、Instant 为不可变对象，在修改它们时会返回一个副本。 格式化可以使用 DateTimeFormatter 进行格式化，其默认提供了一些格式化方式： 12345678LocalDate localDate = LocalDate.of(2019,10,20);//默认格式化方式String s1 = localDate.format(DateTimeFormatter.BASIC_ISO_DATE);String s2 = localDate.format(DateTimeformatter.ISO_LOCAL_DATE);//自定义格式化DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ofPattern(&quot;dd/MM/yyyy&quot;);String s3 = localDate.format(dateTimeFormatter); 解析时间12LocalDate localDate1 = LocalDate.parse(&quot;20190910&quot;, DateTimeFormatter.BASIC_ISO_DATE);LocalDate localDate2 = LocalDate.parse(&quot;2019-09-10&quot;, DateTimeFormatter.ISO_LOCAL_DATE); 另外，DateTimeFormatter 是线程安全的。 在 SpringBoot 中的使用 指定日期序列化为时间戳 返回前端 1234567\tpublic class LocalDateTimeConverter extends JsonSerializer&lt;LocalDateTime&gt; &#123; @Override public void serialize(LocalDateTime value, JsonGenerator gen, SerializerProvider serializers) throws IOException &#123; gen.writeNumber(value.toInstant(ZoneOffset.of(&quot;+8&quot;)).toEpochMilli()); &#125;&#125; 然后在字段上加了如下注解： 1@JsonSerialize(using=LocalDateTimeConverter.class) 指定日期时间格式化 返回前端 在字段上加上如下注解： 1@JsonFormat(shape=JsonFormat.Shape.STRING,pattern=&quot;yyyy-MM-dd HH:mm:ss&quot;) 指定前端传入的日期进行格式化 在字段上加如下注解： 1@DateTimeFormat(pattern=&quot;yyyy-MM-dd HH:mm:ss&quot;)","tags":["Java8"],"categories":["Java"]},{"title":"CXF动态客户端调用WebService","path":"/posts/48c14765/","content":"不积跬步，无以至千里。不积小流，无以成江海。 对于WebService的调用，之前是使用CXF直接生成代理类的方式，这样的好处是拿到后可以直接调用，但生成的文件太多，并且包名需要与服务端相同，使用进起来感觉很重。 使用CXF动态客户端的方式去调用也很简单，并且不需要生成任何文件。 示例代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106public class CxfApp &#123; private static Map&lt;Object, Endpoint&gt; factoryMap = new HashMap&lt;&gt;(); private static Map&lt;Object, Client&gt; clientMap = new HashMap&lt;&gt;(); public static void main(String[] args) &#123; Gson gson = new Gson(); SysCategoryWbSReq sysCategoryWbSReq = new SysCategoryWbSReq(); sysCategoryWbSReq.setCategoryName(&quot;test20190920&quot;); sysCategoryWbSReq.setCategoryCode(&quot;ML0025&quot;); sysCategoryWbSReq.setEnabledFlag(1); List&lt;Object&gt; paramList = new ArrayList&lt;&gt;(); paramList.add(gson.toJson(sysCategoryWbSReq)); String result = dynamicCallWebServiceByCXF(&quot;http://127.0.0.1:9095/application/applicationWBS/addAndUpdateSyscategory?wsdl&quot;, &quot;addAndUpdateSyscategory&quot;, &quot;http://applicationWBS.expense.app.hcf.hand.com&quot;,&quot;ApplicationWbService&quot;,paramList); System.out.println(result); &#125; /** * * @param wsdlUrl wsdl的地址：http://localhost:8001/demo/HelloServiceDemoUrl?wsdl * @param methodName 调用的方法名称 selectOrderInfo * @param targetNamespace 命名空间 http://service.limp.com/ * @param name name HelloServiceDemo * @param paramList 参数集合 * @throws Exception */ public static String dynamicCallWebServiceByCXF(String wsdlUrl, String methodName, String targetNamespace, String name, List&lt;Object&gt; paramList)&#123; //临时增加缓存，增加创建速度 if(!factoryMap.containsKey(methodName))&#123; // 创建动态客户端 JaxWsDynamicClientFactory factory = JaxWsDynamicClientFactory.newInstance(); // 创建客户端连接 Client client = factory.createClient(wsdlUrl); ClientImpl clientImpl = (ClientImpl) client; Endpoint endpoint = clientImpl.getEndpoint(); factoryMap.put(methodName,endpoint); clientMap.put(methodName,client); System.out.println(&quot;初始化&quot;); &#125; //从缓存中换取 endpoint、client Endpoint endpoint=factoryMap.get(methodName); Client client=clientMap.get(methodName); // Make use of CXF service model to introspect the existing WSDL ServiceInfo serviceInfo = endpoint.getService().getServiceInfos().get(0); // 创建QName来指定NameSpace和要调用的service String localPart=name+&quot;SoapBinding&quot;; QName bindingName = new QName(targetNamespace, localPart); BindingInfo binding = serviceInfo.getBinding(bindingName); //创建QName来指定NameSpace和要调用的方法绑定方法 QName opName = new QName(targetNamespace, methodName);//selectOrderInfo BindingOperationInfo boi = binding.getOperation(opName);// BindingMessageInfo inputMessageInfo = boi.getInput(); BindingMessageInfo inputMessageInfo = null; if (!boi.isUnwrapped()) &#123; //OrderProcess uses document literal wrapped style. inputMessageInfo = boi.getWrappedOperation().getInput(); &#125; else &#123; inputMessageInfo = boi.getUnwrappedOperation().getInput(); &#125; List&lt;MessagePartInfo&gt; parts = inputMessageInfo.getMessageParts(); /***********************以下是初始化参数，组装参数；处理返回结果的过程******************************************/ Object[] parameters = new Object[parts.size()]; for(int m=0;m&lt;parts.size();m++)&#123; MessagePartInfo part=parts.get(m); // 取得对象实例 Class&lt;?&gt; partClass = part.getTypeClass();//OrderInfo.class; System.out.println(partClass.getCanonicalName()); // GetAgentDetails //实例化对象 Object initDomain=null; //普通参数的形参，不需要fastJson转换直接赋值即可 if(&quot;java.lang.String&quot;.equalsIgnoreCase(partClass.getCanonicalName()) ||&quot;int&quot;.equalsIgnoreCase(partClass.getCanonicalName()))&#123; initDomain=paramList.get(m).toString(); &#125; //如果是数组 else if(partClass.getCanonicalName().indexOf(&quot;[]&quot;)&gt;-1)&#123; //转换数组 initDomain= JSON.parseArray(paramList.get(m).toString(),partClass.getComponentType()); &#125;else&#123; initDomain=JSON.parseObject(paramList.get(m).toString(),partClass); &#125; parameters[m]=initDomain; &#125; //定义返回结果集 Object[] result=null; //普通参数情况 || 对象参数情况 1个参数 ||ArryList集合 try &#123; result = client.invoke(opName,parameters); &#125;catch (Exception ex)&#123; ex.printStackTrace(); return &quot;参数异常&quot;+ex.getMessage(); &#125; //返回调用结果 if(result.length&gt;0)&#123; return JSON.toJSON(result[0]).toString(); &#125; return &quot;invoke success, but is void &quot;; &#125;&#125;","tags":["WebService"],"categories":["WebService"]},{"title":"Odata v2过滤常用指令","path":"/posts/34f6a5f4/","content":"不积跬步，无以至千里。 不积小流，无以成江海。 最近在使用Odata,从官网上摘录一些常用的Odta v2指令记录一下。 运算符 Operator Description Example Logical Operators 逻辑操作 Eq Equal - 等于 &#x2F;Suppliers?$filter&#x3D;Address&#x2F;City eq ‘Redmond’ Ne Not equal - 不等于 &#x2F;Suppliers?$filter&#x3D;Address&#x2F;City ne ‘London’ Gt Greater than - 大于 &#x2F;Products?$filter&#x3D;Price gt 20 Ge Greater than or equal -大于或等于 &#x2F;Products?$filter&#x3D;Price ge 10 Lt Less than - 小于 &#x2F;Products?$filter&#x3D;Price lt 20 Le Less than or equal - 小于或等于 &#x2F;Products?$filter&#x3D;Price le 100 And Logical and - 逻辑与 &#x2F;Products?$filter&#x3D;Price le 200 and Price gt 3.5 Or Logical or - 逻辑或 &#x2F;Products?$filter&#x3D;Price le 3.5 or Price gt 200 Not Logical negation - 逻辑非 &#x2F;Products?$filter&#x3D;not endswith(Description,’milk’) Arithmetic Operators 算术操作 Add Addition - 加 &#x2F;Products?$filter&#x3D;Price add 5 gt 10 Sub Subtraction -减 &#x2F;Products?$filter&#x3D;Price sub 5 gt 10 Mul Multiplication - 乘 &#x2F;Products?$filter&#x3D;Price mul 2 gt 2000 Div Division - 除 &#x2F;Products?$filter&#x3D;Price div 2 gt 4 Mod Modulo - 模 &#x2F;Products?$filter&#x3D;Price mod 2 eq 0 Grouping Operators ( ) Precedence grouping &#x2F;Products?$filter&#x3D;(Price sub 5) gt 10 函数 Function Example String Functions bool substringof(string po, string p1) https://services.odata.org/Northwind/Northwind.svc/Customers?$filter=substringof(‘Alfreds’, CompanyName) eq true bool endswith(string p0, string p1) https://services.odata.org/Northwind/Northwind.svc/Customers?$filter=endswith(CompanyName, ‘Futterkiste’) eq true bool startswith(string p0, string p1) https://services.odata.org/Northwind/Northwind.svc/Customers?$filter=startswith(CompanyName, ‘Alfr’) eq true int length(string p0) https://services.odata.org/Northwind/Northwind.svc/Customers?$filter=length(CompanyName) eq 19 int indexof(string p0, string p1) https://services.odata.org/Northwind/Northwind.svc/Customers?$filter=indexof(CompanyName, ‘lfreds’) eq 1 string replace(string p0, string find, string replace) https://services.odata.org/Northwind/Northwind.svc/Customers?$filter=replace(CompanyName, ‘ ‘, ‘’) eq ‘AlfredsFutterkiste’ string substring(string p0, int pos) https://services.odata.org/Northwind/Northwind.svc/Customers?$filter=substring(CompanyName, 1) eq ‘lfreds Futterkiste’ string substring(string p0, int pos, int length) https://services.odata.org/Northwind/Northwind.svc/Customers?$filter=substring(CompanyName, 1, 2) eq ‘lf’ string tolower(string p0) https://services.odata.org/Northwind/Northwind.svc/Customers?$filter=tolower(CompanyName) eq ‘alfreds futterkiste’ string toupper(string p0) https://services.odata.org/Northwind/Northwind.svc/Customers?$filter=toupper(CompanyName) eq ‘ALFREDS FUTTERKISTE’ string trim(string p0) https://services.odata.org/Northwind/Northwind.svc/Customers?$filter=trim(CompanyName) eq ‘Alfreds Futterkiste’ string concat(string p0, string p1) https://services.odata.org/Northwind/Northwind.svc/Customers?$filter=concat(concat(City, ‘, ‘), Country) eq ‘Berlin, Germany’ Date Functions int day(DateTime p0) https://services.odata.org/Northwind/Northwind.svc/Employees?$filter=day(BirthDate) eq 8 int hour(DateTime p0) https://services.odata.org/Northwind/Northwind.svc/Employees?$filter=hour(BirthDate) eq 0 int minute(DateTime p0) https://services.odata.org/Northwind/Northwind.svc/Employees?$filter=minute(BirthDate) eq 0 int month(DateTime p0) https://services.odata.org/Northwind/Northwind.svc/Employees?$filter=month(BirthDate) eq 12 int second(DateTime p0) https://services.odata.org/Northwind/Northwind.svc/Employees?$filter=second(BirthDate) eq 0 int year(DateTime p0) https://services.odata.org/Northwind/Northwind.svc/Employees?$filter=year(BirthDate) eq 1948 Math Functions double round(double p0) https://services.odata.org/Northwind/Northwind.svc/Orders?$filter=round(Freight) eq 32d eq 32) decimal round(decimal p0) https://services.odata.org/Northwind/Northwind.svc/Orders?$filter=round(Freight) eq 32 double floor(double p0) https://services.odata.org/Northwind/Northwind.svc/Orders?$filter=round(Freight) eq 32d eq 32) decimal floor(decimal p0) https://services.odata.org/Northwind/Northwind.svc/Orders?$filter=floor(Freight) eq 32 double ceiling(double p0) https://services.odata.org/Northwind/Northwind.svc/Orders?$filter=ceiling(Freight) eq 33d eq 33) decimal ceiling(decimal p0) https://services.odata.org/Northwind/Northwind.svc/Orders?$filter=floor(Freight) eq 33 Type Functions bool IsOf(type p0) https://services.odata.org/Northwind/Northwind.svc/Orders?$filter=isof(&#39;NorthwindModel.Order&#39;) bool IsOf(expression p0, type p1) https://services.odata.org/Northwind/Northwind.svc/Orders?$filter=isof(ShipCountry, ‘Edm.String’) 格式 $format Value Response Media Type Atom application&#x2F;atom+xml Xml application&#x2F;xml Json application&#x2F;json Any other IANA-defined content type Any IANA-defined content type A service-specific value indicating a format specific to the specific OData service Any IANA-defined content type Examples https://services.odata.org/OData/OData.svc/Products?$format=atom odata分页Examples 1https://services.odata.org/OData/OData.svc/Products?$top=pageSize&amp;skip=(page-1)*pageSize&amp;$inlinecount=allpages $top 表示取多少条数据 $skip 表示跳过多少条数据 $inlinecount&#x3D;allpages 返回总记录数 __count","tags":["Odata"],"categories":["Odata"]},{"title":"Gson反序列化无法识别泛型的问题","path":"/posts/7ec6642a/","content":"Gson提供了fromJson() 和toJson() 两个直接用于解析和生成的方法，前者实现反序列化，后者实现了序列化。 今天在使用 fromJson()遇到一个无法识别泛型的问题。 我的需求是通过fromJson() 反序列化返回一个List。 主要代码如下： 12345678public List&lt;T&gt; select(T var) &#123; List&lt;T&gt; list = Lists.newArrayList(); ... ... JsonArray jsonArray = jsonObject.getAsJsonObject(&quot;d&quot;).getAsJsonArray(&quot;results&quot;); list = gson.fromJson(jsonArray, new TypeToken&lt;ArrayList&lt;T&gt;&gt;() &#123;&#125;.getType()); return list;&#125; T 的当前类： 12345678910111213141516171819@Getter@Setterpublic class AvsPerson &#123; /** * person */ @SerializedName(value = &quot;Person&quot;) private String person; /** * 业务员编号 */ @SerializedName(value = &quot;PersonWorkAgreement&quot;) private String personWorkAgreement; /** * 业务员姓名 */ @SerializedName(value = &quot;PersonFullName&quot;) private String personFullName;&#125; jsonArray的数据： 123456789101112131415161718192021&#123; &quot;__metadata&quot;: &#123; &quot;id&quot;: &quot;1&quot;, &quot;uri&quot;: &quot;2&quot;, &quot;type&quot;: &quot;3&quot; &#125;, &quot;Person&quot;: &quot;9980000020&quot;, &quot;PersonWorkAgreement&quot;: &quot;50000000&quot;, &quot;PersonFullName&quot;: &quot;Sarah Huang&quot;&#125;,&#123; &quot;__metadata&quot;: &#123; &quot;id&quot;: &quot;1&quot;, &quot;uri&quot;: &quot;2&quot;, &quot;type&quot;: &quot;3&quot; &#125;, &quot;Person&quot;: &quot;9980000021&quot;, &quot;PersonWorkAgreement&quot;: &quot;50000002&quot;, &quot;PersonFullName&quot;: &quot;Shanna Sun&quot;&#125; 反序列化后List中的值： 数据是已经到list中了，但是它不是我需要的那个AvsPerson类型，而是完全把jsonArray中的数据反序列化为一个Map。 DEBUG跟一下发现如下： 直接把T当成一个Type 了。。 因此我怀疑Gson没有识别到我使用的泛型类，所以直接当成 Object类型了，我测试把 T 换成Object是一样的结果，Type中则显示为Object类型了。 经过一番摸索，解决方案如下： 使用 Guava，它也是Google家的，直接引入依赖即可： 123456&lt;!-- https://mvnrepository.com/artifact/com.google.guava/guava --&gt;&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;27.0.1-jre&lt;/version&gt;&lt;/dependency&gt; 使用如下方法： 12345public &lt;T&gt; Type setModelAndGetCorrespondingList(Class&lt;T&gt; type) &#123; return new TypeToken&lt;ArrayList&lt;T&gt;&gt;() &#123;&#125; .where(new TypeParameter&lt;T&gt;() &#123;&#125;, type) .getType();&#125; 这里的 TypeToken 使用的是 Guava 下的包了，注意更换。 在代码中调用这个方法即可： 12345678public List&lt;T&gt; select(T var) &#123; List&lt;T&gt; list = Lists.newArrayList(); ... ... JsonArray jsonArray = jsonObject.getAsJsonObject(&quot;d&quot;).getAsJsonArray(&quot;results&quot;); list = gson.fromJson(jsonArray, classHelper.setModelAndGetCorrespondingList(var.getClass())); return list;&#125; 更改后再次 debug 后 Type 的类型正常了！ 得到的List内容也恢复正常： 具体的原因可以看这里：https://stackoverflow.com/questions/20773850/gson-typetoken-with-dynamic-arraylist-item-type","tags":["Gson"],"categories":["Java"]},{"title":"Gson反序列化无法处理时间戳的问题","path":"/posts/7c60c143/","content":"今天在使用Gson的时候遇到无法转换时间戳的问题。 代码如下： 123Gson gson = new Gson();String jsonStr = &quot;json字符串，省略。。&quot;;Person person = gson.fromJson(jsonStr, Person.class); 报错如下： 1234567891011121314com.google.gson.JsonSyntaxException: /Date(1552348800000)/\tat com.google.gson.internal.bind.DateTypeAdapter.deserializeToDate(DateTypeAdapter.java:87)\tat com.google.gson.internal.bind.DateTypeAdapter.read(DateTypeAdapter.java:75)\tat com.google.gson.internal.bind.DateTypeAdapter.read(DateTypeAdapter.java:46)\t.... ....\tat com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)\tat com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)Caused by: java.text.ParseException: Failed to parse date [&quot;/Date(1552348800000)/&quot;]: Invalid number: /Dat\tat com.google.gson.internal.bind.util.ISO8601Utils.parse(ISO8601Utils.java:274)\tat com.google.gson.internal.bind.DateTypeAdapter.deserializeToDate(DateTypeAdapter.java:85)\t... 31 moreCaused by: java.lang.NumberFormatException: Invalid number: /Dat 后面查询接口给的数据才发现，Odata服务给的时间格式都是这种时间戳格式，所以Gson无法自动完成转换。 123456&#123; &quot;Person&quot;: &quot;9980000020&quot;, &quot;PersonWorkAgreement&quot;: &quot;50000000&quot;, &quot;PersonFullName&quot;: &quot;Sarah Huang&quot;, &quot;CreateDate&quot;:&quot;/Date(1552348800000)/&quot;&#125; 经过一番摸索，解决方案如下： 给 Gson 注册一个处理时间戳的适配器，当字段为日期类型时会按时间戳处理。因此如果还有其它日期格式那就无法转换了！如：2019-06-17 12:00:01 这种格式，使用了这个适配器后就无法处理这种日期了！ 123456789101112131415//创建GsonBuilder 管理收到的信息GsonBuilder builder = new GsonBuilder();//注册适配器以将日期类型作为长值进行管理builder.registerTypeAdapter(Date.class, new JsonDeserializer&lt;Date&gt;() &#123; @Override public Date deserialize(JsonElement json, Type typeOfT, JsonDeserializationContext context) throws JsonParseException &#123; String dateStr = json.getAsJsonPrimitive().getAsString(); long timestamp = Long.parseLong(dateStr.substring(6, dateStr.length() - 2)); Date d = new Date(timestamp); return d; &#125;&#125;);Gson gson = builder.create();String jsonStr = &quot;json字符串，省略。。&quot;;Person person = gson.fromJson(jsonStr, Person.class); 更改后反序列化成功。 转换出来的时间格式如下： 1Person(person=9980000020, personWorkAgreement=50000000, personFullName=Sarah Huang, createDate=Tue Mar 12 08:00:00 GMT+08:00 2019)","tags":["Gson"],"categories":["Java"]},{"title":"SpringMVC中参数绑定常用注解","path":"/posts/d804aaaf/","content":"不积跬步，无以至千里。 不积小流，无以成江海。 今天在写一个Ajax请求到Controller时被参数接收的问题耽误了很久。随后详细查了SpringMVC中Controller参数绑定注解，整理记录如下。 在写SpringMVC参数绑定注解前，先看看该如何使用Ajax请求。 Ajax中Content-TypeHTTP请求中使用 Content-Type 这个字段来表示报文主体的对象类型。下面是常用的 Content-Type application&#x2F;x-www-form-urlencoded 这是最常用的POST提交数据的方式。Ajax中不设置Content-Type则默认以这种方式提交数据。 数据格式以key=value&amp;key2=value2 的方式进行编码，作为一个FormData对象进行发送。 multipart&#x2F;form-data 在使用表单进行上传文件时使用这种方式。这里不详细展开。 application&#x2F;json 这种方式现在很常用，它表示数据是序列化的JSON字符串。通常配合 JSON.stringify() 将一个对象转化成JSON字符串。 text&#x2F;xml 表示数据是XML的格式。data中传递的数据需要符合XML格式。 常用注解在接收请求中的参数时我们经常使用以下注解，可以根据它们处理的Request不同部分分类： 处理Request Uri (指Uri中的Variable，不包括拼接在URL中的 QueryString ) @PathVariable 处理Request Header @RequestHeader @CookieValue 处理Request Body @ReuqestParam @RequestBody 处理 Attribute 类型 @SessionAttributes @ModelAttribute @PathVariable示例： 12@PostMapping(value = &quot;/&#123;userId&#125;/roles&quot;)public void queryUserAndRoles(HttpServletRequest request, @PathVariable Long userId) &#123;&#125; @RequestHeader从名字可以知道它是获取Request的Header部分字段。 示例： 一个Request Header: 1234567POST /dwp/contract/rebuildpdf HTTP/1.1Host: localhost:8080Connection: keep-aliveContent-Length: 20Pragma: no-cacheCache-Control: no-cacheAccept: application/json, text/javascript, */*; q=0.01 在Controller中获取Request Header中的 Host 与 Content-Length 字段值 12@PostMapping(value = &quot;/dwp/contract/headerInfo&quot;)public void headerInfo(@RequestHeader(&quot;Host&quot;) String Host, @RequestHeader(&quot;Content-Length&quot;) String ContentLength) &#123;&#125; @CookieValue获取Request Header 中 Cookie 中的值。 示例： 有如下Cookie： 1Cookie: _ga=GA1.1.54311297.1548073822; Webstorm-244b1d3=90a9034f-e9c9-4a89-86a7-aaef4addf18e; Idea-4ae3b438=0eb7fd0e-54c2-4672-960c-36b458508707; Webstorm-ead79411=ce4860a6-bbd1-487d-900b-3b8fd9825418; Hm_lvt_3a101c3aa7d1cde834d9d6b197500902=1557208148,1559381054; loginKey=cbe4e8d0-059e-4bd3-82f9-5360e570839a; 在Controller中获取Cookie中的loginKey的值： 12@RequestMapping(value = &quot;/dwp/contract/rebuildpdf&quot;)public void rebuildpdf(HttpServletRequest request, @CookieValue(&quot;loginKey&quot;) String cookieLoginKey)&#123;&#125; @ReuqestParamrequestParam有两种情况： 1、处理简单类型绑定 可以处理使用request.getParameter()获取的String，或Get方式中的queryParams ，和POST 方式中 form data的值 示例： Ajax： 123456$.ajax(&#123; url:_basePath + &#x27;/dwp/contract/rebuildpdf?contractTempNumber=&#x27; + $(&#x27;#contractTempNumber&#x27;).val(), type: &quot;GET&quot;, dataType: &quot;json&quot;, contentType: &quot;application/json&quot;,&#125;); Controller: 123@RequestMapping(value = &quot;/dwp/contract/rebuildpdf&quot;)@ResponseBodypublic ResponseData rebuildpdf(HttpServletRequest request, @RequestParam String contractTempNumber)&#123;&#125; 2、处理表单数据 获取Request中的formData参数。因此Ajax请求时Content-Type 必须是 application/x-www-form-urlencoded ,data中传的是对象。 示例： Ajax： 12345678$.ajax(&#123; url:_basePath + &#x27;/dwp/contract/rebuildpdf&#x27;, type: &quot;POST&quot;, dataType: &quot;json&quot;, data: &#123; contractTempNumber: $(&#x27;#contractTempNumber&#x27;).val() &#125;&#125;); Controller: 12@RequestMapping(value = &quot;/dwp/contract/rebuildpdf&quot;)public void rebuildpdf(HttpServletRequest request, @RequestParam String contractTempNumber)&#123;&#125; @RequestBody获取Request 的Body 示例： Ajax： 使用JSON.stringify 把对象转化成JSON字符串 1234567$.ajax(&#123; url:_basePath + &#x27;/dwp/contract/rebuildpdf&#x27;, type: &quot;POST&quot;, dataType: &quot;json&quot;, contentType: &quot;application/json&quot;, data: JSON.stringify(&#123;contractTempNumber: $(&#x27;#contractTempNumber&#x27;).val()&#125;)&#125;); Controller: 对象DwpContracts中有contractTempNumber 这个属性字段。 12@RequestMapping(value = &quot;/dwp/contract/rebuildpdf&quot;)public void rebuildpdf(HttpServletRequest request, @RequestBody DwpContracts contract)&#123;&#125; @SessionAttributes该注解用来绑定HttpSession中的attribute对象的值，便于在方法中的参数里使用。该注解有value、types两个属性，可以通过名字和类型指定要使用的attribute 对象； 示例： 123456@Controller@RequestMapping(&quot;/editPet.do&quot;)@SessionAttributes(&quot;pet&quot;)public class EditPetForm &#123; // ...&#125; @ModelAttribute该注解有两个用法，一个是用于方法上，一个是用于参数上； 用于方法上时： 通常用来在处理@RequestMapping之前，为请求绑定需要从后台查询的model； 用于参数上时： 用来通过名称对应，把相应名称的值绑定到注解的参数bean上；要绑定的值来源于： A） @SessionAttributes 启用的attribute 对象上； B） @ModelAttribute 用于方法上时指定的model对象； C） 上述两种情况都没有时，new一个需要绑定的bean对象，然后把request中按名称对应的方式把值绑定到bean中。 用到方法上@ModelAttribute的示例代码： 123456789// Add one attribute// The return value of the method is added to the model under the name &quot;account&quot;// You can customize the name via @ModelAttribute(&quot;myAccount&quot;) @ModelAttributepublic Account addAccount(@RequestParam String number) &#123; return accountManager.findAccount(number);&#125; 用在参数上的@ModelAttribute示例代码： 1234@RequestMapping(value=&quot;/owners/&#123;ownerId&#125;/pets/&#123;petId&#125;/edit&quot;, method = RequestMethod.POST)public String processSubmit(@ModelAttribute Pet pet) &#123;&#125; 首先查询 @SessionAttributes有无绑定的Pet对象，若没有则查询@ModelAttribute方法层面上是否绑定了Pet对象，若没有则将URI template中的值按对应的名称绑定到Pet对象的各属性上。 参考：https://blog.csdn.net/walkerjong/article/details/7946109","tags":["SpringMVC"],"categories":["Spring"]},{"title":"DDD及开发模式对比","path":"/posts/156e5ee9/","content":"不积跬步，无以至千里。 不积小流，无以成江海。 转载自：Jiangzhou.bo 本文总结常用开发模式，简述DDD领域驱动设计，基于DEMO整理出后台架构模式。 分层模式从大的范围来分，软件可以分为两个层次：前端和后台。前端负责与用户进行交互，负责接收和校验用户输入，并向用户反馈输出，其业务操作是委托给后台来实现的。我们平常见到很多分层架构模式，核心目的都是分层、解耦。 MVC模式MVC(Model-View-Controller)，即数据模型-视图-控制器，MVC 是开发客户端最经典的设计模式。MVC这个概念最早出现在桌面客户端上面，是C&#x2F;S里面的C，在Web开发中得以发扬光大，实际上，移动App中也几乎都是MVC模式的。 在客户端开发中，Controller管理用户输入、输出和图形界面。数据来自Model部分，Model实际上集中管理了业务数据，是通向后台系统的通道。这使对同一种业务数据展现多种图形界面成为了可能，Controller存在的目的则是确保M和V的同步，一旦M改变，V应该同步更新。 后端最典型的MVC就是JSP + servlet + javabean的模式。当用户发出一个请求后，这个请求会被控制器Servlet接收到；Servlet将请求的数据转换成数据模型JavaBean，然后调用业务逻辑模型JavaBean的方法，并将业务逻辑模型返回的结果放到合适的地方，比如请求的属性里；最后，根据业务逻辑模型的返回结果，由控制器来选择合适的视图(JSP)，由视图把数据展现给用户。 在很多开源框架中也有MVC设计的体现，如Struts2、SpringMVC等。就单从SpringMVC框架来说，DispatcherServlet是前端控制器，是整个流程控制的中心，由它调用其它组件处理用户的请求，相当于MVC的Controller；Handler(即我们开发的Controller)是继DispatcherServlet前端控制器的后端控制器，Handler对具体的用户请求进行处理，并返回ViewModel；最后由ViewResolver负责将处理结果生成View视图。 在Javaweb开发中，MVC框架充当了UI层和业务逻辑层的适配器的作用，MVC框架实现了UI层和业务逻辑层最大程度的分离。 三层架构现在开发中常用的分层模式就是三层架构，这也是比较传统的一种架构模式。通常意义上的三层架构就是将整个业务应用划分为：表现层（UI）、业务逻辑层（BLL）、数据访问层（DAL）。区分层次的目的即为了“高内聚，低耦合”。 表现层：负责与用户进行交互；业务逻辑层：主要是针对具体的问题的操作，也可以理解成对数据层的操作，对数据业务逻辑处理；数据访问层：直接操作数据库，针对数据的增、删、改、查等。 对应到开发中，即常用的 Controller – Service – Dao 三层。Controller层为控制层，用来接收用户的请求，不会涉及太多的业务处理操作，会做一些简单的数据校验，业务处理完毕返回数据模型或视图。Controller层中拥有某一个Service层的引用，但凡涉及到业务处理，就交给Service层来操作。Service一般持有某一个或几个Dao层的引用来对数据做处理。一般来说，Service层和Dao层中，都是直接存放的接口类，然后有一个包放所有接口的实现类，impl就是指每个接口对应的实现类。 联系这三层之间的就是实体对象，比如，使用一个DTO映射数据表，Dao层返回的数据对象(DO)、Service层处理的业务对象(BO)、Controller层返回给用户的展示对象(VO)、甚至远程接口数据对象(DTO)都由一个DTO对象完成，这种模式一般在中小型项目中用得比较多，毕竟分层过多会引入复杂性。 三层架构与MVC我们经常将三层架构与MVC混为一谈，但是它俩并不是一个概念。三层架构是一个分层式的软件体系架构设计，它可适用于任何一个项目。MVC是一个设计模式，它是根据项目的具体需求来决定是否适用于该项目。三层架构的着重点是“高内聚，低耦合”，MVC的目的则是实现Web系统的职能分工，即职责划分。但它们的总体目的是一样的，都是为了解耦。 三层架构的表现层即客户端，一般是基于MVC模式开发，使用诸如AngularJS、Vue、React等前端框架。数据交互使用JSON。 DDD 领域驱动设计DDD(DomainDriven Design)，领域驱动设计，作为一种软件开发方法，它可以帮助我们设计高质量的软件模型。在正确实现的情况下，我们通过DDD完成的设计恰恰就是软件的工作方式。在实施DDD时，设计就是代码，代码就是设计。 DDD中重要的概念 通用语言(UL)：通用语言是团队自己创建的公用语言，团队中同时包含领域专家和软件开发人员，可以看成业务顾问和技术顾问。我们使用通用语言来捕捉特定核心业务领域中的概念和术语，比如店铺运营、商场促销。 限界上下文(BC)：限界上下文是一个显示的边界，领域模型便存在于这个边界之内。这个边界之内的每种领域术语、词组或句子就组成了通用语言，都有确定的上下文含义。但是，限界上下文并不只局限于容纳模型，它通常标定了一个系统、一个应用程序或者一种业务服务。限界上下文中可以包含模块、聚合、领域事件、领域服务等基础部件，限界上下文应该足够大，以能够表达它所对应的整套通用语言。 在使用Java时，限界上下文可以看成IDE中的一个工程项目；顶层包名通常表示界限上下文中顶层模块的名字。我们可能将一个限界上下文放在一个jar或者war文件中。对于大型模型，可以将松耦合的领域模型放在不同的jar文件中，这样我们可以按照版本号对领域模型进行单独部署，这其实就可以看成单独的组件、微服务了。 通用语言和限界上下文同时构成了DDD的两大支柱，并且他们是相辅相成的。限界上下文和通用语言存在一对一的关系。 领域：Domain，即一个组织所做的事情以及其中包含的一切。当我们为某个组织开发软件时，所面对的便是这个组织的领域。一个领域被分为若干子域，几乎所有软件的领域都包含多个子域。 子域：我们可以按照实际功能将领域中交织的模型划分成逻辑上相互分离的子域，从而在一定程度上减少系统的复杂性。子域不一定要包含很多功能，简单的子域可以以模块的形式存在。子域又分核心域、支撑子域、通用子域。 核心域：是整个业务领域的一部分，也是业务成功的主要促成因素，系统核心竞争力的体现。从战略层面上讲，企业应该在核心域上胜人一筹。 支撑子域：支撑子域专注于业务的某个方面。 通用子域：通用子域被用于整个业务系统。像用户和权限就可以放到通用子域。 领域模型：领域模型是关于某个特定业务领域的软件模型。通常，领域模型通过对象模型来实现，这些对象同时包含了数据和行为，并且表达了准确的业务含义。领域模型在限界上下文中完成开发，在开发一个领域模型时，我们关注的通常只是这个业务系统的某个方面。 问题空间：在问题空间，我们思考的是业务所面临的挑战。问题空间是领域的一部分，对问题空间的开发将产生一个新的核心域，问题空间是核心域和其它子域的组合。 解决方案空间：在解决方案空间，我们思考如何实现软件以解决问题空间的业务挑战。解决方案空间包括一个或多个界限上下文，即一组特定的软件模型。 上下文映射图： 由多个界限上下文和子域组成的表示当前单个领域或者多个领域之间的集成关系图。上下文映射图主要帮助我们从解决方案空间的角度看待问题。 贫血领域对象：指领域对象主要是些公共的getter&#x2F;setter方法，几乎没有业务逻辑，主要就是用来容纳属性值的对象，比如我们之前使用的DTO。这种对象一般不叫领域对象，只是将关系型数据库中的模型映射到了对象上而已。反之，充血模型就是DDD中的领域模型。 贫血症导致的失忆症：例如，有一个保存店铺的功能，saveStore(Store dto)，Store关联有合同、员工、经营证照等，现在的开发模式中，仅仅修改了店铺的名称或者备注属性，会调用这个保存店铺的方法；在新建店铺时，新增了合同关系、店铺员工，也会在saveStore里保存合同关系、店铺员工。这样一来，我们并没有正确的使用saveStore方法，他的职责范围过大，业务意图不明确了，方法的实现本身就增加了潜在的复杂性，时间久了，我们可能会被这段代码搞得一头雾水。说白了就是业务过于集中，不同模型之间耦合度过高，甚至分不清职责边界，导致后期无法维护。 DDD架构DDD四层架构 DDD系统所采用的传统分层架构，核心域位于领域层，上层为用户界面层和应用层，下层是基础设施层。分层架构的一个重要原则是：每层只能与位于其下方的层发生耦合。分层架构有严格分层架构和松散分层架构，严格分层架构中，某层只能与直接位于其下方的层发生耦合；松散分层架构则允许任意上层与任意下层发生耦合。由于用户界面层和应用服务通常需要与基础设施打交道，许多系统都是基于松散分层架构的。 用户接口层：这里指的用户可以是另一个计算机系统，也可以是使用用户界面的人。它不应该包含领域或业务逻辑，如果用户界面使用了领域模型中的对象，那么此时的领域对象仅限于数据的渲染展现，可以使用展现模型对用户界面和领域对象进行解耦。 应用服务：应用服务位于应用层中，用户界面和面向服务端点都会将操作委派给应用服务。应用服务是很轻量的，主要负责用例流的任务协调，本身并不处理业务逻辑，可以用于控制事务和安全认证，或者向其他系统发送基于事件的消息通知，还可以用于创建邮件以发送给用户等等。如果应用服务过于复杂，就要考虑领域逻辑是否已经渗透到应用服务中了。有时，应用服务被设计成将用户界面完全地隔离于领域模型，此时，应用服务中的方法签名中将只出现最基本的数据类型(int，long，double，String等)，有可能还有DTO。在不使用领域对象返回时，我们避免了依赖和耦合，此时我们可以使用DTO，并提供转换器，虽然会导致一定的内存耗费，但避免了领域对象与不同系统间的耦合。 领域服务：领域服务表示一个无状态的操作，它用于实现特定于某个领域的任务。当某个操作不是实体(领域对象)的职责时，最好的方式就是使用领域服务，我们应该尽量避免在实体中使用资源库。要避免所有的业务逻辑都位于领域服务中，否则可能导致贫血领域模型。一般来说，我们不需要为领域服务设计独立的接口，直接使用实现类即可，如果确实针对不同的租户有不同的实现标准，可以使用独立接口。因为通常情况下，领域服务总是和领域密切相关，并且不会有技术性的实现，或者不会有多个实现，此时采用独立接口便只是一个风格上的问题。虽然独立接口对于解耦来说是有用处的，此时客户端只需要依赖于接口，而不需要知道具体实现。但是，如果我们使用了依赖注入或工厂，即便接口和实现类是合并在一起的，我们依然能达到这样的目的。 基础设施层：在传统的分层架构中，基础设施层位于底层，持久化和消息机制便位于该层中，包括持久化、消息、邮件等。可以将基础设施层中所有的组件和框架看作是应用程序的底层服务，较高层与该层发生耦合以重用技术上的基础设施。 总的来说DDD就是从以数据库为中心过度到以领域模型为中心，将侧重点从效率改变为维护。从长远的角度看，以领域模型为中心的设计更加清晰，也是一种更忠实于领域抽象的实现，因而可维护性更高。 六边形架构（端口与适配器） 六边形架构也称为端口与适配器。对于每种外界类型，都有一个适配器与之对应。在这种架构中，不同的客户端通过“平等”的方式与系统交互。该架构中存在两个区域，分别是“外部区域”和“内部区域”。在外部区域中，不同的客户(系统、用户等)均可以提交输入；而内部的系统则处理数据相关。为了保证领域模型所在的应用程序的干净简洁和自治性，各种适配器作为防腐层在整个程序的最外层保护着当前的界限上下文不受外部入侵。 从图中可以看出，每种类型的客户都有他自己的适配器，该适配器用于将客户输入转为为程序内部API所能理解的输入。六边形的每条不同的边代表了不同类型的端口，可以将端口想成是HTTP，而将适配器想成Controller用来请求的类。新的客户只需要添加一个新的适配器将客户输入转化成能被系统API所理解的参数就行了。对于每种特定的输出，都有一个新建的适配器负责完成相应的转化功能。 CQRS——命令和查询职责分离从资源库中查询所有需要显示的数据是困难的，特别是在需要显示来自不同聚合类型与实例的数据时。我们需要从不同的资源库获取聚合实例，然后再将这些实例数据组装成一个数据传输对象(DTO)。 CQRS强调一个方法要么是执行某种动作的命令，要么是返回数据的查询，而不能两者皆是。如果一个方法修改了对象的状态，该方法便是一个命令，它不应该返回数据，这样的方法应该声明为void。如果一个方法返回了数据，该方法便是一个查询，此时它不应该通过直接或间接的手段修改对象的状态，这样的方法应该以其返回的数据类型进行声明。 CQRS指导我们将领域模型中包含命令和查询的聚合拆分开，将那些纯粹的查询功能从命令功能中分离出来，最终分为命令模型和查询模型。 CQRS架构本身只是一个读写分离的思想，实现方式多种多样，比如数据存储不分离，仅仅只是代码层面读写分离，也是CQRS的体现；数据存储的读写分离，C端负责数据存储，Q端负责数据查询。这种架构方式有待深入研究。 DDD总结微服务架构在微服务架构实践中，人们大量地使用了DDD中的概念和技术： 微服务中应该首先建立UL，然后再讨论领域模型。 一个微服务最大不要超过一个BC，否则微服务内会存在有歧义的领域概念。 一个微服务最小不要小于一个聚合，否则会引入分布式事务的复杂度。 微服务的划分过程类似于BC的划分过程，每个微服务都有一个领域模型。 微服务间的集成可以通过ACL。 微服务间最好采用Domain Event（领域事件）来进行交互，使得微服务可以保持松耦合。 DDD所带来的业务价值 你获得了一个非常有用的领域模型 你的业务得到了更准确的定义和理解 领域专家可以为软件设计做出贡献 更好的用户体验 清晰的模型边界 更好的企业架构 敏捷、迭代式和持续建模 使用战略和战术工具 DDD所带来的挑战 为创建通用语言腾出时间和精力 持续地将领域专家引入项目 改变开发者对领域的思考方式","tags":["开发模式"],"categories":["技术杂谈"]},{"title":"使用docker-compose编排微服务","path":"/posts/625295f9/","content":"上篇 使用Maven构建微服务的Docker镜像 写了如何构建微服务的镜像及运行镜像。但往往我们整个微服务架构中会有几十个甚至几百个微服务，我们不可能都使用手动去启停，那样效率很低，维护量也很大。 因此我们需要一个自动化的工具帮助我们管理容器。本篇使用的是docker-compose 。 安装 Compose 支持 Linux、macOS、Windows 10 三大平台. Compose 可以通过 Python 的包管理工具 pip 进行安装，也可以直接下载编译好的二进制文件使用，甚至能够直接在 Docker 容器中运行。 我是Centos 7因此我使用的是二进制包的形式安装。使用如下命令进行安装： 12$ sudo curl -L https://github.com/docker/compose/releases/download/1.17.1/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose$ sudo chmod +x /usr/local/bin/docker-compose 安装完成后检查docker-compose版本OK就好了 12[root@HJWDEV ~]# docker-compose -vdocker-compose version 1.23.1, build b02f1306 编排微服务使用docker-compose编排微服务大致步骤如下： 1、编写各服务的Dockerfile 2、编写docker-compose.yml 定义组成应用程序的服务及各服务的依赖，网络配置等 3、运行docker-compose up 命令，启动并运行整个项目 本例项目源码使用之前做过的项目，把Eureka，provider-user，consumer-movie进行编排。源码查看：microservice-discovery-eureka、microservice-simple-provider-user、microservice-simple-consumer-movie 目录结构如下： 12345678910111213141516171819202122232425262728293031323334353637383940├─.idea│ ├─inspectionProfiles│ └─libraries├─microservice-discovery-eureka│ ├─src│ │ └─main│ │ ├─java│ │ │ └─com│ │ │ └─hjwzyy│ │ │ └─META-INF│ │ └─resources| └─Dockerfile| └─pom.xml├─microservice-simple-consumer-movie│ ├─src│ │ └─main│ │ ├─java│ │ │ └─com│ │ │ └─hjwzyy│ │ │ ├─contorllers│ │ │ └─pojo│ │ └─resources│ │ ├─static│ │ └─templates| └─Dockerfile| └─pom.xml├─microservice-simple-provider-user│ ├─src│ │ └─main│ │ ├─java│ │ │ └─com│ │ │ └─hjwzyy│ │ │ ├─contorllers│ │ │ ├─dao│ │ │ └─pojo│ │ └─resources| └─Dockerfile| └─pom.xml├─ docker-compose.yml├─ pom.xml Dockerfile这三个微服务的Dockerfile文件如何编写可以参考前面的博客：使用Maven构建微服务的Docker镜像 这里不再赘述。 docker-compose.ymldocker-compose.yml 文件的编写是关键。 12345678910111213141516171819202122232425262728version: &#x27;2&#x27; # docker-compose 版本services: eureka: # 服务名 build: context: ./microservice-discovery-eureka # Dockerfile 文件所在路径 dockerfile: Dockerfile # Dockerfile 文件名 args: JAR_FILE: target/microservice-discovery-eureka-0.0.1-SNAPSHOT.jar # 构建参数传递 ports: # 开放端口 - &quot;8761:8761&quot; microservice-simple-provider-user: build: context: ./microservice-simple-provider-user dockerfile: Dockerfile args: JAR_FILE: target/microservice-simple-provider-user-0.0.1-SNAPSHOT.jar ports: - &quot;8010:8010&quot; microservice-simple-consumer-movie: build: context: ./microservice-simple-consumer-movie dockerfile: Dockerfile args: JAR_FILE: target/microservice-simple-consumer-movie-0.0.1-SNAPSHOT.jar ports: - &quot;8011:8011&quot; depends_on: # 依赖 - microservice-simple-provider-user 以上是使用最简单的方式进行配置。 build : 使用 build 并指定Dockerfile ，docker-compose 会根据Dockerfile 构建镜像。这里使用了args 传入了Dockerfile 构建镜像时需要的参数，前面通过 maven 构建镜像时，这个参数是从pom文件中传入的。这里的构建不通过 maven ，因此需要单独在docker-compose.yml 中传入args，否则会由于缺少参数而构建镜像失败。 depends_on : 表示这个服务依赖于某个服务，如microservice-simple-consumer-movie 是依赖 microservice-simple-provider-user 的，provider 启动了，consumer 才能访问到数据。因此如果配置了依赖的服务，docker-compose 会先启动被依赖的服务。 启动上面的步骤做完之后，就可以启动Docker-compse了。只需要简单的一条命令： 1docker-compose up docker-compose 会根据docker-compose.yml 配置的内容进行镜像的构建并启动。 docker-compose 配置Docker-compose 的具体用法 及 yml 文件的具体配置可以参考 : https://docker_practice.gitee.io/compose/compose_file.html","tags":["docker-compose"],"categories":["微服务"]},{"title":"使用Maven构建微服务的Docker镜像","path":"/posts/719142df/","content":"在预习了Docker的知识后，开始对微服务进行Docker容器化改造。 本篇内容前提： 已安装 Docker 的服务器环境 Docker 基础操作 Docker 的基础学习笔记可以在本博客 Docker 分类中查看。 我使用 Maven 来构建 Docker 镜像。Maven 有几个Docker 插件可以使用，这里使用的是 由Spotify 公司开发的 Maven 插件。 插件名称：dockerfile-maven 地址：https://github.com/spotify/dockerfile-maven 在它前面还有一个”哥哥” docker-maven-plugin 这个插件已经不推荐使用了。docker-maven-plugin 插件可以不使用Dockerfile 而直接在pom.xml中来构建镜像，但随着时间的推移Spotify公司也意识到这种方式可能导致很多不必要的混淆，使用Dockerfile构建镜像是最简单的方法，因此但这种方式不推荐,这个项目也不会再增加新功能，仅限错误修复。所以我一开始就学习的dockerfile-maven 插件。它会连接远程Docker，只要一个命令就能把本地的jar包打成Docker镜像，命令执行完毕后，服务器上就会有刚打包好的镜像，此时再执行该镜像即可。 下面使用dockerfil-maven来构建镜像，以前面的 microservice-consumer-movie 项目为例。 添加属性在添加插件前先在pom.xml中添加一些属性，方便配置，在pom中添加如下properties： 12&lt;docker.image.prefix&gt;hjwjw&lt;/docker.image.prefix&gt;&lt;docker.plugin.version&gt;1.4.9&lt;/docker.plugin.version&gt; 添加插件在项目的pom.xml中添加dockerfile-maven插件 123456789101112131415161718192021&lt;plugin&gt; &lt;groupId&gt;com.spotify&lt;/groupId&gt; &lt;artifactId&gt;dockerfile-maven-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;docker.plugin.version&#125;&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;default&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;build&lt;/goal&gt; &lt;!--&lt;goal&gt;push&lt;/goal&gt;--&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;repository&gt;$&#123;docker.image.prefix&#125;/$&#123;project.artifactId&#125;&lt;/repository&gt; &lt;tag&gt;$&#123;project.version&#125;&lt;/tag&gt; &lt;buildArgs&gt; &lt;JAR_FILE&gt;target/$&#123;project.build.finalName&#125;.jar&lt;/JAR_FILE&gt; &lt;/buildArgs&gt; &lt;/configuration&gt;&lt;/plugin&gt; 插件配置说明 repository : 用于命名构建镜像的存储库，生成的镜像名如：hjwjw/microservice-discovery-eureka。 tag : 是标签名称，这里指定标签名为pom中的$&#123;project.version&#125; ，这里即是：0.0.1-SNAPSHOT buildArgs : 为构建参数，可以在构建里将参数传到Dockerfile中，$&#123;project.build.finalName&#125; 表示打包后jar包的名称。这里是将jar的路径传递到Dockerfile中。 executions : 这里设置把插件的goal 绑定到某个phase 上，实现打包完成后自动构建镜像并推送，这里没有配置远程仓库因此不推送到远程仓库。 phase 和 goal 可以这样理解：maven 命令格式是：mvn phase:goal ,例如mvn package dockerfile:build .那么，package 和 dockerfile 都是 phase ,build则是 goal 。 添加Dockerfile在项目根目录添加Dockerfile 文件，内容如下： 1234567FROM livingobjects/jre8VOLUME /tmpARG JAR_FILEADD $&#123;JAR_FILE&#125; app.jarRUN bash -c &#x27;touch /app.jar&#x27;EXPOSE 8010ENTRYPOINT [&quot;java&quot;,&quot;-Djava.security.egd=file:dev/./urandom&quot;,&quot;-jar&quot;,&quot;/app.jar&quot;] livingobjects&#x2F;jre8 是一个比较小的jre8的基础镜像，构建镜像时会快些。 Dockerfile 的参数在 Dockerfile实践 中已经写过，可以在本博客搜索查看。 12ARG JAR_FILEADD $&#123;JAR_FILE&#125; app.jar 表示接收构建参数JAR_FILE 这个参数在前面的pom.xml中配置插件时已经添加。参数内容为jar 的的路径，$&#123;JAR_FILE&#125;则是使用传递进来的参数。 开启Docker远程开启Docker远程方法可参考博客：Docker安装配置 开启后在本机上配置一个环境变量： 1DOCKER_HOST=tcp://ip:2375 IP 为Docker 所安装的服务器IP ，端口一般没有修改过默认会是2375 dockerfile-maven 插件就是通过在本机操作 Docker api 构建镜像的，因此本机上不需要安装Docker，能够访问到Docker服务器即可。 构建image前面的配置都完成后，在IDE的命令窗口只需要一个命令即可实现对项目的打包及构建镜像到Docker服务器。 1mvn package 执行命令过程如图： 可以看到按Dockerfile一步步构建镜像了，执行完成后我们就可以在Docker服务器上查看新的image，如下： 至此，Docker 构建微服务镜像完成。源码： microservice-customer-movie 镜像运行可以参考本博客分类：Docker 镜像tag在使用上面的方式生成镜像时，我们前面在pom.xml 中已经配置了构建时的tag： 1&lt;tag&gt;$&#123;project.version&#125;&lt;/tag&gt; 如果不配置这个参数，默认生成的tag 是 latest。 有时我们想修改成自己的tag ，可以通过dockerfile:tag 修改，但需要先把 pom.xml 中指定的tag 配置去掉。在pom.xml中删除如下： 1&lt;tag&gt;$&#123;project.version&#125;&lt;/tag&gt; 接着使用如下maven 命令： 1mvn dockerfile:tag -Ddockerfile.tag=config 执行完成后Docker 服务器上会出现一个基于hjwjw/microservice-consumer-movie:0.0.1-SNAPSHOT镜像的tag 为config 的新镜像。 同样的，如果在pom中没有指定tag，在使用dockerfile:build 构建新镜像时也可以指定tag名： 1mvn dockerfile:build -Ddockerfile.tag=consumer","tags":["Dockerfile"],"categories":["微服务"]},{"title":"Git合并指定commits","path":"/posts/2fce1aeb/","content":"在使用 git 时我们可能会遇到这样的需求： 想要合并某个分支下的某个特定的 commits 如我想把 feature 分支上的 commit eef67189e9cc73f9bcb4af416ae8248f8e65dca4 合并到我的主分支中。这时我可以使用 git cherry-pick 命令来实现。 首先切换到主分支,然后使用命令加上指定commit 的Revision Number 12git checkout mastergit cherry-pick eef67189e9cc73f9bcb4af416ae8248f8e65dca4 操作完后在master 分支上会出现一个新的commit。这样就可以实现我们上面的需求了。 有时可能不只需要合并一个commit 而是一系列相连的 commits，这时我们可以使用rebase 会更适合。 如我需要把featrue 分支的 commit 992e6c 到 dc40bb 合并到主分支. 首先需要基于featrue 创建一个新的分支，并指明最后一个commit 1git checkout -b newbranch dc40bb 然后 rebase 这个新分支的commit 到 master 1git rebase --onto master 992e6c^ 992e6c ^ 表示从哪个特定的commit 开始 。 这样就实现了我的第二个需求。","tags":["TortoiseGit"],"categories":["TortoiseGit"]},{"title":"Java中执行bat或shell命令","path":"/posts/c691c44f/","content":"不积跬步，无以至千里。不积小流，无以成江海。 今天为了博客能实现自动部署，写了一个Java小程序调用cmd命令来实现自动部署hexo并备份博客。 java中提供了两种方式来调用exe或shell程序： 使用Runtime.getRuntime().exec() 使用new ProcessBuilder().start() 下面我在实际使用中的代码如下，这里使用的是new ProcessBuilder().start()的方式，调用cmd执行命令。 1234567891011121314151617181920212223public String deployHexo() throws Exception&#123; String cmd = &quot;cd &quot; + HEXO_DIR + &quot; &amp;&amp; hexo clean &amp;&amp; hexo g &amp;&amp; hexo d&quot;; List&lt;String&gt; cmds = new LinkedList&lt;String&gt;();// cmds.add(&quot;sh&quot;); cmds.add(&quot;cmd.exe&quot;);// cmds.add(&quot;-c&quot;); cmds.add(&quot;/c&quot;); cmds.add(cmd); ProcessBuilder pb = new ProcessBuilder(cmds); //重定向到标准输出 pb.redirectErrorStream(true); Process p = pb.start(); p.waitFor(3, TimeUnit.SECONDS); BufferedReader br = new BufferedReader(new InputStreamReader(p.getInputStream())); StringBuffer sb = new StringBuffer(); String line; while ((line = br.readLine()) != null) &#123; sb.append(line).append(&quot; &quot;); &#125; String result = sb.toString(); return result; &#125; 这个方法会在调用后返回命令行处理的输出结果。如果是在Linux平台中调用，则把上面的5 -8行的命令改成如下即可： 1234 cmds.add(&quot;sh&quot;);// cmds.add(&quot;cmd.exe&quot;); cmds.add(&quot;-c&quot;);// cmds.add(&quot;/c&quot;); 对Runtime.getRuntime().exec()方式的使用与上面的代码差别不大，将第4-10行替换成如下，另外可删除第12行即可： 12String[] cmdArray =&#123;&quot;cmd.exe&quot;,&quot;/c&quot;,cmd&#125;;Process p = Runtime.getRuntime().exec(cmdArray); 我们使用这两种方式执行命令时，都会返回一个Process 并创建一个子线程。在Jdk文档中对它有下列描述： 默认情况下，创建的子进程没有自己的终端或控制台。 其所有的标准I/ O（即标准输入，标准输出，标准错误）操作将被重定向到父进程，在那里他们可以经由使用所述方法获得的流进行访问getOutputStream()，getInputStream()和getErrorStream()。 父进程使用这些流将输入提供给子进程并从子进程获取输出。 因为一些本地平台只为标准输入和输出流提供有限的缓冲区大小，因此无法及时写入输入流或读取子进程的输出流可能导致子进程阻塞甚至死锁。 我在使用过程中也遇到了进程阻塞的情况，因为缓冲区满了后，没有对缓冲区进行清空，数据也写不进才会阻塞卡住。因此我们在使用这两种方式执行命令操作时一定要对其子线程标准I&#x2F;O进行处理。","tags":["Shell"],"categories":["Java"]},{"title":"我写博客的方式","path":"/posts/9a5fac69/","content":"今天终于能舒服的写博客了 我的博客是使用Hexo静态部署，使用markdown写好文档后放到_posts文件夹里就可以直接部署了。但_posts目录里把所有文档都放在一起没有文件夹归类，因此我的思路是使用Typora写好笔记后存在本地指定目录，定时运行java小程序把目录里的文档全部复制到_posts里并调用cmd命令生成、发布hexo博客，最后把我的存放笔记的目录上传gitee.com中的仓库备份。 推荐使用Typora写markdown，所写即所见，很方便、很优雅。如图： 左侧使用目录的管理方式，可以很方便的在本地写文档。我不太喜欢博客网站中写，需要打开浏览器，访问网站等一系列操作，如断网还保存不了。 这里分享一个使用Typora写hexo博客的小技巧，使用输入法的自定义短语生成文章头部的yaml样式,可自动生成时间 ，我们只需要填写标题，标签，分类即可。 如下图我写好的note按分类放在F:/Markdown目录，在这个目录下面还有一个。java文件与一个bat文件。 java文件的作用是把该目录中的所有md复制到hexo中的_posts中然后调用cmd命令进行生成、发布等操作，bat文件的作用主要是设置系统的定时任务调用java小程序，最后把F:Markdown中的文件上传gitee.com仓库备份。 代码比较简单，直接上代码了： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091class DeployBlog &#123; public static final String HEXO_DIR = &quot;F:/Blog/source/_posts&quot;; public static final String MARKDOWN_DIR = &quot;F:/Markdown&quot;; public static List&lt;File&gt; fileList = new ArrayList&lt;File&gt;(); /** * 获取Markdown目录中的所有md文件 * @param file */ public void getMarkdown(File file) &#123; if (file.isFile()) &#123; if (file.getName().indexOf(&quot;md&quot;) &gt; -1) &#123; fileList.add(file); &#125; &#125; else if (file.isDirectory()) &#123; if (file.getName() != &quot;.git&quot;) &#123; File[] files = file.listFiles(); if (files != null) &#123; for (File f : files) &#123; getMarkdown(f); &#125; &#125; &#125; &#125; &#125; /** * 将获取到的文件复制到hexo的_posts目录(先清空_posts目录) */ public void writeFile() &#123; File file = new File(HEXO_DIR); if (!file.exists()) &#123; file.mkdirs(); &#125; for (File f1 : file.listFiles()) &#123; f1.delete(); &#125; for (File f2 : fileList) &#123; File dest = new File(file, f2.getName()); try &#123; Files.copy(f2.toPath(), dest.toPath()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 执行命令发布并部署 * @return * @throws Exception */ public String deployHexo() throws Exception&#123; String cmd = &quot;cd &quot; + HEXO_DIR + &quot; &amp;&amp; hexo clean &amp;&amp; hexo g &amp;&amp; hexo d&quot;; List&lt;String&gt; cmds = new LinkedList&lt;String&gt;();// cmds.add(&quot;sh&quot;); cmds.add(&quot;cmd.exe&quot;);// cmds.add(&quot;-c&quot;); cmds.add(&quot;/c&quot;); cmds.add(cmd); System.out.println(&quot;deploy...&quot;); ProcessBuilder pb = new ProcessBuilder(cmds); //重定向到标准输出 pb.redirectErrorStream(true); Process p = pb.start(); p.waitFor(3, TimeUnit.SECONDS); BufferedReader br = new BufferedReader(new InputStreamReader(p.getInputStream())); StringBuffer sb = new StringBuffer(); String line; while ((line = br.readLine()) != null) &#123; sb.append(line).append(&quot; &quot;); &#125; String result = sb.toString(); return result; &#125; public static void main(String[] args) &#123; DeployBlog deployBlog = new DeployBlog(); File file = new File(MARKDOWN_DIR); deployBlog.getMarkdown(file); deployBlog.writeFile(); try &#123; String cmdstr = deployBlog.deployHexo(); System.out.println(cmdstr); &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; System.out.println(&quot;deploy success!&quot;); &#125;&#125; 关于java中执行cmd命令可以搜索我博客中的文章：Java中执行bat或shell命令.md，其它文件操作比较简单就不多说了。最后使用一个bat完成java的调用。 12345678910111213@echo off echo 正在更新博客...javac DeployBlog.javajava DeployBlogecho 博客更新成功！访问hjwjw.github.io查看echo ====================================echo 正在备份笔记...git add .git commit -m &quot;backup blog&quot;git pushecho 备份成功！choice /T 5 /C ync /CS /D y /n bat文件内容也比较简单，最后一条是暂停5秒后才关闭窗口。 最后一步是设置windows定时任务，定时执行这个bat文件即可:joy: 虽然方式算不上很高级，技术含量也不是很高，不过我感觉写博客已经很舒适了，使用自己喜欢的Markdown软件，自己的博客，本地写作，自动部署、自动备份。 写在最后： 写Markdown文档最头疼的就是图床了，这里推荐我在用的图床工具,点击直达github:PicGo","tags":["技术杂谈"],"categories":["技术杂谈"]},{"title":"Window下单机ELK搭建入门并与Spring Boot项目整合","path":"/posts/6f31418d/","content":"ELK是一款非常流行的日志分析系统，在微服务架构中，我们可以使用ELK来跟踪分析各个微服务的日志，从而来了解服务的运行情况。 ELK是由三个开源工具搭建而成一个系统，分别是： ElasticSearch: ES是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等。 Logstash：一个完全开源的工具，可以对日志进行收集、分析、并将其存储供以后使用。 Kibana：一个开源和免费的工具，他Kibana可以为Logstash和ES提供的日志分析友好的Web界面，可以帮助您汇总、分析和搜索重要数据日志。 环境 Window 10 jdk 1.8 ElasticSearch-6.4 logstash-6.4 kibana-6.4-windows-x86_64 ELK安装需要先安装jdk 1.8，其实只需要java运行环境即可，具体安装与环境变量配置过程这时不记录了 ElasticSearchWindow下的安装比较简单。在官网下载对应平台的安装包即可。 官网：https://www.elastic.co/downloads/elasticsearch 下载下来后是一个zip的压缩文件，直接解压到安装目录即可。 ES的启动也非常简单，打开/bin 目录双击 elasticsearch.bat 即可。 当然我们也可以选择作为服务启动，在命令行模式下切换到ES的/bin执行 Service install 再通过/bin目录下的 elasticsearch-service-mgr.exe来管理与启动服务。这里我选的第一种，使用的时候直接打开就行了。 启动后直接访问：http://localhost:9200/ 若出现如下页面表示ES已经启动成功了。 Logstash官网下载：https://www.elastic.co/downloads/logstash 我下载的是ZIP格式的，将压缩包解压到安装目录即可。 配置在其/config目录可以看到已有一些配置文件，我们可以按自己的要求来新建一个配置文件,在此目录新建一个logstach-test.conf内容如下 ： 123456789101112131415input &#123; tcp &#123; port =&gt; 5044 type =&gt; &quot;test&quot; codec =&gt; json_lines &#125; &#125; output &#123; elasticsearch &#123; hosts =&gt; [&quot;localhost:9200&quot;] index =&gt; &quot;logstash-%&#123;type&#125;&quot; &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125; 这里的配置比较简单，logstash主要是收集日志给ElasticSearch。因此 input&#123;&#125;中主要是配置logstash监听的端口，后面我们在项目中配置日志向这个端口传输，logstash就会收集。 output&#123;&#125;则配置ElasticSearch的地址，logstash将收集的日志向ES的地址进行输出，index 可以指定索引名， stdout是标准输出，会将收集到的日志进行输出，方便我们调试。 input中 codec =&gt; json_lines是一个json解析器，接收json的数据。这个要装logstash-codec-json_lines 插件。在命令行中切换到Logstash的 /bin目录下，执行以下命令： 1logstash-plugin install logstash-codec-json_lines ####启动 命令行中切换到Logstash的 /bin目录下，执行以下命令，使用我们上面的配置文件启动Logstash 1logstash -f ../config/logstash-test.conf 命令窗口中打印如下内容无报错并处于监听状态则启动成功： 1[2018-09-18T16:29:03,364][INFO ][logstash.agent] Successfully started Logstash API endpoint &#123;:port=&gt;9600&#125; Kibana官网下载：https://www.elastic.co/downloads/kibana 这里下载的是Window版本，下载下来是一个压缩包，直接解压到指定的安装目录即可。 启动则直接在/bin目录双击kibana.bat 新建Spring Boot项目这里使用之前写的项目作测试，项目的构建请看:微服务简单实例–电影购票 这里使用用户微服务(服务提供者)。需要稍微作修改为我们整合Logstash 添加依赖123456&lt;!--添加logstas--&gt;&lt;dependency&gt; &lt;groupId&gt;net.logstash.logback&lt;/groupId&gt; &lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt; &lt;version&gt;4.7&lt;/version&gt;&lt;/dependency&gt; 修改Controller为了方便测试我们在findId方法中添加一条日志： 123456@GetMapping(&quot;/&#123;id&#125;&quot;)public User findById(@PathVariable Long id)&#123; User findOne = this.userRepository.getOne(id); logger.info(&quot;找到用户 ： &#123;&#125;&quot;,findOne); return findOne;&#125; 新建bootstrap.yml在application.yml同目录下新建bootstrap.yml，并将application.yml中的 以下内容移动到bootstrap.yml中 123spring: application: name: microservice-simple-provider-user-trace-elk 配置logback-spring.xml在resources目录下新建logback-spring.xml文件添加以下内容: 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration&gt; &lt;springProperty scope=&quot;context&quot; name=&quot;springAppName&quot; source=&quot;spring.application.name&quot; /&gt; &lt;appender name=&quot;LOGSTASH&quot; class=&quot;net.logstash.logback.appender.LogstashTcpSocketAppender&quot;&gt; &lt;destination&gt;localhost:5044&lt;/destination&gt; &lt;encoder charset=&quot;UTF-8&quot; class=&quot;net.logstash.logback.encoder.LogstashEncoder&quot; /&gt; &lt;/appender&gt; &lt;include resource=&quot;org/springframework/boot/logging/logback/base.xml&quot;/&gt; &lt;root level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;LOGSTASH&quot; /&gt; &lt;appender-ref ref=&quot;CONSOLE&quot; /&gt; &lt;/root&gt;&lt;/configuration&gt; 在上面的XML配置中添加了 &lt;destination&gt;localhost:5044&lt;/destination&gt;指定了我们上面配置logstash的监听地址，这样项目的日志才能被收集。 配置完成后可以启动项目，观察logstash窗口会有日志打印出来。 可视化查看分别启动Es，Logstash，用户微服务，Kibana。打开kibana的地址：http://localhost:5601 默认是没有密码可以直接登陆的。 创建索引模式我这里使用了汉化，默认为英文。点击发现添加索引。索引名为Logstash配置方案中指定的index。所以可以看到Kibana为我们列出了可用或使用过的索引名。在索引模式的输入框中填入 logstash-test下一步即可。 索引名称可以使用匹配规则，可以匹配多个索引。 下一步需要选择过滤字段，这里我们先选择时间戳，并点击创建索引模式。创建完成后出现如下系统管理&gt;索引模式界面界面即创建成功，这里列出了可用的过滤字段，并可以在些删除该索引模式。 查看数据点击发现这里我们可以看到刚刚新建的索引模式的数据。为了测试效果我们访问项目：http://localhost:8000/1多刷新几次再次返回`发现`界面查看，可以看到项目的日志已经过来了。下方可以看到日志内容，柱形图可以看到时间点上日志统计，左边为可显示的字段。可以把左边的字段加到下方进入展示。 添加筛选器我们可以添加筛选器message筛选出我们打印的日志 可视化分析在可视化界面中，我们添加一些视图。 通过一些条件定制，最后把视图放到仪表盘查看 总结以上只是简单的记录了下ELK的搭建与基本使用，对于生产部署还需要做一些配置。在学习过程中找到一些资料分享： Kibana 3指南 logstash的最佳实战-项目实战","tags":["Elk","日志管理"],"categories":["日志管理"]},{"title":"Spring Cloud Sleuth配合Zipkin实现微服务的跟踪","path":"/posts/6883841/","content":"在微服务架构中可以使用Zipkin来追踪服务调用链路，可以知道各个服务的调用依赖关系。在Spring Cloud中，也提供了Spring Cloud Sleuth来方便集成Zipkin实现。 本文使用一个Zipkin Server，用户微服务，电影微服务来实现。完整实现的代码放在github中： Zipkin Server：microservice-trace-zipkin-server-stream-mysql 用户微服务：microservice-simple-provider-user-trace-zipkin-stream 电影微服务：microservice-simple-consumer-movie-trace-zipkin-stream Zipkin ServerZipkin可以不配置数据库，但跟踪的数据只存在内在中，不能长久保存，因此这里使用mysql存储跟踪数据。项目中还使用了rabbitMQ作为消息中间件进行数据收集，实现Zipkin与微服务的解耦。 添加依赖新建一个Spring Boot项目，添加以下依赖： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin-stream&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-binder-rabbit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/io.zipkin.java/zipkin-autoconfigure-storage-mysql --&gt; &lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-storage-mysql&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt;\t&lt;/dependencies&gt;\t&lt;!--引入SpringCloud 依赖--&gt;\t&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Edgware.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;\t&lt;/dependencyManagement&gt; 添加注解在启动类上添加注解： 1@EnableZipkinStreamServer 修改配置配置文件如下： 12345678910111213141516171819spring: application: name: microservice-trace-zipkin-server-stream-mysql rabbitmq: host: localhost port: 5672 username: guest password: guest datasource: schema: classpath:/mysql.sql url: jdbc:mysql://localhost:3306/zipkin username: root password: 123456zipkin: storage: type: mysqlserver: port: 9411 mysql.sql文件可以在上面项目地址中找到,首先需要在Mysql数据库中新建zipkin数据库，在项目启动时会自动执行mysql.sql。 RabbitMQ的安装与配置可以参考：RabbitMQ学习系列:一、RabbitMQ 的安装 微服务整合Zipkin用户微服务与电影微服务作一样的修改。 ##添加依赖 主要需要添加以下依赖： 123456789101112&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt; &lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin-stream&lt;/artifactId&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-binder-rabbit&lt;/artifactId&gt;&lt;/dependency&gt; 修改配置在配置文件中添加以下内容： 12345678910111213spring: application: name: microservice-simple-consumer-movie-trace-zipkin zipkin: base-url: http://localhost:9411 sleuth: sampler: percentage: 1.0 rabbitmq: host: localhost port: 5672 username: guest password: guest 启动测试完成两个微服务的整合修改后，首先启动rabbitmq,保证mysql可以连通。分别启动Zipkin Server，用户微服务，电影微服务。 为了能看到跟踪数据，我们先访问服务让其产生数据：http://localhost:8011/user/1 再方便Zipink Server地址查看：http://localhost:9411/zipkin/ 选择我们需要查看的时间点，点击 Find Traces我们就能看到跟踪数据了。 点击导航栏上的Dependencies可以查看服务依赖 这里我们可以看到服务的调用方向。 打开Mysql数据库也可以看到跟踪数据已经被存储了： 这样即使Zipkin被关闭，跟踪数据也不会丢失。","tags":["Zipkin"],"categories":["微服务"]},{"title":"本地仓库与远程仓库的关联问题","path":"/posts/c2e49464/","content":"今天新建一个本地仓库在关联远程仓库时遇到的问题，作下记录。 [TOC] 配置SSH KEY 本地仓库与远程仓库通过SSH协议，所以需要配置SSH KEY 使用命令： 1ssh -keygen -t rsa –C “youremail@example.com” 运行后会在本地用户目录生成一个 .ssh目录，里面有id_rsa和d_rsa.pub两个文件，d_rsa.pub是公钥。在远程仓库中打开配置SSH的页面粘贴d_rsa.pub文件里面的内容即可。如果已经生成过可以直接使用。 关联空仓库在一个已经文件的文件夹里打开bash命令行窗口 使用命令初始化本地仓库 1$ git init 提交本地仓库内容到暂缓区 1$ git add . 提交到本地仓库 1$ git commit -m &quot;first commit&quot; 新建一个远程空仓库，关联本地仓库 1$ git remote add origin git仓库地址.git 把本地仓库分支master内容推送到远程仓库 1$ git push -u origin master 第一次提交加 -u 表示该分支与远程仓库中的master分支关联，后面提交直接 git push 即可 关联非空仓库如果你的远程仓库新建时已经添加了文件，并且本地仓库也不是空的，那么在本地仓库关联远程仓库后还是无法推送，需要先拉取远程仓库中的内容。 123$ git pull origin master * branch master -&gt; FETCH_HEADfatal: refusing to merge unrelated histories 但我在操作时遇到了上面的错误 fatal: refusing to merge unrelated histories经过查找得知这个错误在2.9.0版本之后才出现，以前的版本可以正常使用，因此我们必需加一个可选项 1$ git pull origin master --allow-unrelated-histories 通过这个选项允许我们把origin仓库的master分支同步到本地，命令执行后还会打开一个vim填写备注，vim使用自行Google。 与远程同步成功后我们再执行推送命令 1$ git push -u origin master 这样就可以成功把远程仓库与本地仓库关联了。","tags":["TortoiseGit"],"categories":["TortoiseGit"]},{"title":"Spring Zuul 微服务网关的构建","path":"/posts/2d527b76/","content":"[TOC] 在学习完前面的知识后，微服务架构已经初具雏形。但还有一些问题：不同的微服务一般会有不同的网络地址，客户端在访问这些微服务时必须记住几十甚至几百个地址，这对于客户端方来说太复杂也难以维护。如下图： 因此，我们需要一个微服务网关，介于客户端与服务器之间的中间层，所有的外部请求都会先经过微服务网关。客户端只需要与网关交互，只知道一个网关地址即可，这样简化了开发还有以下优点： 1、易于监控 2、易于认证 3、减少了客户端与各个微服务之间的交互次数 如下图： 在Spring Cloud中 Zuul是Netflix的基于JVM的路由器和服务器端负载均衡器。Zuul可以和Eureka、Ribbon、Hystrix、等组件配合使用。 构建微服务网关新建一个spring Boot项目microservice-gateway-zuul 。Spring Boot :1.5.9.RELEASESpring Cloud:Edgware.RELEASE 引入依赖12345678&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t&lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t&lt;artifactId&gt;spring-cloud-starter-netflix-zuul&lt;/artifactId&gt;&lt;/dependency&gt; 修改启动类上面依赖中也加入了Eureka ，一般我们都会在启动类中添加上@EnableDiscoveryClient 注解，但在ZUUL 中我们只需要添加一个@EnableZuulProxy注解即可 1234567@SpringBootApplication@EnableZuulProxypublic class MicroserviceGatewayZuulApplication &#123;\tpublic static void main(String[] args) &#123;\tSpringApplication.run(MicroserviceGatewayZuulApplication.class, args);\t&#125;&#125; 添加配置这里先使用最简单的配置，即配置应用名、端口、注册Eureka即可。后面再慢慢优化配置。 123456789spring: application: name: microservice-gateway-zuulserver: port: 8040eureka: client: service-url: defaultZone: http://user:admin@localhost:8761/eureka 测试完成配置后则可以分别启动Eureka、用户微服务、Config Server(如果使用了)、microservice-gateway-zuul 项目。这样一个简单的微服务网关就完成了。我们可以通过 microservice-gateway-zuul 的路径加 微服务应用名 来访问微服务。如访问用户微服务：http://localhost:8040/microservice-provider-user/1这样所有的微服务都可以通过 Zuul 进行访问。前面提到Zuul 可以配合Ribbon使用，我们不用加任何配置和依赖，即可以实现负载均衡，可以再启动一个用户微服务进行测试。 优化配置上一节使用 Zuul 实现了最简单的微服务网关，在实际环境中需要对Zuul 进行优化配置。 路由1、转发在Zuul的配置中可以为各微服务添加路由映射，如添加配置： 123zuul: routes: microservice-provider-user: /user/** 表示HTTP调用将 /user 转发到microservice-provider-user服务，于是我们访问用户微服务的地址可以简化为：http://localhost:8040/user/1 &#x2F;user&#x2F;* 表示匹配其下的一个级别的路径，&#x2F;user&#x2F;** 表示匹配其下多个级别的路径。 另外 还有一种方式也可以实现： 12345zuul: routes: user: path: /user/** serviceId: microservice-provider-user 这种写法与上面的效果相同。zuul.routes.user 这里的 user 是只该路由名称，可以自己随意命名。2、正则表达式指定路由规则Zuul中可以写正则来指定路由规则，如微服务名命名规则为：微服务名+版本。通过添加以下正则规则 ，我们可以通过 版本 + 微服务名 来访问。 1234567891011121314@SpringBootApplication@EnableZuulProxypublic class MicroserviceGatewayZuulApplication &#123;\t//正则表达式指定路由规则\t@Bean\tpublic PatternServiceRouteMapper serviceRouteMapper() &#123; return new PatternServiceRouteMapper( &quot;(?&lt;name&gt;^.+)-(?&lt;version&gt;v.+$)&quot;, &quot;$&#123;version&#125;/$&#123;name&#125;&quot;);\t&#125;\tpublic static void main(String[] args) &#123; SpringApplication.run(MicroserviceGatewayZuulApplication.class, args);\t&#125;&#125; 添加PatternServiceRouteMapper 传入两个参数，第一个为微服务的命名规则正则表达式，第二个是需要转化成什么形式的正则表达式。(?&lt;name&gt;^.+)-(?&lt;version&gt;v.+$)表示我们的微服务命名 为服务名+版本;$&#123;version&#125;/$&#123;name&#125; 表示访问路径转化成版本+服务名测试为了测试方便，把用户微服务的spring.application.name 更改为 microservice-provider-user-v1 ,如果使用了Config Server 把git上的配置也记得更改。 我们重启下Zuul，再次访问用户微服务会我发现原来的访问路径不行了。需要使用以下方式：http://localhost:8040/v1/microservice-provider-user/1 3、前缀添加如下配置： 12zuul: prefix: /api 访问Zuul 的/api/microservice-provider-user/1会被映射到 /microservice-provider-user/1 即添加了前缀。添加配置： 123zuul: prefix: /api strip-prefix: false 配置 strip-prefix后，访问Zuul 的api/microservice-provider-user/1会被转发到 microservice-provider-user的 /api/1strip-prefix需与prefix配合使用。 4、忽略微服务或路径在有些情况下我们不想微服务网关去代理某个微服务，或者想保护某个微服务下了敏感路径可以使用以下配置： 12zuul: ignored-services: microservice-config-server zuul.ignored-services 表示忽略指定的微服务，则通过Zuul不能访问到该微服务。 12zuul: ignored-patterns: /**/getProfile/** zuul.ignored-patterns 表示忽略所以包含 /getProfile/的路径，上一篇笔记中我们为用户微服务添加了 /getProfile/ 现在可以测试已无法访问了，但其它路径则正常，这种方式常可以用来屏蔽 /admin/等比较敏感的路径。 Zuul 过滤器Zuul 过滤器是Zuul的核心组件，Zuul中已经实现了一些过滤器，同时我们也可以自己定义过滤器。在Zuul中定义过滤器很简单，只需要继承 ZuulFilter 类。 自定义过滤器在上面构建的 microservice-gateway-zuul中新建类 并继承 ZuulFilter 如下： 12345678910111213141516171819202122232425262728293031public class MyFilter extends ZuulFilter &#123; /** * 过滤器类型 */ @Override public String filterType() &#123; return &quot;pre&quot;; &#125; /** * 过滤器的优先级,数字越大顺序越后 */ @Override public int filterOrder() &#123; return 1; &#125; /** * 是否使用该过滤器 */ @Override public boolean shouldFilter() &#123; return true; &#125; /** * 具体实现 */ @Override public Object run() &#123; System.out.println(&quot;这里是通过ZuulFilter中打印出来的！&quot;); return null; &#125;&#125; 继承 ZuulFilter后实现它的四个方法，作用可看上面的注释。过滤器类型有如下几种： PRE：这种过滤器在请求被路由之前调用，可以利用这种过滤器实现身份认证，记录调用信息等。ROUTING：这种过滤器将请求路由到微服务。POST：这种过滤器在路由到微服务后执行。ERROR：在其他阶段发生错误时执行该过滤器。 在启动类中添加Bean 1234567891011@SpringBootApplication@EnableZuulProxypublic class MicroserviceGatewayZuulApplication &#123;\t@Bean\tpublic MyFilter myFilter()&#123; return new MyFilter();\t&#125;\tpublic static void main(String[] args) &#123;\tSpringApplication.run(MicroserviceGatewayZuulApplication.class, args);\t&#125;&#125; 重新启动Zuul，随意访问一个微服务，可以在Zuul控制台看到打印了过滤器run方法中输出的内容： 禁用过滤器Zuul默认启用了一些过滤器，这些过滤器存放在spring-cloud-netflix-core包中的com.netflix.zuul.filters中，在发某些场景下，我们想禁用一些过滤器可以直接在配置文件中设置：zuul.&lt;SimpleClassName&gt;.&lt;filterType&gt;.disable=true SimpleClassName:是指过滤器类名filterType:是过滤器类型如禁用我们上面自定义过滤器可以这样写： 1234zuul: MyFilter: pre: disable: true 重启Zuul再次通过Zuul访问微服务，会发现Zuul的控制台不会再打印run方法中的内容，说明已经被禁用了。","tags":["Zuul","API GateWay"],"categories":["微服务"]},{"title":"Spring Cloud Config 同步刷新配置及加密解密","path":"/posts/525fe2aa/","content":"目录 [TOC] 前面的文章为微服务架构引入了统一配置管理Spring cloud config，实现了各个微服务配置分布式管理。配置被修改后，我们不可能重新启动微服务，前面说到过Spring Cloud Config可以自动更新配置，本篇会对同步自动刷新配置进行学习记录。另外配置文件存储在GIT仓库中，很多场景下，对于某些敏感的配置内容(例如数据库账号，密码等)，应当加密存储。部分内容涉及上篇文章：微服务学习笔记–使用Spring Cloud Config 统一管理微服务配置 同步刷新在项目中加入actuator，就会有一个/refresh端点，当配置被修改后，只需要手动通过POST方式去访问这个端口，项目配置即可被刷新。但这种手动刷新的方式只能针对单个项目刷新，如果所有的微服务节点的配置都需要手动去刷新则工作量会很大。因此这里介绍的是同步刷新的方式，使用Spring Cloud Bus实现。之所以它能实现同步刷新，是因为Spring Cloud Bus 使用了轻量级的消息代理(如rabbitmq,kafka)，通过广播的方式让所以有被修改配置的微服务节点都能被刷新。 添加依赖在上篇文章的Config Server，用户微服务，电影微服务三个项目中都引入以下依赖： 1234&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t&lt;artifactId&gt;spring-cloud-starter-bus-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 添加RabbitMQ1、RabbitMQ安装可以参考之前的文章：RabbitMQ学习系列:一、RabbitMQ 的安装在三个项目的配置文件中添加rabbitmq配置 12345rabbitmq: host: localhost port: 5672 username: guest password: guest 添加测试方法其实这样就改造完成了，为了方便测试，我们把git上存储的各个配置文件添加相应配置： profile: user-dev-v1.0 在用户与电影微服务中的Controller中添加以下红框中的内容以获取上面添加的配置值： @RefreshScope 添加该注解的类会在类更改时得到特殊处理 通过这样改造，我们可以修改上面的属性，push到仓库，刷新后测试属性是否被更新来验证我们配置的Spring cloud bus 是否生效。 测试首先我们访问用户与电影微服务的getProfile方法查看当前配置的profile值：把microservice-provider-user-dev.yml与 microservice-consumer-movie-dev.yml中的profile配置值分别修改成：user-dev-v2.0 和 movie-dev-v2.0 并推送到git仓库。再次使用getProfile方法获取配置值发现没有变化。通过POSTMAN访问Config Server的/bus/refresh 端口：再次访问用户与电影微服务的getProfile方法：发现获取的配置值已经改变，说明我们刷新配置生效了。这里我们是访问Config Server的 /bus/refresh端口，其实访问用户或电影微服务的/bus/refresh端口效果也是一样的，不过在实际环境中，微服务被迁移，网络地址可能会发生改变，因此把Config Server也加入到消息总线中，它的地址一般比较固定，它的配置更新状态可以广播到其它微服务节点中。 设置自动刷新前面其实还是使用手动通过POST方式去访问/bus/refresh端口进行刷新，但已经实现了同步刷新。关于自动刷新我们可以借助GIT仓库的WebHook配置，在PUSH时让GIT给指定网络地址发送一个POST请求。由于这里是在本地进行学习测试，没有网络地址可以进行测试。 加密解密Config Server 为配置内容的加密解密提供了两种方式。 对称加密安装JCE使用JCE,下载地址：http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html需要对应相应版本的JDK，上面地址是jdk8的。下载后打开解压有两个jar文件，把它们替换JDK安装目录下：%JAVA_HOME%\\jre\\lib\\security 在Config Server的配置文件中添加密钥： 12encrypt: key: wei key 可以自己随意设置需要注意的是，这个配置必须配置在bootstrap.yml中，因此在Config Server中我们需要新建bootstrap.yml并将配置写入。 存储加密内容做好上面的工作后，重新启动Config Server 项目，可能在控制到看到打印了以下端点：这两个端点可以进行加密与解密。我们用POSTMAN把上面添加的profile配置值加密：然后把返回的加密值写入配置文件中，以&#123;cipher&#125;开关，以让Config Server识别这是需要解密的内容。将修改后的配置文件push到git仓库，使用/bus/refresh刷新配置后访问用户微服务的getProfile方法以测试：可以看到返回到Config Client的配置已经被自动解密了 非对称加密非对称加密相对与对称加密来说更安全。1、生成keyStore非对称加密可以使用JDK自带的keytool工具，打开cmd输入以下命令： keytool -genkeypair -alias config-server -keyalg RSA -keystore config-server.keystore 按下图方式填写内容：方便测试，上图我只输入了红框中的内容，其它地方直接回车即可。 运行完后会在当前运行路径下生成一个config-server.keystore 把这个文件移动到Config Server项目的 \\src\\main\\resources目录下。2、添加配置在Config Server 的bootstrap.yml配置下添加以下内容： 123456encrypt: key-store: location: config-server.keystore alias: config-server password: ****** secret: ****** 这里的password与secret分别是生成keystroe时第一次与第二次输入的密码。location 是指向config-server.keystore的放置路径。3、非对称加密测试配置好上面的内容后，重启Config Server ，和对称加密一样，通过POSTMAN生成加密内容，写到配置文件中PUSH到GIT仓库即可。","categories":["微服务"]},{"title":"Spring Cloud Config 统一管理微服务配置","path":"/posts/e60cfe17/","content":"目录 [TOC] 微服务架构中为了方便管理与更新各微服务的配置，在Spring Cloud中可以使用 Spring Cloud Config 来统一管理系统内各微服务的配置文件。使用Config统一管理后，可实现git分布式版本控制，不同环境不同配置，动态调整自动更新配置等功能。Spring Cloud Config 包括Config Server 和 Config Client 两部分，Config Server用于管理配置，Config Client 则与各微服务集成负责向Config Server请求获取配置并进行缓存以提高性能。Config Server默认使用Git存储配置内容，当然也可以使用SVN,本地文件系统或Vault存储配置。下面把Config Server 、 Config Client 和 Eureka配合使用记录下来。 编写Config Server在编写Config Server 前，我们需要使用Git作为后端存储。 创建Git仓库可以在github.com上创建一个仓库。先前面用到了电影与用户微服务的配置文件直接放到仓库中。配置文件命名使用规范：服务名+环境 。这里我创建的仓库如下图：每个微服务的配置我这里按开发与生产环境分别部署两份。配置文件内容与前面做的demo配置一样，但开发与生产配置的端口号不同。另外，方便测试版本控制，我给仓库新建了一个 config-label-v2.0 分支，将端口号进行修改以便区分。 编写Config Server新建一个Spring Boot项目，1、引入依赖 12345678910111213141516171819202122232425262728293031&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;\t&lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Edgware.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;\t&lt;/dependencyManagement&gt; 2、添加注解在启动类上添加 @EnableConfigServer 声明这是一个Config Server 123456789@EnableDiscoveryClient@EnableConfigServer@SpringBootApplicationpublic class MicroserviceConfigServerApplication &#123;\tpublic static void main(String[] args) &#123; SpringApplication.run(MicroserviceConfigServerApplication.class, args);\t&#125;&#125; 因为我们是配合Eureka使用，因此启动类上也需要添加 @EnableDiscoveryClient 注解 3、编写配置编写application.yml ，并添加以下配置： 12345678910111213141516server: port: 8082spring: application: name: microservice-config-server cloud: config: server: git: uri: https://github.com/hjwzyy/spring-cloud-edgware-configServer-demo username: ******@*******.com password: *******eureka: client: service-url: defaultZone: http://user:admin@localhost:8761/eureka/ 至此 一个简单的Config Server就完成了。下面我们来启动Config Server进行测试。访问如下地址：http://localhost:8082/microservice-provider-user/dev 如下图：这里显示的是我们放在Git仓库里的用户微服务的开发环境配置文件，说明Config Server正常。测试版本控制，访问如下地址：http://localhost:8082/microservice-provider-user/dev/config-label-v2.0可以看到访问地址加上指定分支后显示的是指定分支下的配置文件。 关于Config Server 获取git上的资源信息遵循如下规则： 12345/&#123;application&#125;/&#123;profile&#125;[/&#123;label&#125;]/&#123;application&#125;-&#123;profile&#125;.yml/&#123;label&#125;/&#123;application&#125;-&#123;profile&#125;.yml/&#123;application&#125;-&#123;profile&#125;.properties/&#123;label&#125;/&#123;application&#125;-&#123;profile&#125;.properties 我们可以按以上规则去访问git上的配置文件 编写Config Client前面我们写过，Config Client与微服务集成，因此在微服务中添加Config的依赖，稍微配置一下就可以了。下面我将前面的用户微服务与电影微服务添加Config Client，整个用户与电影微服务的编写，服务注册这里不再贅述，可参考前文： 微服务简单实例–电影购票 微服务学习笔记 –使用Spring Cloud Eureka实现服务注册与发现 添加依赖向用户微服务添加以下依赖： 1234&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t&lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&lt;/dependency&gt; 电影微服务同上添加依赖。 修改配置因为用户微服务的配置已经放在了git上，所以项目的中的application.yml文件只需要配置一个端口即可，其它配置清除： 12server: port: 8005 git上的配置文件也可以配置端口，并且会以git上的配置文件为准 添加bootstrap.yml配置文件 Spring Cloud 有一个“引导上下文”的概念，这里主应用程序的父上下文。引导上下文负责从配置服务器加载配置属性，以及解密外部配置文件中的属性。和主应用程序加载application.*(yml或priperties)中的属性不同，引导上下文加载bootstarap.*中的属性。配置在bootstrap.*中的属性有更高的优先级，因此默认情况下它们不能被本地配置覆盖。 1234567891011121314spring: application: name: microservice-provider-user cloud: config: profile: prod label: master discovery: enabled: true serviceId: microservice-config-servereureka: client: service-url: defaultZone: http://user:admin@localhost:8761/eureka/ 配置文件中加上服务注册配置，因为我们是与Eureka配合使用，Config Client需要先从Eureka中找到Config Server，再从Config Server中获取到对应的配置。而获取对应配置是依靠spring.application.name 与 spring.cloud.config.profile来找到对应配置文件的。还记得前面我们在Git仓库中建立的配置文件命名吗？都是以 服务名+环境 进行命名的。 spring.cloud.config.label 则是指定分支为主分支 masterspring.cloud.config.discovery.enabled 表示使用服务发现组件中的Config Server，而不是自己指定的Config Serverr uri，默认为falsespring.cloud.config.discovery.serviceId指定Config Server在服务发现中的serviceId，默认是configserver ，因此这里写的是前面编写的Config Server 的spring.application.name 电影微服务的配置同上至此，Config Client已经配置完成。 测试分别启动Eureka，Config Server，用户微服务与电影微服务。在Eureka界面可以看到有三个注册的服务：用户微服务与电影微服务启动的端口是git配置文件中设置的端口，访问各各微服务通过正常获取数据：说明配置文件已经实现了通过Spring cloud Config 统一管理与获取。","tags":["Spring Cloud Config"],"categories":["微服务"]},{"title":"Hystrix实现微服务的容错处理与监控数据","path":"/posts/31090589/","content":"目录 [TOC] 在微服务架构中，如果服务提供者响应缓慢，那么服务消费者的请求就会被强制等待，或响应超时。在高负载场景下，如果不做任何处理，这类问题可能会导致服务消费者资源耗竭甚至整个系统的崩溃。 HystrixHystrix是一个实现了超时机制和断路器模式的工具类库。是由Netflix开源的，用于隔离访问系统、服务或者第三方库，防止级联失败，从而提升系统的可用性与容错性。Hystrix主要通过以下几点实现延迟和容错：1、包裹请求： 使用HystrixCommand（或HystrixObservableCommand）包裹对依赖的调用逻辑，每个命令在独立线程中执行。这使用到了设计模式中的“命令模式”。2、跳闸机制： 当某服务的错误率超过一定阈值时，Hystrix可以自动或手动跳闸，停止请求该服务一段时间 。3、资源跳闸： Hystrix为每个依赖都维护了一个小型的线程池（或者信号量）。如果该线程池已满，发往该依赖的请求就被立即拒绝，而不是排除等候，从而加速失败判定。4、监控： Hystrix可以近乎实时地监控运行指标和配置的变化，例如成功、失败、超时、以及被拒绝的请求等。5、回退机制： 当请求失败、超时、被拒绝，或当断路器打开，执行回退逻辑。回退逻辑可由开发人员自行提供，例如返回一个缺省值。6、自我修复： 断路器打开一段时间后，会自动进行“半开”状态。断路器打开、关闭、半开的逻辑转换。 整合Hystrix在Spring Cloud 中整合Hystrix非常方便。我们以前面的服务消费者项目为例进行修改。 添加依赖1234&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t&lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; 修改启动类在启动类中添加注解 @EnableHystrix 或@EnableCircuitBreaker，为项目启用断路器支持。 12345678910111213@EnableDiscoveryClient@EnableHystrix@SpringBootApplicationpublic class MicroserviceConsumerMovieRibbonApplication &#123;\t@Bean\t@LoadBalanced\tpublic RestTemplate restTemplate()&#123; return new RestTemplate();\t&#125;\tpublic static void main(String[] args) &#123; SpringApplication.run(MicroserviceConsumerMovieRibbonApplication.class, args);\t&#125;&#125; Controller123456789101112131415161718192021222324252627@RestControllerpublic class UserController &#123; private static final Logger LOGGER = LoggerFactory.getLogger(UserController.class); @Autowired private RestTemplate restTemplate; @Autowired private LoadBalancerClient loadBalancerClient; @HystrixCommand(fallbackMethod = &quot;findByIdFallback&quot;) @GetMapping(&quot;/user/&#123;id&#125;&quot;) public User findById(@PathVariable Long id)&#123; return this.restTemplate.getForObject(&quot;http://microservice-provider-user/&quot; + id,User.class); &#125; public User findByIdFallback(@PathVariable Long id)&#123; User user = new User(); user.setId(-1L); user.setName(&quot;默认用户&quot;); return user; &#125; @GetMapping(&quot;/log-user-instance&quot;) public void logUserinstance()&#123; ServiceInstance serviceInstance = this.loadBalancerClient.choose(&quot;microservice-provider-user&quot;); UserController.LOGGER.info(&quot;&#123;&#125;:&#123;&#125;:&#123;&#125;&quot;,serviceInstance.getServiceId(),serviceInstance.getHost(),serviceInstance.getPort()); &#125;&#125; 在Controller中，我们为findById方法编写了一个回退方法findByIdFallback，在findByIdFallback方法中返回了一个默认用户，在findById中添加注解@HystrixCommand在fallbackMethod 属性中指定它的回退方法即可。 容错测试修改好上述内容后我们分别启动一个服务提供者，服务消费者与Eureka。访问服务消费者：http://localhost:8011/user/1 可以正常得到数据。当我们把服务提供者关闭后再次访问，会发现返回了之前的默认用户:。 Hystrix监控前面提到Hystrix除了可以实现容错处理，还有监控功能。使用Hystrix的模块hystrix-metrics-event-stream，就可以将这些监控信息以文本的形式暴露给外部系统。spring-cloud-starter-netflix-hystrix已经包含了这个模块，我人只需要添加Actuator，就可以使用/hystrix.stream端口获得Hystrix监控信息。 添加Actuator只需要在项目中添加Actuator依赖即可。 1234&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 重新启动项目，访问 http://localhost:8011/hystrix.stream会出现类似如下图的信息： 可视化监控数据上一节我们可以看到监控信息，但都是通过文字形式展示的，不能直观的显示系统状态。我们可以使用Hystrix Dashboard，让监控数据图形化，可视化。我们新建一个最简单的Spring Boot 项目 添加以下依赖1234&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t&lt;artifactId&gt;spring-cloud-starter-netflix-hystrix-dashboard&lt;/artifactId&gt;&lt;/dependency&gt; 启动类添加注解在启动类上添加注解 @EnableHystrixDashboard12345678@EnableHystrixDashboard@SpringBootApplicationpublic class MicroserviceHystrixDashboardApplication &#123;\tpublic static void main(String[] args) &#123; SpringApplication.run(MicroserviceHystrixDashboardApplication.class, args);\t&#125;&#125; 配置文件在配置文件中添加访问端口12server: port: 8030 启动项目启动新建的项目：http://localhost:8030/hystrix ，就可以看到Dashboard界面，在中间的地址栏中输入上一节中我们查看监控数据的地址随意填写一个Title 点击按钮，Dashboard即可把http://localhost:8011/hystrix.stream 的文本信息转化成可视化视图：关于显示的指标信息解释可以参考如下图： Turbine聚合监控数据上一节我们把要监控的项目的 &#x2F;hystrix.stream 地址填入Dashboard即可以可视化的监控项目状态。但在微服务中往往有若干个微服务，每个微服务又有多个实例，如果这样一个个监控非常不方便。这一节我们使用Turbine来聚合Hystrix监控数据。首页我们把服务消费者项目复制一份，修改应用名、端口与方法名，模拟多个微服务的场景。这里我把前面使用的服务消费者项目直接复制一份，在配置文件中修改spring.application.name为microservice-consumer-movie2,修改server.port为8031。在Controller中将原方法findById 名更改为findById2,回退方法名更改为findByIdFallback2然后新建一个Spring Boot项目1、依赖 1234&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t&lt;artifactId&gt;spring-cloud-starter-netflix-turbine&lt;/artifactId&gt;&lt;/dependency&gt; 2、配置文件 1234567891011121314server: port: 8031spring: application: name: microservice-hystrix-turbineeureka: client: service-url: defaultZone: http://user:admin@localhost:8761/eureka/ instance: prefer-ip-address: trueturbine: app-config: microservice-consumer-movie,microservice-consumer-movie2 cluster-name-expression: &quot;&#x27;default&#x27;&quot; 在配置文件中，把项目注册到Eureka，使用 turbine.app-config 属性添加需要聚合监控的项目名3、添加注解在启动类上添加注解 @EnableTurbine 4、聚合监控数据最后我们启动服务提供者，两个服务消费者，一个Eureka，上一节新建的Dashboard项目与刚新建的Turbine项目。上一节中我们是把服务消费者的 /hystrix.stream 放入Dashboard中进行可视化，但Turbine项目已经聚合了两个服务消费者的监控数据，因为我们只需要把Turbine项目的地址放入Dashboard中即可。Turbine项目聚合监控信息的地址为 http://localhost:8031/turbine.stream点击按钮后可展示如下图的可视化界面可以看到两个服务消费者的方法都显示出来了，如果同名会显示成一个。这样我们就完成了监控的聚合，更加方便的显示各各微服务的状态。","tags":["Hystrix","Turbine"],"categories":["微服务"]},{"title":"Feign实现声明式REST调用","path":"/posts/f0c3ea98/","content":"[TOC] 前面的文章中，服务消费者调用服务提供者的接口我们是使用RestTemplate实现的REST API调用的。但这种方式在参数比较多时会变得低效，难以维护。 FeignFeign是Netflix开发的声明式，模板化的HTTP客户端。它可以让我们更加便捷，优雅的调用HTTP API。Feign有自己的注解，在Spring Cloud中对Feign进行了增强，使其可以支持Spring MVC注解，并整合Ribbon与Eureka。 整合Feign前面的服务消费都使用RestTemplate来调用服务提供者，这里更改成使用Feign，使用声明式的RestFul API。 添加依赖1234&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t&lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 配置配置其实无需修改 1234567891011server: port: 8011spring: application: name: microservice-consumer-movieeureka: client: service-url: defaultZone: http://user:admin@localhost:8761/eureka/ instance: prefer-ip-address: true 创建Feign接口123456@FeignClient(name = &quot;microservice-provider-user&quot;)public interface UserFeignClient &#123; @GetMapping(value = &quot;/&#123;id&#125;&quot;) public User findById(@PathVariable(&quot;id&quot;) Long id);&#125; @FeignClient 注解中的microservice-provider-user 是服务提供者的主机名，用于创建Ribbon负载均衡器，与上篇文章中使用主机名是一样的。也可以使用URL属性指定请求的URL: @FeignClient(name = &quot;microservice-provider-user&quot;,url=&quot;http://localhost:8000/&quot;) Controller123456789101112@RestControllerpublic class UserController &#123; @Autowired private UserFeignClient userFeignClient; @GetMapping(&quot;/user/&#123;id&#125;&quot;) public User findById(@PathVariable Long id)&#123; return this.userFeignClient.findById(id); &#125;&#125; Controller 中去掉RestTemplate，定义 UserFeignClient 。我在测试中定义的 UserFeignClient 会提示 Could not autowire. No beans of &#39;UserFeignClient&#39; type found. 但运行并不影响。 添加注解在启动类上注解 @EnableFeignClients 12345678@EnableDiscoveryClient@SpringBootApplication@EnableFeignClientspublic class MicroserviceSimpleConsumerMovieApplication &#123;\tpublic static void main(String[] args) &#123; SpringApplication.run(MicroserviceSimpleConsumerMovieApplication.class, args);\t&#125;&#125; 测试完成上述修改后，就可以像之前一样去调用服务提供者了。与上篇Ribbon一样，启动一个Eureka，一个或多个服务提供者(Feign自己集成了Ribbon)，启动修改完成后的服务消费者。访问http://localhost:8011/user/1 自定义Feign配置按上一节整合Feign很简单，但有时候我们想自定义Feign的配置，如更改Feign使用的编码器，解码器，契约，拦截器等。如服务提供者是需要Http Basic的认证才能调用的，那么服务消费者可以在Feign中自定义配置拦截器，添加 Http Basic 认证。 编写配置类12345678@Configurationpublic class FooConfiguration &#123; @Bean public BasicAuthRequestInterceptor basicAuthRequestInterceptor()&#123; return new BasicAuthRequestInterceptor(&quot;user&quot;,&quot;123456&quot;); &#125;&#125; 使用这个配置，只需要在Feign接口类UserFeignClient 的@FeignClient注解中添加configuration 属性 @FeignClient(name = &quot;microservice-provider-user&quot;,configuration=&quot;FooConfiguration.class&quot;) 另外还需要把服务提供者加上Http Basic认证。在服务提供都项目中添加security 1234&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 在其配置文件中添加用户名与密码： 123456security: basic: enabled: true user: name: user password: 123456 测试重新启动服务提供者与服务消费者这里我们直接去访问服务提供者是需要进行认证的：访问服务消费者，通过服务消费者去调用服务提供者，不会弹出认证窗口，因为服务消费者在Feign中已经配置了用户名与密码了。 Feign服务间传送文件转 ：Spring Cloud微服务【Finchley.RELEASE版本】(四)使用feign服务间传送文件 Feign构造多参数请求GET对于GET请求可以使用Map来构建。接口修改如下 : 12345@FeignClient(name = &quot;microservice-provider-user&quot;,configuration = FooConfiguration.class)public interface UserFeignClient &#123; @GetMapping(value = &quot;/get&quot;) public User get(@RequestParam Map&lt;String,Object&gt; map);&#125; 调用时构建一个Map传递到接口中 POSTpost请求则简单得多，可以直接使用 User 实体类 12345@FeignClient(name = &quot;microservice-provider-user&quot;,configuration = FooConfiguration.class)public interface UserFeignClient &#123; @PostMapping(value = &quot;/get&quot;) public User get(@RequestBody User user);&#125;","tags":["Feign"],"categories":["微服务"]},{"title":"Ribbon实现负载均衡","path":"/posts/d09d9c9a/","content":"目录 [TOC] 为了实现微服务架构的高可用性，一般在生产环境中，各个微服务会部署多个实例。这里我们需要用到负载均衡，将服务消费者的请求分摊到多个服务提供者实例上。 RibbonRibbon 是Netflix发布的负载均衡器，它有助于控制HTTP和TCP客户端的行为。Ribbon配置好后，它可以根据如轮询，随机等负载均衡算法自动帮助服务消费去请求。在Spring Cloud中，Ribbon 与Eureka可以很好的配合 ，Ribbon向Eureka注册后可以自动从Eureka中获取服务提供者的地址列表，然后基于某种负载均衡算法去请求其中一个服务提供者实例。 集成Ribbon在Spring Cloud中使用Ribbon很简单，只需要做一点小修改。将 微服务简单实例–电影购票 中的服务消费者为基础进行修改。##添加依赖 1234&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t&lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt;&lt;/dependency&gt; 由于 Eureka已经包含了Ribbon 所以这里其实不需要添加上面这个依赖了。 添加注解还需要为RestTemplate添加@LoadBalanced注解。 12345678910111213141516171819@EnableDiscoveryClient@SpringBootApplicationpublic class MicroserviceConsumerMovieRibbonApplication &#123;\t/** * @Bean 是一个方法注解，作用是实例化一个Bean并使用该方法的名称命名。在本例中，添加@Bean注解的restTemplate()方法，等价于RestTemplate restTemplate = new RestTemplate(); * @LoadBalanced 可以为RestTemplate整合Ribbon 使其具备负载均衡的能力。 * @return */\t@Bean\t@LoadBalanced\tpublic RestTemplate restTemplate()&#123; return new RestTemplate();\t&#125;\tpublic static void main(String[] args) &#123; SpringApplication.run(MicroserviceConsumerMovieRibbonApplication.class, args);\t&#125;&#125; 修改Controller1234567891011121314151617181920@RestControllerpublic class UserController &#123; private static final Logger LOGGER = LoggerFactory.getLogger(UserController.class); @Autowired private RestTemplate restTemplate; @Autowired private LoadBalancerClient loadBalancerClient; @GetMapping(&quot;/user/&#123;id&#125;&quot;) public User findById(@PathVariable Long id)&#123; return this.restTemplate.getForObject(&quot;http://microservice-provider-user/&quot; + id,User.class); &#125; @GetMapping(&quot;/log-user-instance&quot;) public void logUserinstance()&#123; ServiceInstance serviceInstance = this.loadBalancerClient.choose(&quot;microservice-provider-user&quot;); UserController.LOGGER.info(&quot;&#123;&#125;:&#123;&#125;:&#123;&#125;&quot;,serviceInstance.getServiceId(),serviceInstance.getHost(),serviceInstance.getPort()); &#125;&#125; 这里需要把请求地址改成 http://microservice-provider-user/ 。microservice-provider-user是用户微服务(服务提供者)的虚拟主机名。当Ribbon与Eureka配合使用时，会自动将虚拟主机名映射成微服务的网络地址。在新增的方法logUserinstance() 方法中使用LoadBalancerClient 的API更加直观的获取当前选择的用户微服务节点。注意需要在服务提供者的配置文件中添加其虚拟主机名，并且命名不能使用 _ ： 123spring: application: name: microservice-provider-user 测试我们启动前文中 微服务学习笔记 –使用Spring Cloud Eureka实现服务注册与发现 的一个Eureka项目，启动多个服务提供者，因为是在本地做测试，需要把端口改成不一样的，避免端口占用。最后启动修改后的服务消费者。查看Eureka界面：可以看到在Eureka上注册了三个服务提供者，分别使用了不同的端口号。另外还注册了集成Ribbon后的服务消费者。访问localhost:8010&#x2F;user&#x2F;1 可以请求到数据多次请求后后台可以看到请求会随机分配给不同的服务提供者。可以通过访问 http://localhost:8010/log-user-instance 更加直观的查看负载均衡效果。多次访问 http://localhost:8010/log-user-instance ，查看后台日志如下：","tags":["Ribbon"],"categories":["微服务"]},{"title":"Centos 7离线安装Nginx 配置负载均衡集群","path":"/posts/6a085911/","content":"目录 [TOC] 场景 项目中有三台应用服务器，系统为Centos 7 ，应用地址分别为: 192.168.198.229:8080 192.168.198.230:8080 192.168.198.231:8080 应用使用tomcat部署，目前没有域名，都是使用IP在局域网中单独访问。因为没有单独的服务器可以用来部署Nginx,所以Nginx部署在229服务器上。 安装依赖包在安装Nginx前，需要先安装好一些依赖包。gcc依赖包 gcc-4.8.5-16.el7.x86_64.rpm glibc-devel-2.17-196.el7.x86_64.rpm glibc-headers-2.17-196.el7.x86_64.rpm kernel-headers-3.10.0-693.el7.x86_64.rpm 其它依赖包 pcre-devel-8.32-17.el7.x86_64.rpm zlib-devel-1.2.7-17.el7.x86_64.rpm openssl-fips-2.0.10.tar.gz 因为无法使用yum，我下载好后通过ftp上传到服务器。依赖包下载传送门：https://centos.pkgs.org/前四个为gcc安装包与相关依赖，最后一个openssl-fips如果使用rpm，还需要安装很多依赖包，因此使用压缩包安装更简单。gcc安装gcc安装验证：​ 其它依赖包安装 1234567891011121314[root@APP1 opt]# rpm -ivh pcre-devel-8.32-17.el7.x86_64.rpm 警告：pcre-devel-8.32-17.el7.x86_64.rpm: 头V3 RSA/SHA256 Signature, 密钥 ID f4a80eb5: NOKEY准备中... ################################# [100%]正在升级/安装...[root@APP1 opt]# rpm -ivh zlib-devel-1.2.7-17.el7.x86_64.rpm 警告：zlib-devel-1.2.7-17.el7.x86_64.rpm: 头V3 RSA/SHA256 Signature, 密钥 ID f4a80eb5: NOKEY准备中... ################################# [100%]正在升级/安装... 1:zlib-devel-1.2.7-17.el7 ################################# [100%] [root@APP1 opt]# tar zxvf openssl-fips-2.0.10.tar.gz [root@APP1 opt]# cd openssl-fips-2.0.10/[root@APP1 openssl-fips-2.0.10]# ./config &amp;&amp; make &amp;&amp; make install 安装Nginx安装好上述依赖包后就可以安装Nginx了。安装如下：使用tar将nginx-1.12.0.tar.gz 解压到 &#x2F;usr&#x2F;local&#x2F;目录，编译安装 12345[root@HMDMAPP1 opt]# tar -zxvf nginx-1.12.0.tar.gz -C /usr/local/[root@HMDMAPP1 opt]# cd /usr/local/nginx-1.12.0/[root@HMDMAPP1 nginx-1.12.0]# ./configure &amp;&amp; make &amp;&amp; make install[root@HMDMAPP1 nginx-1.12.0]# whereis nginxnginx: /usr/local/nginx 配置Nginx安装好后我们需要对Nginx进行配置。配置文件路径为：&#x2F;usr&#x2F;local&#x2F;nginx&#x2F;sconf&#x2F;nginx.conf主要配置点：1、upstream这里配置一组被代理的服务器地址 123456upstream mysvr &#123; server 192.168.198.229:8080 weight=1 max_fails=3 fail_timeout=15; server 192.168.198.230:8080 weight=1 max_fails=3 fail_timeout=15; server 192.168.198.231:8080 weight=1 max_fails=3 fail_timeout=15;\t&#125; 2、server 123456789101112server &#123; listen 80; #监听端口，与应用端口不同 server_name 192.168.198.229; #监听地址，一般是配置域名 #charset koi8-r; #access_log logs/host.access.log main; location / &#123; proxy_pass http://mysvr; #请求转向upstream配置中mysvr定义的服务器列表 &#125;&#125; 请求转向还有另外一种写法：如果upstream 中的服务器列表地址前加了http:// 则在server中的请求转向地址mysvr不需要加http:// 1234567891011upstream mysvr&#123;\tserver http://192.168.198.229:8080 weight=1 max_fails=3 fail_timeout=15; ... ...&#125;server&#123;\t....\tlocation / &#123; proxy_pass mysvr; #请求转向upstream配置中mysvr定义的服务器列表 &#125;&#125; 启动Nginx123[root@HMDMAPP1 /]# cd /usr/local/nginx/sbin[root@HMDMAPP1 sbin]# ./nginx Nginx常用命令查看进程: ps -aux |grep ‘nginx’重启nginx: .&#x2F;nginx -s reopen停止nginx: .&#x2F;nginx -s stop重新载入配置文件: .&#x2F;nginx -s reload 验证配置：.&#x2F;nginx -t 通过 192.168.198.229+应用地址 进行访问，我们可以在不同的服务器中的页面中添加标识来测试Nginx配置是否成功。下面访问test3.html页面不同刷新显示结果如下：可以看到访问地址没有变化，但Nginx把请求分配到了不同的服务器上。 本文中使用到了依赖包与Nginx.conf完整配置文件下载：https://download.csdn.net/download/ftdd_hw/10578071 推荐学习：Nginx部署与配置","tags":["Nginx"],"categories":["Nginx"]},{"title":"Spring Cloud Eureka实现服务注册与发现","path":"/posts/7d66ca0d/","content":"目录 [TOC] 服务发现组件是微服务架构中非常关键的一个组件。SpringCloud 提供的服务发现有多种，如Eureka，Consul和Zookeeper等。本篇介绍的是Eureka的使用。 服务发现简介服务提供者，服务消费者，服务发现组件这三者之间的关系大致如下： 服务提供者与服务消费者都需要向服务发现组件进行注册，服务消费者从服务发现组件中获取服务提供者的信息(如名称、地址、端口等)。在服务发现组件注册的微服务需要通过心跳机制来保持连接状态并更新注册信息，当服务发现组件长时间无法与某个微服务实例进行通信时，会注销这个实例。这种机制使得即使服务提供者信息发生变化，服务消费者也无须修改配置文件。 由以上得知，服务发现组件应具备以下功能： 服务注册表：用于记录各个微服务的注册信息，它还提供查询API与管理API。 服务注册与服务发现：服务注册是指微服务在启动是将自己的信息注册到服务发现组件的过程。服务发现是指查询可用的微服务列表及其网络地址的机制。 服务检查：服务发现组件应有一定的机制定时检测已注册的微服务，如长时间无法访问，就会从服务注册表中移除该实例。 Eureka原理以下是Eureka官方的架构图，比较详细的描述了Erueka集群的工作原理：我们可以把us-east-1c、us-east-1d与us-east-1e理解成独立的机房，而这整个图则是一个跨机房的Eureka集群。其中： Application Service 相当于前面说的服务提供者 Application Client 相当于前面的服务消费者 Make Remote Call 可以理解成调用RESTful API的行为 由于图中所示，我们可以知道Eureka包含两个组件:Eureka Server 和 Eureka Client，它们的作用如下： Eureka Server提供服务发现的能力 Eureka Client 是一个java客户端，用于简化与Eureka Server的交互 微服务在启动后，会同期性（默认30s）地向Eureka Server 发送心跳来续约自己的“租期” 如果Eureka Server 在一定时间内（默认90s）没有接收到某个微服务的实例心跳，Eureka Server将会注销该实例 默认情况下，Eureka Server同时也是Eureka Client。多个Eureka Server 实例，互相之间通过复制的方式，来实现服务注册表中的数据同步 Eureka Client 会缓存服务注册表的信息，所以微服务无须每次请求都查询Eureka Server，从而降低了Eureka Server的压力。另外，即使Eureka Server 所以节点都挂了，服务消费者仍然可以使用缓存中的信息找到服务提供者并完成调用 Eureka Server实现环境 Idea Spring Boot ：1.5.9.RELEASE Spring Cloud：Edgware.RELEASE JDK ：1.8 引入依赖在Idea新建 一个Spring Boot项目，添加以下依赖： 12345678910111213141516171819&lt;dependencies&gt; &lt;!--引入Eureka依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;!-- 引入spring cloud的依赖 --&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Edgware.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 编写启动类我们需要在启动类上加上 @EnableEurekaServer 注解，声明这是一个Eureka Server 1234567@SpringBootApplication@EnableEurekaServerpublic class MicroserviceDiscoveryEurekaApplication &#123;\tpublic static void main(String[] args) &#123; SpringApplication.run(MicroserviceDiscoveryEurekaApplication.class, args);\t&#125;&#125; 添加配置 12345678server: port: 8761eureka: client:\tregister-with-eureka: false fetch-registry: false serviceUrl: defaultZone: http://localhost:8761/eureka/ eureka.client.register-with-eureka 表示是否将自己注册到Eureka Server，默认为true,由于当前应用就是Eureka Server，所以设为falseeureka.client.fetch-registry 表示是否从Eureka Server获取注册信息，默认为true,因为这是一个单点的Eureka Server，不需要同步其他的Eureka Server节点的数据，所以设定为falseeureka.client.serviceUrl.defaultZone 设置与Eureka Server交互的地址，查询服务和注册服务都需要依赖这个地址，多个地址可以使用 , 分隔 启动项目，访问http://localhost:8761/ 即可看到Eureka Server的首页。可以看到界面展示了实例的状态，可用与不可用的Eureka节点，注册的服务实例列表，常用信息等，当然，目前还没有服务向这个Server注册过，下面记录实现服务注册。 微服务注册Eureka Server建好后，我们的微服务就可以向这个Eureka Server进行注册了。我们把之前文章(微服务简单实例–电影购票)中实现的服务提供者修改一下。添加依赖 12345&lt;!--注册Eureka Server--&gt;&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt;\t&lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 修改配置在之前项目配置的基础上添加以下配置 123456789spring: application: name: microservice-provider-usereureka: client: service-url: defaultZone: http://peer1:8761/eureka/ instance: prefer-ip-address: true eureka.client.instance.prefer-ip-address表示将自己的IP注册到Eureka Serverspring.application.name 是指定一个应用名称eureka.client.serviceUrl.defaultZone 设置Eureka Server的地址，多个Eureka地址使用,分隔 修改启动类同样的，在微服务的启动类上加上一个注解 @EnableDiscoveryClient 表示这是一个Eureka Client。上面也说到Eureka 包含Server 与 Client两个组件，Client本身是一个JAVA客户端，把它集成到微服务中，简化了微服务与Eureka Server 的交互。 12345678@EnableDiscoveryClient@SpringBootApplicationpublic class MicroserviceSimpleProviderUserApplication &#123;\tpublic static void main(String[] args) &#123; SpringApplication.run(MicroserviceSimpleProviderUserApplication.class, args);\t&#125;&#125; 完成以上修改后，启动服务提供者项目。刷新Eureka Server可以看到在 Instances currently registered with Eureka 栏目下出现了服务提供者的名称，地址，状态等信息。这样，一个微服务就注册到Eureka Server上了。另外如果是非JAVA服务注册到Eureka Server，可以使用Eureka的api进行注册。 Erueka高可用部署前面我们写了一个Eureka Server，并将一个微服务注册到了Eureka Server，在实际环境中，Eureka 需要是一个高可用的集群环境。这样Eureka Server 宕机时，其它的Eureka节点还是能够继续提供服务，虽然Eureka Client有缓存注册表信息，也可以提供服务查询，但缓存不及时更新也会影响之后的服务调用。前面在编写Eureka Server时配置了 eureka.client.register-with-eureka=false 和eureka.client.fetch-registry=false，现在在多节点的环境中，要实现Eureka实例之间相互注册彼此增量地同步信息，才能确保各节点数据一致，实现Eureka的高可用部署。所以下面的集群环境中，这两个配置应该为 true 或不配置默认为 true 。修改hosts因为是在本机实现多节点的Eureka Server集群，需要修改一下系统的hosts文件，添加以下配置： 1127.0.0.1 peer1 peer2 peer3 如修改hosts无法保存，需要添加用户修改权限。添加成功后可以ping peer1试试是否配置生效。 修改配置将上面编写的Eureka Server 项目的application.yml修改如下 : 12345678910111213141516171819202122232425262728293031323334353637383940414243444546spring: application: name: microservice-discovery-eureka---spring: profiles: peer1 # 指定profile=peer1server: port: 8761eureka: instance: hostname: peer1 # 指定当profile=peer1时，主机名是peer1 prefer-ip-address: true client: serviceUrl: defaultZone: http://peer2:8762/eureka/,http://peer3:8763/eureka/ # 将自己注册到peer2和peer3这两个Eureka上面去 register-with-eureka: true fetch-registry: true---spring: profiles: peer2server: port: 8762eureka: instance: hostname: peer2 prefer-ip-address: false client: serviceUrl: defaultZone: http://peer1:8761/eureka/,http://peer3:8763/eureka/ register-with-eureka: true fetch-registry: true---spring: profiles: peer3server: port: 8763eureka: instance: hostname: peer3 prefer-ip-address: false client: serviceUrl: defaultZone: http://peer1:8761/eureka/,http://peer2:8762/eureka/ register-with-eureka: true fetch-registry: true YAML 文件可以由一或多个文档组成（即相对独立的组织结构组成），文档间使用---（三个横线）在每文档开始作为分隔符。同时，文档也可以使用...（三个点号）作为结束符（可选）。所以这里的配置文件实际上是三个配置文件组成，---分隔符不可少，否则编译出错。也可以把这几段配置写在三个YAML文件中。 启动启动项目时通过给spring.profiles.active 传递不同的参数，以使用不同的配置。在IDEA中我们可以在启动前配置参数：添加三个Spring Boot Application，并选择启动类，在Active Profiles中分别配置peer1,peer2,peer3参数后，分别启动。在启动时，先启动成功的Eureka 可能会报以下错误： ​\t2018-07-05 14:32:10.983 ERROR 13768 — [-target_peer2-8] c.n.e.cluster.ReplicationTaskProcessor : Network level connection to peer peer2; retrying after delay​\tcom.sun.jersey.api.client.ClientHandlerException: java.net.SocketTimeoutException: Read timed out 这是因为先启动成功的peer1根据配置会向peer2和peer3进行注册请求，而此时这两个应用可能还没启动好，所以出现上面的错误，三个应用启动成功后就正常了。正常情况下如下图：显示了注册到该Eureka的实例信息，Eureka各节点地址。在下面可以看到显示了两个注册的节点，同时这两个节点是可用的。我之前测试时出现不可用(unavailable-replicas)的节点，如下：解决情况是将 eureka.instance.prefer-ip-address 设置为 false ，因为是在本地配置集群环境，IP是相同的，因此使用IP注册可能会导致节点不可识别，另外，三个应用的spring.application.name不一致也会出现 unavailable-replicas 的情况 服务注册到Erueka 集群前面我们将服务提供者注册到了一个单节点的Eureka Server ，在Eureka的集群环境中，只需要稍加修改即可。把服务提供者的 eureka.client.serviceUrl.defaultZone配置修改如下: 1234eureka: client: service-url: defaultZone: http://peer1:8761/eureka/,http://peer2:8762/eureka/,http://peer3:8763/eureka/ 其实可以继续使用localhost，因为是在本地测试。另外，我们可以只写一个Eureka Server的地址，因为各Eureka Server节点之间会自动同步注册的实例信息，正常情况下这两种方式是一样。建议全部写上。 启动服务提供者，刷新三个Eureka Server可以看到服务已经注册了: Eureka用户认证在生产环境中Eureka Server需要登录才能访问，Eureka的用户认证可以使用Security实现。下面稍微修改一下Eureka Server添加依赖 1234&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 添加配置添加以下配置在第一段中，作为三个Eureka Server的公共配置。 123456security: basic: enabled: true #开启HTTP basic的认证 user: name: user #配置登录账号证 password: admin #配置登录密码 最后把Eureka以及服务提供者的配置文件中的 defaultZone 地址修改成如下格式： http://user:admin@peer1:8761/eureka/ 这样才能注册到需要认证的Eureka Server。 重新启动Eureka Server和服务提供者后，再次访问三个Eureka可以看到弹出了登录框，输入上面的账号与密码后登录后即可进行Eureka Server首页，服务已正常注册成功。 Eureka自我保护模式在实现上面的练习过程中，打开Eureka可能会遇到如下情况：这是Eureka开启自我保护模式时的警告。前面说过Eureka Server在一定时间内没有接收到某个微服务实例心跳，Eureka Server将会注销该实例。但如果出现网络分区故障，微服务与Eureka Server之间无法正常通信，微服务其实是正常的，但由于无法接收到该微服务的通信，Eureka Server则会注销该服务。为了避免这种情况的发生，Eureka Server 通过 自我保护模式 来解决这个问题。当Eureka Server在短时间内丢失过多客户端时，那么这个Eureka节点会进行自我保护模式。Eureka 会保护服务注册表中的信息，不再删除服务注册表中的数据，当网络故障恢复后，该Eureka节点会自动退出自我保护模式。这种模式使得Eureka 集群更加健壮，稳定。另外也可以选择禁用该模式，配置如下： ​\teureka.server.enable-self-preservation &#x3D; false Eureka健康检查登录Eureka首页可以看到注册的微服务状态是 UP表示应用程序状态正常，应用的状态还有其它取值，如DOWN,OUT_OF_SERVICE,UNKNOWN等。只有标记为 UP 的微服务会被请求。默认情况下，服务器端与客户端的心跳保持正常，应用程序状态就会显示UP 状态。但这个机制并不能完全反应微服务的状态，举个栗子，微服务与Eureka Server心跳正常，但微服务的数据源出现了问题（如数据库被关闭）。这个微服务其实无法正常工作，但Eureka Server认为该微服务为UP状态。为了解决这个问题，我们可以把微服务的健康状态传播到Eureka Server，只需要在微服务中作如下配置： 1234eureka: client: healthcheck: enabled: true 这样，当微服务健康状态出问题时，Eureka Server能够实时反应其它真实状态。 参考：官方API文档 以上，为《Spring Cloud与Docker微服务架构实战》第4章学习笔记。","tags":["Eureka"],"categories":["微服务"]},{"title":"Spring Boot Actuator监控端点","path":"/posts/297fa226/","content":"目录 [TOC] 微服务的这种架构虽然解决了单体应用的一些劣势，但它也面临一些挑战，比如对运维的要求更高了。一个微服务架构中可能有几十个上百个应用构成，要保证这些应用都正常运行，相互协调是比较麻烦的事情，因此我们需要一个组件来对这些应用进行监控和管理。spring-boot-starter-actuator 就是Spring Boot提供这个功能的模块。 示例运行环境： Spring Boot 2.0.3.RELEASE 1、引入依赖 1234&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 2、启动应用重新启动应用访问 http://localhost:8000/actuator/health 会显示如下信息： 123&#123;\tstatus: &quot;UP&quot;&#125; Actuator监控管理默认的访问路径是在 /actuator 下。在测试Spring Boot 1.5.9版本时是直接访问端点路径，不需要加 /actuator 配置除了health端点外，Actuator还为我们提供了很多端点，有些可以直接访问，有些需要授权或通过配置才能访问。 端点列表 端点 描述 actuator 为其他端点提供基于超媒体的“发现页面”。要求Spring HATEOAS在类路径上 auditevents 公开当前应用程序的审核事件信息 autoconfig 显示自动配置报告，显示所有自动配置候选项以及它们“未被”应用的原因 beans 显示应用程序中所有Spring bean的完整列表 configprops 显示所有配置信息。 dump 打印线程栈 env 查看所以环境变量 health 显示应用程序运行状况信息 info 显示应用信息 loggers 显示和修改应用程序中记录器的配置 liquibase 显示已应用的任何Liquibase数据库迁移 metrics 显示当前应用程序的“指标”信息 mappings 显示所有@RequestMapping路径的整理列表 shutdown 允许应用程序正常关闭（默认情况下不启用） trace 显示跟踪信息（默认情况下是最近的100个HTTP请求 Actuator配置自定义默认路径 1management.endpoints.web.base-path = /application 修改后访问端点的默认路径不再是 /actuator 而是 /application 自定义访问端口号 1management.server.port= 8012 修改后我们查看Actuator需要修改成8012端口进行访问，如http://localhost:8012/actuator/health 关闭验证 1management.security.enabled= false 默认情况下只开放了health 与 info 端口，关闭验证后，其它的也可以访问了，但不安全，最好添加 security 验证 控制端点是否开放 1management.endpoints.web.exposure.include= &#x27;info&#x27; 表示只暴露info端口，如添加其它端口使用 , 分隔，暴露所有端口使用 * 端口属性配置 1management.endpoint.端口名.属性=值 如 management.endpoint.health.show-details= always 表示显示 health端口的详细信息，大多数端口可以这样配置。 整合Spring Security监控端点的很多信息住信比较隐私，不能让没有权限的人随意查看，因此可以添加Srping Security进行控制。添加依赖 1234&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 配置Security 12345spring: security: user: name: user password: 123 配置security后，访问端口需要进行登陆验证。 总结另外还有其它的端口配置，还可以自定义端口等，这里不一一总结，以后需要时查查资料。贴上一个学习链接： http://blog.didispace.com/spring-boot-actuator-1/","tags":["Actuator"],"categories":["微服务"]},{"title":"微服务简单实例--电影购票","path":"/posts/5a5916f3/","content":"[TOC] 通过上一篇文章理解了微服务后我们通过一个简单的电影购票场景来实现微服务。 如图：这个场景当中，用户微服务是一个服务提供者，电影微服务是一个服务消费者，之前我们也说到，每个微服务从开发，测试，构建，部署，都应当独立运行，即每个微服务是单独的子项目。下面来实现这个场景。 一、编写服务提供者新建一个Spring Boot （版本1.5.9.RELEASE) 项目，不知道如何在IDEA中新建的可以看这篇&gt;&gt;传送门：：Spring Boot 入门知识 添加依赖项目使用H2作为数据库，使用jpa作为持久层框架。Spring Boot环境中H2数据库的基本配置可参考这篇&gt;&gt;传送门：Spring Boot环境下的 H2数据库基本配置 1234567891011121314151617181920212223242526&lt;dependencies&gt;&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;\t&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;\t&lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt;\t&lt;groupId&gt;com.h2database&lt;/groupId&gt;\t&lt;artifactId&gt;h2&lt;/artifactId&gt;&lt;/dependency&gt;&lt;/dependencies&gt;&lt;!--引入SpringCloud 依赖--&gt;&lt;dependencyManagement&gt;\t&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Edgware.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt;\t&lt;/dependencies&gt;&lt;/dependencyManagement&gt; 配置application.yml 文件配置如下 ： 123456789101112131415161718server: port: 8000spring: jpa: generate-ddl: false show-sql: true hibernate: ddl_auto: none h2: console: path: /h2-console #h2 web控制台路径 enabled: true #开启 Web Console settings: web-allow-others: true #允许远程访问 Web Console datasource: platform: h2 #指定数据源类型 schema: classpath:schema.sql #指定数据库的数据脚本 data: classpath:data.sql #指定数据库的数据脚本 spring.h2.console.path 指定了h2控制台的路径，可以通过localhost:8000&#x2F;h2-console 去访问H2的控制台。spring.datasource.schema 与 datasource.data 会在每次启动项目时都会被执行 schema.sql 12drop table user if exists;create table user (id bigint generated by default as identity, username varchar(40), name varchar(20), age int(3), balance decimal(10,2), primary key (id)); data.sql 123insert into user (id, username, name, age, balance) values (1, &#x27;account1&#x27;, &#x27;张三&#x27;, 20, 100.00);insert into user (id, username, name, age, balance) values (2, &#x27;account2&#x27;, &#x27;李四&#x27;, 28, 180.00);insert into user (id, username, name, age, balance) values (3, &#x27;account3&#x27;, &#x27;王一&#x27;, 32, 280.00); pojo123456789101112131415161718@Entity@JsonIgnoreProperties(value=&#123;&quot;hibernateLazyInitializer&quot;,&quot;handler&quot;&#125;)public class User &#123; @Id @GeneratedValue(strategy = GenerationType.AUTO) private Long id; @Column private String username; @Column private String name; @Column private Integer age; @Column private BigDecimal balance;//...get//...set&#125; 报错​ com.fasterxml.jackson.databind.exc.InvalidDefinitionException: No serializer found for class org.hibernate.proxy.pojo.javassist.JavassistLazyInitializer and no properties discovered to create BeanSerializer如果在运行时报以上错误，则需要添加注释：@JsonIgnoreProperties(value&#x3D;{“hibernateLazyInitializer”,”handler”}) 这是因为 hibernate会给每个被管理的对象加上hibernateLazyInitializer属性，jsonplugin通过java的反射机制将pojo转换成json，会把hibernateLazyInitializer也拿出来操作,但是hibernateLazyInitializer无法由反射得到，就会抛异常了。 Dao123@Repositorypublic interface UserRepository extends JpaRepository&lt;User,Long&gt;&#123;&#125; Controller1234567891011@RestControllerpublic class UserController &#123; @Autowired private UserRepository userRepository; @GetMapping(&quot;/&#123;id&#125;&quot;) public User findById(@PathVariable Long id)&#123; User findOne = this.userRepository.getOne(id); return findOne; &#125;&#125; @GetMapping() 等同于 @RequestMapping(method &#x3D; {RequestMethod.GET}) 测试运行项目，访问测试 二、编写服务消费者消费者作为服务调用方，这里只使用最简单的方式来实现。 添加依赖1234567891011121314151617181920212223&lt;dependencies&gt;\t&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;\t&lt;/dependency&gt;\t&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;\t&lt;/dependency&gt;&lt;/dependencies&gt;&lt;!--引入SpringCloud 依赖--&gt;&lt;dependencyManagement&gt;\t&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Edgware.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt;\t&lt;/dependencies&gt;&lt;/dependencyManagement&gt; 配置application.yml 1234server: port: 8011user: userServiceUrl: http://localhost:8000/ user.userServiceUrl ：把调用地址写入配置文件 pojo这里与上面的服务提供者相同 RestTemplate实例化RestTemplate在启动类中添加以下方法： 1234@Beanpublic RestTemplate restTemplate()&#123;\treturn new RestTemplate();&#125; Controller123456789101112@RestControllerpublic class UserController &#123; @Autowired private RestTemplate restTemplate; @Value(&quot;$&#123;user.userServiceUrl&#125;&quot;) private String userServiceUrl; @GetMapping(&quot;/user/&#123;id&#125;&quot;) public User findById(@PathVariable Long id)&#123; return this.restTemplate.getForObject(this.userServiceUrl + id,User.class); &#125;&#125; 这里使用 restTemplate 来调用服务@Value(“${user.userServiceUrl}”) 从配置文件中取user.userServiceUrl值 测试启动项目进行测试 三、总结至此，一个简单的微服务完成！是不是觉得与我们平时写接口是差不多的，平时我们是在整个系统内部，各个功能模块之间进行接口调用，微服务则是把这些模块单独出来成为一个子系统，每个子系统提供接口给其它系统调用。在整个电影购票系统中使用单一职责原则：两个微服务只关注整个系统中单独，有界限的一部分。满足服务自治原则：每个微服务具备独立的业务能力，依赖与运行环境。使用了轻量级通信机制：消费者中使用了Rest 进行服务调用微服务粒度：两个微服务都有明确的功能划分。 当然微服务不只是这么简单，还应该包括安全性，高可用性等，还需要集成其它的组件，后面会边学习边作记录。","tags":["微服务"],"categories":["微服务"]},{"title":"Spring Boot环境下的 H2数据库基本配置","path":"/posts/41733482/","content":"目录 [TOC] H2是一个开源的、纯Java实现的关系数据库。 用途1、它可以与应用程序打包一起发布，这样可以很方便存储少量的结构化数据。2、还可以用于单元测试，启动速度快，而且可以关闭持久化功能，每一个用例执行完随即还原到初始状态3、可以作为缓存，作为NoSQL的一个补充。 特点1、纯Java编写，不受平台限制2、只有一个Jar文件，适合作为嵌入式数据库使用3、提供了一个完善的基于浏览器的Console应用4、支持标准的sql和jdbc5、支持内嵌模式，服务器模式和集群 二、下载安装下载地址http://www.h2database.com/html/main.html 我们可以下载win安装包，也可以直接下载zip，解压即用。我使用的是ZIP的压缩包。 安装前面我们说过H2是纯Java编写的，所以在安装H2之前需要配置Java环境，具体Java环境配置这里就不写了。H2的ZIP安装方式很简单，直接把下载下来的ZIP解压到安装目录下就可以了。启动H2 bin&#x2F;h2.bat 三、使用在Spring Boot环境中配置H2 引入依赖 &lt;dependency&gt; &lt;groupId&gt;com.h2database&lt;/groupId&gt; &lt;artifactId&gt;h2&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;/dependency&gt; 这里还引入了jpa 用来对H2进行操作 服务器模式其中URL使用的形式表示这里使用的是H2的服务器模式。连接的是我们上面安装的本地的H2数据库，因此我们在启动程序前必须把本地的H2数据库先启动。URL中指定了数据存放位置，如果数据库test不存在会自动创建。 配置中指定两个SQL脚本，每次启动程序都会重新执行脚本对数据进行初始化，当然我们可以不指定。schema.sql 创建了一个user表结构data.sql 向user表插入了三条数据 查看Console在前面启动H2数据库的时候会自动弹出consol管理页，按程序中指定的URL进行连接即可查看到我们创建的数据用户名 sa 是默认的用户，密码默认为空。登入进Console后我们可以查看user表 在配置文件中配置了Web Console的路径，同样我们可以使用这个路径进行登入。 内嵌模式H2最方便的是只需要一个jar包就可以使用了。前面我们引入H2依赖后其它就可以使用H2了，不需要连接我们本地安装的H2。可以直接不指定URL或者修改一URL为：我们可以使用Web Console连接,连接的url为 jdbc:h2:mem:testdb 当我们没有指定URL的时候，默认的连接的数据库也是testdb 之前看教程说在把日志改成DEBUG可以看到连接的URL，但我试了还是没有找到，只只根据教程上的URL也连接，确定连接上了对应的数据库","tags":["H2","Spring Boot"],"categories":["H2"]},{"title":"RabbitMQ学习系列 五 RabbitMQ整合Spring","path":"/posts/d72bda77/","content":"最后学习一下RabbitMQ如何整合Spring，毕竟现在大多是使用框架来做项目。这篇主要使用的方式是XML配置。 [TOC] 介绍RabbitMQ整合Spring的学习中，搭了两个web项目，一个作为客户端，一个作为服务端，放在一个项目中也可以实现效果，但毕竟RabbitMQ也是在这种类似的环境中使用的。客户端会把info类型和error类型的日志发送给RabbitMQ，RabbitMQ根据所定义的路由与绑定的key分别把日志消息传递给不同的队列。客户端项目结构: 客户端实现RabbitMQ配置文件config.properties 12345# RabbitMQ configrabbitmq.host=localhostrabbitmq.username=guestrabbitmq.password=guestrabbitmq.port=5672 这里在我本机的RabbitMQ，如果是在远程主机上则要做相应修改。需要注意的是，我们访问RabbitMQ管理界面是使用的15672端口，但通过连接访问RabbitMQ是使用5672端口 XML配置123456789101112131415161718192021222324252627282930313233343536373839&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:rabbit=&quot;http://www.springframework.org/schema/rabbit&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/rabbit http://www.springframework.org/schema/rabbit/spring-rabbit-1.7.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd&quot;&gt; &lt;context:component-scan base-package=&quot;com.rabbitmq.spring&quot;/&gt; &lt;context:property-placeholder location=&quot;classpath*:config.properties&quot;/&gt; &lt;mvc:annotation-driven /&gt; &lt;mvc:default-servlet-handler/&gt; &lt;!--连接工厂--&gt; &lt;rabbit:connection-factory id=&quot;connectionFactory&quot; host=&quot;$&#123;rabbitmq.host&#125;&quot; username=&quot;$&#123;rabbitmq.username&#125;&quot; password=&quot;$&#123;rabbitmq.password&#125;&quot; port=&quot;$&#123;rabbitmq.port&#125;&quot;&gt;&lt;/rabbit:connection-factory&gt; &lt;!--RabbitAdmin主要用于创建队列和交换器以及绑定关系等。--&gt; &lt;rabbit:admin id=&quot;rabbitAdmin&quot; connection-factory=&quot;connectionFactory&quot;/&gt; &lt;!--声明队列--&gt; &lt;rabbit:queue name=&quot;rabbitmq_log_info&quot; durable=&quot;true&quot; auto-delete=&quot;false&quot; /&gt; &lt;rabbit:queue name=&quot;rabbitmq_log_error&quot; durable=&quot;true&quot; auto-delete=&quot;false&quot; /&gt; &lt;!--声明路由并绑定队列，指定routingKey--&gt; &lt;rabbit:direct-exchange name=&quot;hap.log.exchange&quot; auto-delete=&quot;false&quot; durable=&quot;true&quot;&gt; &lt;rabbit:bindings&gt; &lt;rabbit:binding queue=&quot;rabbitmq_log_info&quot; key=&quot;info&quot;&gt;&lt;/rabbit:binding&gt; &lt;rabbit:binding queue=&quot;rabbitmq_log_error&quot; key=&quot;error&quot;&gt;&lt;/rabbit:binding&gt; &lt;/rabbit:bindings&gt; &lt;/rabbit:direct-exchange&gt; &lt;!--定义RabbitTemplate，用于发送与接收消息--&gt; &lt;rabbit:template id=&quot;rabbitTemplateLogInfo&quot; connection-factory=&quot;connectionFactory&quot; routing-key=&quot;info&quot; exchange=&quot;hap.log.exchange&quot; message-converter=&quot;jsonMessageConverter&quot;&gt;&lt;/rabbit:template&gt; &lt;rabbit:template id=&quot;rabbitTemplateLogError&quot; connection-factory=&quot;connectionFactory&quot; routing-key=&quot;error&quot; exchange=&quot;hap.log.exchange&quot; message-converter=&quot;jsonMessageConverter&quot;&gt;&lt;/rabbit:template&gt; &lt;!-- 消息对象json转换类 --&gt; &lt;bean id=&quot;jsonMessageConverter&quot; class=&quot;org.springframework.amqp.support.converter.Jackson2JsonMessageConverter&quot; /&gt;&lt;/beans&gt; rabbit-admin 标签如不声明,则 rabbit:queue 与 rabbit:direct-exchange 标签中必须添加 auto-declare 属性为true ,表示如果队列或路由不存在则自动声明，如不声明rabbit-admin，也不添加auto-declare属性则启动时会报声明队列错误，或队列不存在。 rabbit:template 标签中的routing-key、exchange也可以不在XML中配置，在类中发送消息时可以作为参数代入。则XML中只需要配置一个rabbit:template标签即可 ServiceService接口 12345public interface ISendMessageService &#123; public void sendInfoMessage(String message); public void sendErrorMessage(String message);&#125; Service实现类 1234567891011121314151617181920212223@Servicepublic class SendMessageService implements ISendMessageService&#123; @Autowired @Qualifier(&quot;rabbitTemplateLogInfo&quot;) public RabbitTemplate rabbitTemplateLogInfo; @Autowired @Qualifier(&quot;rabbitTemplateLogError&quot;) private RabbitTemplate rabbitTemplateLogError; @Override public void sendInfoMessage(String message) &#123; System.out.println(&quot;Info发送消息中&gt;&gt;&gt;&quot; + message); this.rabbitTemplateLogInfo.convertAndSend(message); &#125; @Override public void sendErrorMessage(String message) &#123; System.out.println(&quot;Error发送消息中&gt;&gt;&gt;&quot; + message); this.rabbitTemplateLogError.convertAndSend(message); &#125;&#125; RabbitTemplate的convertAndSend方法中，如果XML中已经配置好了对应的exchange与routingKey则可以直接传入一个消息进行发送即可。如果没有可以在参数中加入Exchange 与 routingkey Controller1234567891011121314151617181920@Controller@RequestMapping(&quot;/rabbitmqLog&quot;)public class RabbitmqController &#123; @Autowired @Qualifier(&quot;sendMessageService&quot;) ISendMessageService service = new SendMessageService(); @RequestMapping(value = &quot;/sendInfoLog&quot;,method = RequestMethod.GET) public void sendInfoMessage(String message)&#123; service.sendInfoMessage(message); &#125; @RequestMapping(&quot;/sendErrorLog&quot;) public void sendErrorMessage(String message)&#123; service.sendErrorMessage(message); &#125;&#125; 服务端实现因为这里只是做一个简单的示例，所以服务端只做了监听，没有做什么业务逻辑。 RabbitMQ配置文件这里与客户端是一样的 XML配置123456789101112131415161718192021222324252627282930313233&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:rabbit=&quot;http://www.springframework.org/schema/rabbit&quot; xmlns:context=&quot;http://www.springframework.org/schema/context&quot; xmlns:mvc=&quot;http://www.springframework.org/schema/mvc&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd http://www.springframework.org/schema/rabbit http://www.springframework.org/schema/rabbit/spring-rabbit-1.7.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.2.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.2.xsd&quot;&gt; &lt;context:component-scan base-package=&quot;com.rabbitmq.spring&quot;/&gt; &lt;context:property-placeholder location=&quot;classpath*:config.properties&quot;/&gt; &lt;mvc:annotation-driven /&gt; &lt;mvc:default-servlet-handler/&gt; &lt;rabbit:connection-factory id=&quot;connectionFactory&quot; host=&quot;$&#123;rabbitmq.host&#125;&quot; username=&quot;$&#123;rabbitmq.username&#125;&quot; password=&quot;$&#123;rabbitmq.password&#125;&quot; port=&quot;$&#123;rabbitmq.port&#125;&quot;&gt;&lt;/rabbit:connection-factory&gt; &lt;rabbit:admin id=&quot;rabbitAdmin&quot; connection-factory=&quot;connectionFactory&quot;/&gt; &lt;rabbit:queue name=&quot;rabbitmq_log_info&quot; durable=&quot;true&quot; auto-delete=&quot;false&quot; /&gt; &lt;rabbit:queue name=&quot;rabbitmq_log_error&quot; durable=&quot;true&quot; auto-delete=&quot;false&quot; /&gt; &lt;rabbit:listener-container connection-factory=&quot;connectionFactory&quot; acknowledge=&quot;auto&quot;&gt; &lt;rabbit:listener ref=&quot;messageRecevicer&quot; queues=&quot;rabbitmq_log_info&quot;/&gt; &lt;rabbit:listener ref=&quot;messageRecevicer&quot; queues=&quot;rabbitmq_log_error&quot;/&gt; &lt;/rabbit:listener-container&gt; &lt;bean id=&quot;messageRecevicer&quot; class=&quot;com.rabbitmq.spring.listener.QueueListener&quot;/&gt; &lt;!-- 消息对象json转换类 --&gt; &lt;bean id=&quot;jsonMessageConverter&quot; class=&quot;org.springframework.amqp.support.converter.Jackson2JsonMessageConverter&quot; /&gt;&lt;/beans&gt; 服务端的XML与客户端不同的是多了监听配置与监听类的Bean，少了路由声明与队列绑定的配置。 监听类12345678910111213@Componentpublic class QueueListener implements MessageListener&#123; @Override public void onMessage(Message message) &#123; String msg = null; try &#123; msg = new String(message.getBody(),&quot;UTF-8&quot;); &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;监听到 &quot;+ message.getMessageProperties().getConsumerQueue()+&quot; 队列消息:&quot; + msg); &#125;&#125; 测试最后我们分别启动客户端与服务端。客户端调用Controller向服务端发送消息 。","tags":["RabbitMQ"],"categories":["MQ"]},{"title":"RabbitMQ学习系列 三 发布 订阅","path":"/posts/5dabf99c/","content":"[TOC] 上一篇文章[[RabbitMQ学习系列 二 “Hello World”]]记录了一个简单的rabbitmq 发布接收队列消息，但没有使用路由。本篇写一写rabbitmq的路由的使用。 介绍有几个概念介绍一下1、生产者​ ​\t生产者是发送消息的用户的应用程序 2、路由​ ​\t处理生产者消息发到哪个队列 3、队列 队列是存储消息的缓冲器 4、消费者 消费者是接收消息的用户的应用程序 RabbitMQ中的消息传递模型的核心思想是生产者永远不会将任何消息直接发送到队列中。实际上，生产者通常甚至不知道消息是否会被传送到任何队列中。我们的消息实际上是从生产者传递到路由，路由会绑定队列并指定绑定的routingKey。根据routingkey匹配到这个路由上绑定的队列，并向队列发送消息，不能匹配上的队列则不会收到该消息。所以我们上一篇文章中虽然没有明确定义路由，实际上是使用是默认的路由。我们可以根据需求自己声明相应的路由。 路由(Exchange)声明方式exchangeDeclare(String exchange, String type, boolean durable, boolean autoDelete, Map&lt;String, Object&gt; arguments) exchange :路由名type : 路由类型durable: 是否持久化autoDelete：是否自动删除arguments: 其它参数 类型1、Fanout​\t&gt; 广播。这个类型的路由会忽略routingKey。收到生产者消息后会直接发送给绑定在该路由上的所有队列 2、Direct 单播。该类型的路由会根据routingKey去匹配队列，将消息发送给该路由上绑定的并且routingKey完全匹配上的队列 3、Topic 多播。该类型路由的routingKey可以使用通配符进行匹配。即 * 代表一个单词，# 代表多个单词。routingKey 的定义不能是任意字符，只能是由点号分隔的字符串，如: “ stock.usd.nyse ”，“ nyse.vmw ”，“ quick.orange.rabbit ”。如下图Q1队列的与路由绑定的routingKey 是****.orange.****Q2队列的与路由绑定的routingKey 是 ..rabbit 与 lazy.#如果我们发送消息时指定的routingKey为:quick.orange.rabbit ，则消息会被路由发送到Q1与Q2两个队列中。如果我们发送消息时指定的routingKey为:lazy.brown.fox ，则消息会被路由发送到Q2队列如果指定的routingKey为 lazy.pink.rabbit，也会被发送到Q2，但只会发送一次，即使它匹配到了两个绑定的routingKey 4、Headers 这种类型的路由不处理路由键，而是根据发送消息的Headers属性进行匹配，在队列绑定交换机的时候会指定一组键对值; 绑定队列queueBind(String queue, String exchange, String routingKey) 前面我们说过 RabbitMQ中的消息传递模型的核心思想是生产者永远不会将任何消息直接发送到队列中。所以我们每个队列都需要绑定在一个路由上。从生产者发送到该路由的消息只会被传递到与路由绑定的队列上。绑定时还需要指定一个routingKey，路由根据发布消息时传递过来的routintKey来匹配到相应队列并传送消息到该队列中。queue: 队列名exchange: 路由名routingKey: 队列与路由绑定的key 编写生产者123456789101112131415161718public class EmitLogTopic &#123; private final static String EXCHANGE_NAME=&quot;topic_logs&quot;; public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;localhost&quot;); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.TOPIC); String message =&quot;Hello World! This is error info!&quot;; /* 我们使用的是topic类型的路由，第二个参数为routingKey*/ channel.basicPublish(EXCHANGE_NAME,&quot;rabbit.log.error&quot;,null,message.getBytes()); channel.close(); connection.close(); &#125;&#125; 发送消息时我们指定的routingKey为 rabbit.log.error 因为是topic类型的路由，所以需要用点分隔的形式写routingKey ，如果是Direct类型则不需要。 编写消费者12345678910111213141516171819202122232425public class ReceivedLogsTopic &#123; private final static String EXCHANGE_NAME=&quot;topic_logs&quot;; public static void main(String[] args) throws IOException, TimeoutException &#123; ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;localhost&quot;); Connection connection = connectionFactory.newConnection(); Channel channel = connection.createChannel(); channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.TOPIC); String queuesName = channel.queueDeclare().getQueue(); /*将队列绑定路由并定义topic类型路由的匹配规则*/ channel.queueBind(queuesName,EXCHANGE_NAME,&quot;*.log.*&quot;); Consumer consumer = new DefaultConsumer(channel)&#123; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; String message = new String(body,&quot;UTF-8&quot;); System.out.println(&quot;[x] recv:&quot; + message); &#125; &#125;; channel.basicConsume(queuesName,true,consumer); &#125;&#125; 这里队列绑定topic 类型路由时指定的匹配规则为 .log.前面生产者发送时的routingKey 为 rabbit.log.error 所以这条消息会被路由监听到。 如果我们将绑定的匹配规则修改为 log.# 并重新启动一个消费者B，通过生产者再次发送消息，则消费者B不会监听到这条消息，因为routingKey 无法匹配上，路由不会把这条消息传递给消费者B *** 注意在测试时是需要先启动消费者，再启动生产者。因为如果没有消费者在线，消息会被rabbitMq丢弃处理 **","tags":["RabbitMQ"],"categories":["MQ"]},{"title":"RabbitMQ学习系列 二 “Hello World”","path":"/posts/aca91396/","content":"前面写了RabbitMQ的安装，这一篇记录一下 “Hello World” 的实现 。 [TOC] 介绍 使用最简单的方式发布一个消息并接收这里声明了一个hello队列，没有使用路由。 编写生产者1234567891011121314151617181920public class Send &#123; private final static String QUEUE_NAME=&quot;hello&quot;; public static void main(String[] args) throws IOException, TimeoutException &#123; /*创建连接工厂*/ ConnectionFactory connectionFactory = new ConnectionFactory(); connectionFactory.setHost(&quot;localhost&quot;); Connection connection = connectionFactory.newConnection(); /*创建频道*/ Channel channel = connection.createChannel(); /*声明一个队列*/ channel.queueDeclare(QUEUE_NAME,false,false,false,null); String message = &quot;HJW,Hello World!&quot;; /*发布消息*/ channel.basicPublish(&quot;&quot;,QUEUE_NAME,null,message.getBytes()); channel.close(); connection.close(); &#125;&#125; 编写消费者12345678910111213141516171819202122232425262728public class Recv &#123; private final static String QUEUE_NAME =&quot;hello&quot;; public static void main(String[] args) throws IOException, TimeoutException &#123; /*创建连接工厂*/ ConnectionFactory connectionFactory = new ConnectionFactory(); /*设置Rabbitmq主机地址，用户名密码与端口。用户名密码与端口可不设置可以使用默认*/ connectionFactory.setHost(&quot;localhost&quot;); connectionFactory.setUsername(&quot;guest&quot;); connectionFactory.setPassword(&quot;guest&quot;); connectionFactory.setPort(5672); Connection connection = connectionFactory.newConnection(); /*通过连接创建频道*/ Channel channel = connection.createChannel(); /*声明一个队列 */ channel.queueDeclare(QUEUE_NAME,false,false,false,null); /*创建一个消费者*/ Consumer consumer = new DefaultConsumer(channel)&#123; @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException &#123; String message = new String(body,&quot;UTF-8&quot;); System.out.println(&quot; Message is received: &quot; + message ); &#125; &#125;; /*订阅消息并消费*/ channel.basicConsume(QUEUE_NAME, true, consumer); &#125;&#125; 方法介绍1、队列声明 queueDeclare(String queue, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments) queue:队列名称durable:是否持久化exclusive:是否为排它队列。即只有自己可见，对首次连接可见，连接断开自动删除autoDelete:是否自动删除，在没有消费者的时候arguments:其它参数 2、发布消息 basicPublish(String exchange, String routingKey, BasicProperties props, byte[] body) exchange:路由名routingKey:发布到哪个队列props: 其它参数body: 消息体 3、订阅消息并消费 basicConsume(String queue, boolean autoAck, Consumer callback) queue:消息队列名autoAck: 是否自动确认。消息被消息后需要向rabbitmq返回一个确认，rabbitmq才会把消息删除，也可以手动确认callback: 消费者","tags":["RabbitMQ"],"categories":["MQ"]},{"title":"RabbitMQ学习系列 一 RabbitMQ 的安装","path":"/posts/a7407de/","content":"[TOC] 之前项目上使用到了ActiveMQ，所以学习了下ActiveMQ ，使用JMS结合ActiveMQ发送消息或主题，大致了解了它的使用。听说RabbitMQ 才是主流，打算学习RabbitMQ。 介绍 RabbitMQ是一个开源的AMQP实现，服务器端用Erlang语言编写，支持多种客户端，如：Python、Ruby、.NET、Java、JMS、C、PHP、ActionScript、XMPP、STOMP等，支持AJAX。用于在分布式系统中存储转发消息，在易用性、扩展性、高可用性等方面表现不俗。消息中间件主要用于组件之间的解耦。rabbitmq多应用于批量数据异步处理、并行任务串行化，高负载任务的负载均衡等 重量级，高并发，异步高可靠性场景。AMQP的主要特征是面向消息、队列、路由（包括点对点和发布&#x2F;订阅）、可靠性、安全。 下载RabitMQ是使用Erlang开发的，它的运行依赖Erlang。所以在使用前需要下载安装ErlangErlang 下载地址：Erlang官方下载RabbitMQ 下载地址：RabbitMQ 官方下载 安装Erlang 安装Window中 Erlang下载下来的安装程序是 otp_win64_20.3.exe ，直接双击安装即可。配置环境变量添加一个系统变量 ERLANG_HOME 并设置为 Erlang 的目录 ，比如我的安装目录为 ：D:\\Program Files\\erl9.3在 Path 系统变量中加上 %ERLANG_HOME%\\bin验证在cmd命令下输入:erl能够返回版本号则表示安装与配置环境变量成功 RabbitMQ 安装将下载下来的 rabbitmq-server-windows-3.7.4.zip 解压到指定的安装目录即可。配置环境变量添加 RABBITMQ_SERVER 并设置为RabbitMQ 解压到的目录，如我放置的目录为 ：D:\\rabbitmq_server-3.7.4在 Path 系统变量末尾添加 %RABBITMQ_SERVER%\\sbin 验证打开cmd窗口，输入: rabbitmq-service PS C:\\WINDOWS\\system32&gt; rabbitmq-service ********************* Service control usage ********************* rabbitmq-service help - Display this help rabbitmq-service install - Install the RabbitMQ service rabbitmq-service remove - Remove the RabbitMQ service The following actions can also be accomplished by using Windows Services Management Console (services.msc): rabbitmq-service start - Start the RabbitMQ service rabbitmq-service stop - Stop the RabbitMQ service rabbitmq-service disable - Disable the RabbitMQ service rabbitmq-service enable - Enable the RabbitMQ service 如有输出 以上 rabbitmq 命令的解释信息即表示安装成功。 安装服务可以把RabbitMQ服务器作为服务运行，打开一个cmd窗口(管理员)，输入命令： rabbitmq-service install PS C:\\WINDOWS\\system32&gt; rabbitmq-service install D:\\Program Files\\erl9.3\\erts-9.3\\bin\\erlsrv: Service RabbitMQ added to system. 运行命令成功后我们可以查看一下服务是否已添加成功 启动RabbitMQ在cmd 窗口中输入命令:rabbitmq-service start PS C:\\WINDOWS\\system32&gt; rabbitmq-service start RabbitMQ 服务正在启动 . RabbitMQ 服务已经启动成功。 安装web管理插件RabbitMQ 可以通用一个Web界面来进行管理。在cmd命令窗口中输入命令:rabbitmq-plugins enable rabbitmq_management PS C:\\WINDOWS\\system32&gt; rabbitmq-plugins enable rabbitmq_management Enabling plugins on node rabbit@hwacer-hp: rabbitmq_management The following plugins have been configured: rabbitmq_management rabbitmq_management_agent rabbitmq_web_dispatch Applying plugin configuration to rabbit@hwacer-hp... The following plugins have been enabled: rabbitmq_management rabbitmq_management_agent rabbitmq_web_dispatch set 3 plugins. Offline change; changes will take effect at broker restart. 安装好后需要重启RabbitMQ，使用 stop 停止 再使用start 启动即可。​\t​\tPS C:\\WINDOWS\\system32&gt; rabbitmq-service stop​\tRabbitMQ 服务正在停止………​\tRabbitMQ 服务已成功停止。​ PS C:\\WINDOWS\\system32&gt; rabbitmq-service start RabbitMQ 服务正在启动 . RabbitMQ 服务已经启动成功。 重启之后我们访问 http://localhost:15672/ 登陆RabbitMQ 的web管理后台。默认用户密码为 guest&#x2F;guest重启之后可能需要过一会访问才能打开 至此，RabbitMQ 的一系列安装准备工作已经完成了，接下来要学习如何通过编码发送消息。","tags":["RabbitMQ"],"categories":["MQ"]},{"title":"四 ActiveMQ消息持久化与配置","path":"/posts/7cb7ab7d/","content":"四 ActiveMQ消息持久化与配置 一、ActiveMQ 消息持久化1、新建数据库首先我们先新建一个mysql数据库，并把所有权限赋给新建用户，用户需要在建表的权限 123456789101112131415161718/** * 创建数据库 */CREATE DATABASE miscDEFAULT CHARSET=UTF8; /** * 创建用户和授权 */GRANT ALL PRIVILEGES ON misc.*TO &#x27;misc_root&#x27;@&#x27;%&#x27; IDENTIFIEDBY &#x27;misc_root_pwd&#x27;;GRANT ALL PRIVILEGES ON misc.*TO &#x27;misc_root&#x27;@&#x27;localhost&#x27; IDENTIFIEDBY &#x27;misc_root_pwd&#x27;; 2、配置数据源在ActiveMQ目录中找到conf&#x2F;activemq.xml 文件。ActiveMQ默认使用的是kahadb 我们在xml中修改成使用mysql在文件中找到： 123&lt;persistenceAdapter&gt; &lt;kahaDB directory=&quot;$&#123;activemq.data&#125;/kahadb&quot;/&gt; &lt;/persistenceAdapter&gt; 把它注释掉，添加mysql的配置 123&lt;persistenceAdapter&gt; &lt;jdbcPersistenceAdapter dataSource=&quot;#MySQL-DS&quot; /&gt; &lt;/persistenceAdapter&gt; 在&lt;/broker&gt;后面添加数据源的配置 12345678&lt;!-- MySQL DataSource --&gt; &lt;bean id=&quot;MySQL-DS&quot; class=&quot;org.apache.commons.dbcp.BasicDataSource&quot; destroy-method=&quot;close&quot;&gt; &lt;property name=&quot;driverClassName&quot; value=&quot;com.mysql.jdbc.Driver&quot; /&gt; &lt;property name=&quot;url&quot; value=&quot;jdbc:mysql://127.0.0.1:3306/misc?useUnicode=true&amp;amp;characterEncoding=UTF-8&quot; /&gt; &lt;property name=&quot;username&quot; value=&quot;misc_root&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;misc_root_pwd&quot; /&gt; &lt;property name=&quot;poolPreparedStatements&quot; value=&quot;true&quot; /&gt; &lt;/bean&gt; 这里的ID对应的就是我们上面配置的ID。最后，我们需要把mysql的jar包复制到ActiveMQ目录中的lib目录下，接着重新启动ActiveMQ，这时它会自动在数据库中生成三张表 我们可以用上一篇文章写的代码做测试，发布的消息会被存储在mysql数据库中，被消费后会自动删除。 二、ActiveMQ配置在第二篇文章中，我人提到ActiveMQ管理页面的登陆用户与密码默认是admin&#x2F;admin。这个默认的用户与密码在正式环境中肯定是需要修改的。 1、ActiveMQ管理页面登陆配置我们找到ActiveMQ目录中的 conf&#x2F;jetty-realm.properties可以看到配置了两个用户，admin与user。配置的格式为:​\t​\t用户名:密码,用户角色 这里我们修改一下admin用户的密码，重新启动ActiveMQ，打开管理员后台http://localhost:8161/admin弹出的登陆框中需要输入我们新设置的用户与密码才能登陆了。 2、ActiveMQ连接开启密码认证**第一步:**找到 ActiveMQ目录中的conf&#x2F;credentials.properties这里配置的是连接的用户与密码，我们可以把密码修改成123456。第二步：找到activemq&#x2F;conf&#x2F;activemq.xml在systemUsage 后面添加一个插件 1234567&lt;plugins&gt; &lt;simpleAuthenticationPlugin&gt; &lt;users&gt; &lt;authenticationUser username=&quot;$&#123;activemq.username&#125;&quot; password=&quot;$&#123;activemq.password&#125;&quot; groups=&quot;users,admins&quot;/&gt; &lt;/users&gt; &lt;/simpleAuthenticationPlugin&gt;&lt;/plugins&gt; 位置如图: 最后重启ActiveMQ ，如果我们还用前面的代码去连接ActiveMQ 会提示用户或密码无效。我们需要修改代码中的连接工厂内容。在实例化工厂时传入我们修改后的用户与密码 //实例化连接工厂 connectionFactory = new ActiveMQConnectionFactory(&quot;system&quot;,&quot;123456&quot;,JMSProducer.URL); 重新启动程序即可连接成功。这样我们就实现了密码认证连接ActiveMQ。","tags":["ActiveMQ"],"categories":["MQ"]},{"title":"三 JMS发布 订阅模型--ActiveMQ简单应用","path":"/posts/42a9c4a1/","content":"[TOC] 上一篇文章 《二、JMS 点对点模型 – ActiveMQ简单实现》 我们实现了JMS点对点模型的实例，本章对第二种 发布&#x2F;订阅 模型来做一个简单的实例。 其实发布&#x2F;订阅 模型与点对点模型的实现方式基本一致，因此这里就不写完整的过程了。 一、开发环境与上篇文章相同 二、java项目与上篇文章相同 三、具体实现1、编写发布者发布者的代码与上篇文章基本相同，不同的是 使用session 创建是的主题，而不是队列 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import org.apache.activemq.ActiveMQConnection;import org.apache.activemq.ActiveMQConnectionFactory;import javax.jms.*;/** * @author . * @version 1.0 * @name JMS生产者 * @description 消息的生产者类 * @date 2018/4/11 0011. */public class JMSProducer &#123; //默认连接用户名 private static final String USERNAME = ActiveMQConnection.DEFAULT_USER; //默认连接密码 private static final String PASSWORD = ActiveMQConnection.DEFAULT_PASSWORD; //默认连接地址 private static final String URL = ActiveMQConnection.DEFAULT_BROKER_URL; public static void main(String[] args)&#123; //连接工厂 ConnectionFactory connectionFactory; //连接 Connection connection = null; //会话 Session session; //目的地 Destination destination; //生产者 MessageProducer messageProducer; /** * 编写生产者的步骤 */ try &#123; //实例化连接工厂 connectionFactory = new ActiveMQConnectionFactory(JMSProducer.USERNAME,JMSProducer.PASSWORD,JMSProducer.URL); //使用连接工厂获取连接 connection = connectionFactory.createConnection(); //启动连接 connection.start(); //使用连接创建获取会话 session = connection.createSession(true,Session.AUTO_ACKNOWLEDGE); //与点对点唯一不同的地方 //使用会话连接一个主题作为目的地，如果这个主题不存在将会被创建 destination = session.createTopic(&quot;HelloWorld.Topic&quot;); //使用会话创建消息发布者 messageProducer = session.createProducer(destination); //发布主题 sendMessage(session,messageProducer); session.commit(); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125;finally &#123; if (connection!=null)&#123; try &#123; connection.close(); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /** * 发布者发布主题 * @param session 会话 * @param messageProducer 发布者 */ public static void sendMessage(Session session,MessageProducer messageProducer)&#123; try &#123; //使用会话创建一条文本消息,当然，消息的类型有很多，如文字，字节，对象等,可以通过session.create..方法来创建出来 TextMessage textMessage = session.createTextMessage(&quot;你好，世界! by Topic&quot;); //通过消息发布者发出主题 messageProducer.send(textMessage); System.out.println(&quot;已发送主题消息:&quot;+textMessage.getText()); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 2、编写订阅者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import org.apache.activemq.ActiveMQConnection;import org.apache.activemq.ActiveMQConnectionFactory;import javax.jms.*;/** * @author . * @version 1.0 * @name * @description * @date 2018/4/11 0011. */public class JMSConsumer &#123; //默认连接用户名 private static final String USERNAME = ActiveMQConnection.DEFAULT_USER; //默认连接密码 private static final String PASSWORD = ActiveMQConnection.DEFAULT_PASSWORD; //默认连接地址 private static final String URL = ActiveMQConnection.DEFAULT_BROKER_URL; public static void main(String[] args)&#123; //连接工厂 ConnectionFactory connectionFactory; //连接 Connection connection = null; //会话 Session session; //目的地 Destination destination; //消息的消费者 MessageConsumer messageConsumer; /** * 消息的消费者编写 */ try &#123; //实例化工厂 connectionFactory = new ActiveMQConnectionFactory(JMSConsumer.USERNAME,JMSConsumer.PASSWORD,JMSConsumer.URL); //使用实例工厂获取连接 connection = connectionFactory.createConnection(); //启动连接 connection.start(); //使用连接获取会话 session = connection.createSession(false,Session.AUTO_ACKNOWLEDGE); //使用会话连接一个队列作为目的地，如果这个队列不存在将会被创建// destination = session.createQueue(&quot;HelloWorld&quot;); //使用会话创建一个主题，如果这个主题不存在将会被创建 Topic topic = session.createTopic(&quot;HelloWorld.Topic&quot;); //使用会话创建一个订阅者 messageConsumer = session.createConsumer(topic); /** *获取消息 */ /*同步实现*/ //设置接收者接收消息的时间,为了便于测试,这里定为50s,接收到消息之前（或超时之前）将一直阻塞 /*TextMessage textMessage = (TextMessage) messageConsumer.receive(50000); if (textMessage!=null)&#123; System.out.println(&quot;收到的消息是:&quot; + textMessage.getText()); &#125;else &#123; System.out.println(&quot;没有收到消息&quot;); &#125;*/ /*异步实现*/ messageConsumer.setMessageListener(new MessageListener() &#123; @Override public void onMessage(Message message) &#123; try &#123; String text = ((TextMessage)message).getText(); System.out.println(&quot;收到的消息 :&quot; +text ); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; /*需要异步，则不关闭连接*/ &#125;&#125; 3、关于持久订阅模式在第一篇文章 《一、JMS概述》中我们提到: 发布&#x2F;订阅模型还支持持久订阅的概念，在消息发布时，注册了主题的消费者不需要处于活动状态; 当消费者随后变得活跃时，它将收到消息。如果没有活动使用者注册主题，则该主题不会持有它收到的消息，除非它具有持久订阅的不活动消费者。 主要是业务场景如下: A系统通过MQ推送数据到B系统。通过发布订阅的消息传送模型。由于涉及到的数据比较重要：比如是关于资金、交易、股票价格的信息。要保证B系统一定收到A系统发送的消息，考虑B系统会断电重启之类异常，故设置持久订阅模式。可以保证在B订阅A主题后，因为断电，订阅者状态变为不活动的。在B系统重启后，依然可以收到消息。 实现持久订阅模式与普通的发布订阅模式一样，主要的不同是必须设置唯一的客户端ID和订阅者ID。 1、在连接启动前设置设置客户端ID 2、使用createDurableSubscriber 创建订阅者并指定订阅者ID 1234//设置客户端IDconnection.setClientID(&quot;client1&quot;);//使用会话创建一个订阅者，并指定订阅者ID为 sub1TopicSubscriber topicSubscriber = session.createDurableSubscriber(topic,&quot;sub1&quot;); 四、运行1、启动ActiveMQ与上一篇文章相同 2、运行程序首先我们运行一下发布者。发布成功后我们可以在ActiveMQ 的主题中看到我们创建出来的主题消息 我们再运行一下订阅者，会发现订阅者一直在待接收消息，并没有输出我们刚刚发布的主题消息。这是因为 发布&#x2F;订阅 模型 的特点:发布端在发布消息时，如果没有订阅端在线，则不会保留消息，将会认为消息已经发送。 因此我们可以先运行订阅者的代码，启动一个订阅者。为了体现发布&#x2F;订阅模式一对多的特点，我们再启动第二个订阅者。可以在ActiveMQ中看到在两个订阅者在线了。 我们再启动发布者，发布一个消息。在线的两个订阅者就可以接收到我们刚刚发布的消息了。我们再设想一下，其中一个订阅者断电下线了，如果再有消息发布，则待它再次上线时已经接收不到第二次发布的消息了。为了解决一个问题，我们可以使用持久订阅模式。按照上面 持久订阅修改代码后重新启动两个订阅者，注意的时这两个订阅者的客户端ID与订阅者ID都必须唯一。 启动发布者发布一个消息 ，可以看到两个订阅者分别都接收到了发布的消息。这时我们关闭 订阅者 Client1 ，再发布一个消息。这时Client2接收到了。当我们启动订阅者 Client1后，它也能够收到第二次发布的消息 至此，我们实现了JMS发布&#x2F;订阅模型，并使用了持久订阅模式关于持久订阅我们需要注意的是: 很多情况下，持久化订阅非常有用，但有的时候并非如此。虽然使用持久还是非持久通常由业务决定。但是，我们还必须考虑消息消耗的存储容量。比如有一个持久订阅者长期处于不活动的状态，那么jms服务器就必须为这个订阅者存储数以千计、万计的无用信息，浪费JMS数据仓库的宝贵空间。因为，我们必须得考虑这个问题。","tags":["ActiveMQ","JMS"],"categories":["MQ"]},{"title":"二 JMS 点对点模型 -- ActiveMQ简单实现","path":"/posts/762f8ad8/","content":"[TOC] 本文我们使用ActiveMQ实现简单的点对点的消息模型。 一、开发环境这里我使用的是 apache-activemq-5.11.1 可以去官网下载 jdk1.8 idea 二、新建java项目 其中activemq-all-5.11.1.jar在下载下来的 apache-activemq-5.11.1 中就有，直接复制过来导入项目即可。 三、具体实现JMSProducer：消息的生产者JMSConsumer：消息的消费者 大致步骤：（1）创建连接工厂（2）使用连接工厂创建一个连接（3）启动连接（4）使用连接创建一个会话（5）使用会话创建一个队列&#x2F;主题（6）使用会话创建一个生产者&#x2F;消费者（7）使用会话创建一个消息&#x2F;对象&#x2F;集合&#x2F;文件&#x2F;字节（8）使用生产者&#x2F;消费者 发送&#x2F;获取 消息 1、编写生产者123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import org.apache.activemq.ActiveMQConnection;import org.apache.activemq.ActiveMQConnectionFactory;import javax.jms.*;/** * @author * @version 1.0 * @name JMS生产者 * @description 消息的生产者类 * @date 2018/4/11 0011. */public class JMSProducer &#123; //默认连接用户名 private static final String USERNAME = ActiveMQConnection.DEFAULT_USER; //默认连接密码 private static final String PASSWORD = ActiveMQConnection.DEFAULT_PASSWORD; //默认连接地址 private static final String URL = ActiveMQConnection.DEFAULT_BROKER_URL; public static void main(String[] args)&#123; //连接工厂 ConnectionFactory connectionFactory; //连接 Connection connection = null; //会话 Session session; //目的地 Destination destination; //生产者 MessageProducer messageProducer; /** * 编写生产者的步骤 */ try &#123; //实例化连接工厂 connectionFactory = new ActiveMQConnectionFactory(JMSConsumer.USERNAME,JMSConsumer.PASSWORD,JMSConsumer.URL); //使用连接工厂获取连接 connection = connectionFactory.createConnection(); //启动连接 connection.start(); //使用连接创建获取会话 session = connection.createSession(true,Session.AUTO_ACKNOWLEDGE); //使用会话连接一个队列作为目的地，如果这个队列不存在将会被创建 destination = session.createQueue(&quot;HelloWorld&quot;); //使用会话创建消息生产者 messageProducer = session.createProducer(destination); //发送消息 sendMessage(session,messageProducer); //支持事务则必须提交 session.commit(); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125;finally &#123; if (connection!=null)&#123; try &#123; //关闭连接 connection.close(); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /** * 生产者发送消息 * @param session 会话 * @param messageProducer 生产者 */ public static void sendMessage(Session session,MessageProducer messageProducer)&#123; try &#123; //使用会话创建一条文本消息,当然，消息的类型有很多，如文字，字节，对象等,可以通过session.create..方法来创建出来 TextMessage textMessage = session.createTextMessage(&quot;你好，世界!&quot;); //通过消息生产者发出消息 messageProducer.send(textMessage); System.out.println(&quot;已发送消息:&quot;+textMessage.getText()); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 2、编写消费者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import org.apache.activemq.ActiveMQConnection;import org.apache.activemq.ActiveMQConnectionFactory;import javax.jms.*;/** * @author . * @version 1.0 * @name * @description * @date 2018/4/11 0011. */public class JMSConsumer &#123; //默认连接用户名 private static final String USERNAME = ActiveMQConnection.DEFAULT_USER; //默认连接密码 private static final String PASSWORD = ActiveMQConnection.DEFAULT_PASSWORD; //默认连接地址 private static final String URL = ActiveMQConnection.DEFAULT_BROKER_URL; public static void main(String[] args)&#123; //连接工厂 ConnectionFactory connectionFactory; //连接 Connection connection = null; //会话 Session session; //目的地 Destination destination; //消息的消费者 MessageConsumer messageConsumer; /** * 消息的消费者编写 */ try &#123; //实例化工厂 connectionFactory = new ActiveMQConnectionFactory(JMSConsumer.USERNAME,JMSConsumer.PASSWORD,JMSConsumer.URL); //使用实例工厂获取连接 connection = connectionFactory.createConnection(); //启动连接 connection.start(); //使用连接获取会话 session = connection.createSession(false,Session.AUTO_ACKNOWLEDGE); //使用会话连接一个队列作为目的地，如果这个队列不存在将会被创建 destination = session.createQueue(&quot;HelloWorld&quot;); //使用会话获取消费者 messageConsumer = session.createConsumer(destination); /** *获取消息 */ /*同步实现*/ //设置接收者接收消息的时间,为了便于测试,这里定为50s,接收到消息之前（或超时之前）将一直阻塞 /*TextMessage textMessage = (TextMessage) messageConsumer.receive(50000); if (textMessage!=null)&#123; System.out.println(&quot;收到的消息是:&quot; + textMessage.getText()); &#125;else &#123; System.out.println(&quot;没有收到消息&quot;); &#125;*/ /*异步实现*/ messageConsumer.setMessageListener(new MessageListener() &#123; @Override public void onMessage(Message message) &#123; try &#123; String text = ((TextMessage)message).getText(); System.out.println(&quot;收到的消息是:&quot; +text ); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125; &#125;); &#125; catch (JMSException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 3、session与事务处理session = connection.createSession(true,Session.AUTO_ACKNOWLEDGE); 第一个参数 要使用事务处理，必须通过作为第一个参数设置为true来创建一个事务处理会话 事务处理允许您将整个系列的传入和传出消息分组在一起，并将它们视为原子单元。消息代理跟踪事务的各个消息的状态，但在您提交事务之前不会完成它们的传送。在发生故障时，您可以回滚事务，取消其所有消息并从头开始重新启动整个系列。事务处理会话总是只有一个打开的事务，包含自会话创建或前一个事务完成以来发送或接收的所有消息。提交或回滚事务会结束该事务并自动开始另一个事务。 当交易中的所有消息都已成功交付时，您可以调用会话的commit方法来提交交易 session.commit(); 所有会话的传入消息都会被确认，并且所有传出的消息都将被发送。交易被视为完成，并开始新的交易。 发送或接收操作失败时，会引发异常。虽然可以通过忽略或重试操作来处理异常，但建议您使用会话的rollback方法回退事务： session.rollback（）; 第二个参数： 值可为Session.AUTO_ACKNOWLEDGE，Session.CLIENT_ACKNOWLEDGE，DUPS_OK_ACKNOWLEDGE其中一个。Session.AUTO_ACKNOWLEDGE为自动确认，客户端发送和接收消息不需要做额外的工作。哪怕是接收端发生异常，也会被当作正常发送成功。Session.CLIENT_ACKNOWLEDGE为客户端确认。客户端接收到消息后，必须调用javax.jms.Message的acknowledge方法。jms服务器才会当作发送成功，并删除消息。DUPS_OK_ACKNOWLEDGE允许副本的确认模式。一旦接收方应用程序的方法调用从处理消息处返回，会话对象就会确认消息的接收；而且允许重复确认。 4、连接工厂 ConnectionFactoryconnectionFactory = new ActiveMQConnectionFactory(JMSConsumer.USERNAME,JMSConsumer.PASSWORD,JMSConsumer.URL); activemq默认是不需要密码，生产消费者就可以连接的实例化连接工厂时我们可以去掉用户名与密码，同样可以连接到ActiveMQ如果ActiveMQ是部署在你本地的，则默认的用户名为admin,默认密码也为admin,地址为:tcp:&#x2F;&#x2F;localhost:61616 我们可以直接从ActiveMQConnection 中 取得默认的用户名，密码与地址 5、关于同步与异步 同步 订阅者或接收者调用receive方法来接收消息，receive方法在能够接收到消息之前（或超时之前）将一直阻塞 异步 订阅者或接收者可以注册为一个消息监听器。当消息到达之后，系统自动调用监听器的onMessage方法。上面消费者的代码中使用的是这种方法 四、运行1、启动ActiveMQ 如果你的64bit机器，则在这个目录下打开Activemq.bat 32bit机器可以执行 bin\\win32\\activemq.bat 打开后出现以下窗口后，表示启动成功，这个窗口不能关闭 打开ActiveMQ的管理界面：http://127.0.0.1:8161/admin/ 分别点击队列与主题可以查看发送到ActiveMQ的队列消息或主题 2、运行生产者ActiveMQ 启动好后，我们就可以运行生产者向ActiveMQ发送消息了 运行结束后，我们查看一下ActiveMQ的队列 表示ActiveMQ自动创建一个名为”HelloWorld“的队列，队列已经收到了一条消息，暂时没有消费者，我们也可以在ActiveMQ中对消息进行浏览删除等操作 3、运行消费者这里我们使用异步接收，生产者再次发送消息时同样可以接收到。如果使用同步接收，在规定的时间超时后会程序停止，关闭连接。 看看ActiveMQ中的队列发生的变化： 可以看到待处理消息已为0，出队消息加1，有一个消费者在线。 官方文档参考: https://docs.oracle.com/cd/E26576_01/doc.312/e24945/toc.htm","tags":["ActiveMQ"],"categories":["MQ"]},{"title":"一 JMS概述","path":"/posts/b7f40b7d/","content":"[TOC] 一、JMS概念 摘要：The Java Message Service (JMS) API is a messaging standard that allows application components based on the Java Platform Enterprise Edition (Java EE) to create, send, receive, and read messages. It enables distributed communication that is loosely coupled, reliable, and asynchronous. JMS（JAVA Message Service,java消息服务）API是一个消息服务的标准或者说是规范，允许应用程序组件基于JavaEE平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。jms是java的消息服务，JMS客户端之间可以通过JMS服务进行异步的消息传输 消息包括：消息头，消息扩展属性和消息休，其结构看起来与SOAP非常相似，但一般情况下，SOAP主要关注远程服务调用，而消息则专注于信息的交换。 消息分为：消息生产者，消息服务器和消息消费者。生产者与消费者之间是透明的，生产者在产生消息后，把消息发送到消息服务器，再由消息服务器发给消费者，因此它们构成了JMS的3点结构;消息服务器再给消费者时，有2种模式:点到点(point to point )模式和发布&#x2F;订阅(pbulish&#x2F;subscribe) 模式,两种模式下面会详细介绍; 二、消息服务器 消息服务器有很多:ActiveMQ,Jboss MQ,Open MQ,RabbitMQ ,ZeroMQ等等。本文介绍的是开源的java实现的Apache ActiveMQ,后面我们会写到如何应用它与jms结合。 看到一个对消息服务器作用的解释： 消息队列的主要作用不是通讯，主要是用于解除子系统间的耦合，所以异构系统间的通讯实际并不是ActiveMQ发挥作用的场景，那反而是RPC发挥作用的时候。消息队列更适合于需要更大流量和并发的大型系统场景，可以将消息队列视为一个可靠的通道，主交易过程在处理时，遇到需时较多同时又已经确定了条件的处理就丢到消息队列里进行后续处理，这样可以将主交易过程划分为一个一个可以异步处理的更小的处理过程，减少了主交易流程的处理时间，可以提供更快的响应速度和并发速度。例如，象淘宝这样的处理逻辑非常多的系统，在处理付款时，就可以将通知买家和卖家、记日志甚至记帐流程都放到消息队列里处理，整个主流程能够快速处理完成，继续处理下一个买家的请求。 应用场景: 三、消息模型1、Point-to-Point(P2P) – 点对点模式 每条消息都从消息生产者传递到单个消息使用者。生产者将消息传递给队列，稍后将其传递给为队列注册的其中一个消费者。任何数量的生产者和消费者都可以与同一个队列进行交互，但是每个消息都被保证传递给（并且被成功消费）一个消费者，而不会再消费。如果没有消费者注册队列，它将保存它收到的消息，并最终在消费者注册时传递消息。 2、Publish&#x2F;Subscribe(Pub&#x2F;Sub) – 发布订阅模型 单个消息可以从生产者被传递到任何数量的消费者。生产者将消息发布到一个主题，然后将其发送给订阅了该主题的所有活动消费者。任何数量的生产者都可以将消息发布到给定主题，并且每条消息都可以传递给任意数量的订阅消费者。该模型还支持持久订阅的概念，在消息发布时，注册了主题的消费者不需要处于活动状态; 当消费者随后变得活跃时，它将收到消息。如果没有活动使用者注册主题，则该主题不会持有它收到的消息，除非它具有持久订阅的不活动消费者。 三、消息的消费在JMS中，消息的产生和消息是异步的。对于消费来说，JMS的消息者可以通过两种方式来消费消息。○ 同步 订阅者或接收者调用receive方法来接收消息，receive方法在能够接收到消息之前（或超时之前）将一直阻塞 ○ 异步 订阅者或接收者可以注册为一个消息监听器。当消息到达之后，系统自动调用监听器的onMessage方法。 四、JMS编程模型1、 ConnectionFactory 创建Connection对象的工厂，针对两种不同的jms消息模型，分别有QueueConnectionFactory和TopicConnectionFactory两种。可以通过JNDI来查找ConnectionFactory对象。 2、Destination Destination的意思是消息生产者的消息发送目标或者说消息消费者的消息来源。对于消息生产者来说，它的Destination是某个队列（Queue）或某个主题（Topic）;对于消息消费者来说，它的Destination也是某个队列或主题（即消息来源）。 所以，Destination实际上就是两种类型的对象：Queue、Topic可以通过JNDI来查找Destination。 3、Connection Connection表示在客户端和JMS系统之间建立的链接（对TCP&#x2F;IP socket的包装）。Connection可以产生一个或多个Session。跟ConnectionFactory一样，Connection也有两种类型：QueueConnection和TopicConnection。 4、Session Session是我们操作消息的接口。可以通过session创建生产者、消费者、消息等。Session提供了事务的功能。当我们需要使用session发送&#x2F;接收多个消息时，可以将这些发送&#x2F;接收动作放到一个事务中。同样，也分QueueSession和TopicSession。 5、消息的生产者 消息生产者由Session创建，并用于将消息发送到Destination。同样，消息生产者分两种类型：QueueSender和TopicPublisher。可以调用消息生产者的方法（send或publish方法）发送消息。 6、消息消费者 消息消费者由Session创建，用于接收被发送到Destination的消息。两种类型：QueueReceiver和TopicSubscriber。可分别通过session的createReceiver(Queue)或createSubscriber(Topic)来创建。当然，也可以session的creatDurableSubscriber方法来创建持久化的订阅者。 7、MessageListener 消息监听器。如果注册了消息监听器，一旦消息到达，将自动调用监听器的onMessage方法。EJB中的MDB（Message-Driven Bean）就是一种MessageListener。","tags":["ActiveMQ"],"categories":["MQ"]},{"title":"Git常见问题记录","path":"/posts/7fc60b16/","content":"git pull遇到的问题 early EOF index-pack failed 1234567$ git clone https://github.com/boostorg/boost.gitCloning into &#x27;boost&#x27;...remote: Counting objects: 183543, done.remote: Compressing objects: 100% (69361/69361), done.fatal: The remote end hung up unexpectedlyfatal: early EOFfatal: index-pack failed 我遇到这个应该是项目中有一些大文件，所以无法传输 解决方法在项目目录下，先使用 git config -l 查看一下git的配置，添加以下配置 1git config --add core.compression -1 compression 是压缩的意思，从 clone 的终端输出就知道，服务器会压缩目标文件，然后传输到客户端，客户端再解压。取值为 [-1, 9]，-1 以 zlib 为默认压缩库，0 表示不进行压缩，1..9 是压缩速度与最终获得文件大小的不同程度的权衡，数字越大，压缩越慢，当然得到的文件会越小。 git clone 遇到的问题同样是项目中文件过大，无法clone下来 解决方法1git config --global http.postBuffer 524288000 把git的postBuffer设置大一些就可以了","tags":["TortoiseGit"],"categories":["TortoiseGit"]},{"title":"WebService的简单实现","path":"/posts/9c73294c/","content":"WebService的简单实现 服务端发布一个WebService，只需要几个注解： @WebService 注解在一个类上，声明一个WebServier 对外发布，public 方法都会默认发布 @WebMethod 在方法上使用，可以对设置排除某个方法发布 @WebResult 定义方法返回参数说明 @WebParam 定义方法接收参数说明 简单WebService实例代码如下： 1234567891011121314151617181920212223242526272829303132333435/** * @WebService 注解声明一个WebServier对外发布，public 方法都会默认发布 * 被声明的类中必须有一个公开的方法，否则发布失败 */@WebService( serviceName = &quot;HelloWorl2&quot;, // 服务名 targetNamespace = &quot;http://two.hjwjw.github.io&quot; //命名空间,默认包名取反)public class HelloWorldTwo &#123; @WebMethod(operationName = &quot;sayHiWithYou&quot;) //对外发布的方法名 public @WebResult(name = &quot;toYou&quot;) String sayHi(@WebParam(name = &quot;youName&quot;) String name) &#123; return &quot;Hi,&quot;+name; &#125; @WebMethod(exclude = true) //表示排除这个方法 public String sayHello(String name) &#123; return &quot;Hello,&quot;+name; &#125; /** * protected、private、final、static方法不能对外公开 */ private String sayLove(String name)&#123; return &quot;love you,&quot;+ name; &#125; protected String sayNo(String name)&#123; return &quot;No,&quot;+ name; &#125; public static String sayStatic(String name)&#123; return &quot;Static,&quot;+name; &#125; public final String sayFinal(String name)&#123; return &quot;final,&quot;+name; &#125;&#125; 下面定义一个面向接口的WebServic。 服务接口 IHelloWorldThree.java 12345678910111213141516171819202122232425package io.github.hjwjw.three;import javax.jws.WebMethod;import javax.jws.WebParam;import javax.jws.WebResult;import javax.jws.WebService;/** * The IHelloWorldThree class. * * @author hjwjw * @date */@WebService( serviceName = &quot;HelloWorldThree&quot;, //服务名 targetNamespace = &quot;http://three.hjwjw.github.io&quot; //命名空间,默认服务接口包名取反 )public interface IHelloWorldThree &#123; @WebMethod(exclude = true) String sayHi(String name) ; @WebResult(name = &quot;toYou&quot;) String sayHello(@WebParam(name = &quot;youName&quot;) String name);&#125; 服务实现类HelloWorldThree.java 123456789101112131415161718/** * The HelloWorldThree class. * 面向接口的简单WebService发布 * @author * @date */@WebService(endpointInterface = &quot;io.github.hjwjw.three.IHelloWorldThree&quot;) //设置服务端点接口，指定提供服务的接口public class HelloWorldThree implements IHelloWorldThree &#123; @Override public String sayHi(String name) &#123; return &quot;Hi,&quot;+name; &#125; @Override public String sayHello(String name) &#123; return &quot;Hello,&quot;+name; &#125;&#125; 发布WebServiceapp.java 1234567891011public class App &#123; public static void main(String[] args) &#123; IHelloWorldThree helloWorldThree = new HelloWorldThree(); //自定义一个服务地址 String address = &quot;http://127.0.0.1:7856/ws/three/hello&quot;; //使用端点服务，将对象绑定到一个地址和端口，同时必须在端口后给服务取一个名称 Endpoint.publish(address,helloWorldThree); //WebService服务说明文档wsdl地址 System.out.println(&quot;Wsdl地址：&quot; + address + &quot;?wsdl&quot;); &#125;&#125; ​ 在 Spring Boot 中发布 WebService 服务WSDLWSDL是WebService的说明文档，可以在文档中知道服务提供了哪些方法，哪些参数，参数是什么类型等信息。打开上面发布的wsdl地址如下： 分解 一个WSDL文档由四部分组成： 1、types 指定了WebService用到的所有数据类型2、message 指明一个操作所用到的数据类型。 sayHi是指sayHi方法的输入操作用到的数据类型，sayHiResponse是指sayHi的输出操作用到的数据类型。二者的element元素指出了与types中对应到的具体类型。 3、portType 指出了这个WebService所有支持的操作，就是说有哪些方法可供调用。 这里支持一个sayHi调用，它的输入和输出对应到sayHi和sayHiResponse这个两个数据类型。 4、binding soap12:binding元素的transport指明传输协议，这里是http协议。 operation 指明要暴露给外界调用的操作。 use属性指定输入输出的编码方式，这里没有指定编码。 5、services 指定服务的一些信息，主要是指定服务的访问路径。 与下方的客户端调用联系,可以看出我们在使用客户端调用时应按如下步骤： 创建服务实例 使用服务实例调用port 再调用到具体方法 客户端使用Java自带的命令生成客户端代码。 wsimportcmd 命令 1wsimport -s . http://127.0.0.1:7856/ws/three/hello?wsdl wsimport命令用法wsimport [options] WSDL_URI比较常用的[options]有： -d 在指定的目录生成class文件 -clientjar 在当前目录生成jar文件，结合-d 可以在指定的目录生成jar文件 -s 在指定的目录生成java源文件 , 使用一个 . 表示当前目录 -p 指定生成文件的包结构 -keep在生成class文件，或者jar包时，同时保留java源文件 生成后如下图: 我们可以直接使用class文件，也可以把java文件放入项目里。 调用123456789101112/** * 通过wsimport 解析WSDL生成客户端代码调用WebService服务 * @author * */public class App &#123; public static void main(String[] args) &#123; HelloWorldThreeService helloWorldThreeService = new HelloWorldThreeService(); String toyou = helloWorldThreeService.getHelloWorldThreePort().sayHello(&quot;HJW&quot;); System.out.println(toyou); &#125;&#125; 返回如下:","tags":["Webservice"],"categories":["Webservice"]},{"title":"WebService原理","path":"/posts/9fd8791d/","content":"WebService原理介绍 解释WebService就是应用程序之间的远程调用 调用是跨语言的调用 语言XML扩展性标记语言。用于传输格式化的数据，是WEB服务的基础 WSDLWEB服务描述语言。（WebService的使用说明书） 通过XML的形式说明服务在什么地方--地址 通过XML形式说明服务提供什么样的方法 -- 如何调用 SOAPSOAP作为一个基于XML语言的协议用于网上传输数据 SOAP = 在HTTP的基础上 + XML数据 SOAP 是基于HTTP的。 SOAP的组成 ：Envelope 必须的部分 。以XML的根元素出现;Headers -- 可选的;Body -- 必须的。在body部分包含要执行服务器的方法。和发送到服务器的数据。 目前WebService的协议主要有SOAP1.1和1.2。 两者的命名空间不同。 Soap1.1的命名空间： xmlns:soap&#x3D;“http://schemas.xmlsoap.org/so... “ Soap1.2 命名空间： xmlns:soap&#x3D;”http://www.w3.org/2003/05/soap-envelope“ SOAP1.1版本与SOAP1.2版本在头信息上存在差异。 SOAP1.1存在SOAPAction的请求头。 SOAP1.2没有SOAPAction的请求头。 基于SOAP1.1生成的WSDL和基于SOAP1.2生成的WSDL也不一样。 主要看命名空间。 在CXF中两种协议请求的方式也不一样。 1.1为content-Type:text&#x2F;xm;charset&#x3D;UTF-8 1.2为content-Type:application&#x2F;soap+xml;charset&#x3D;UTF-8","tags":["WebService"],"categories":["Webservice"]},{"title":"Spring-loaded实现热部署-开发环境","path":"/posts/66217f64/","content":"Oracle提供的JDK其实已经自带一定程度的热加载功能，但是如果你修改了类名，方法名，或者添加了新类，新方法的话。Tomcat都需要重新启动来使得刚才的更改生效。而JRebel和spring-loaded都能有效地解决这个问题。其中springloaded是开源软件，可以免费使用。其主页：https://github.com/spring-projects/spring-loaded 获取jar包 首先我们需要得到spring-loaded的jar包，上面的github链接中可以下载到。这里我用的是最新的springloaded-1.2.7.RELEASE.jar 存放位置：D:\\springloaded-1.2.7.RELEASE.jar IDE中部署 打开项目，在启动之前按以下进行配置 idea中在启动Tomcat之前配置VM option。填写以下参数：​\t​\t-javaagent:D:&#x2F;springloaded-1.2.7.RELEASE.jar -noverify 其中参数中Springloaded的路径按实际填写 配置完成后可以启动项目了 eclipse中右击项目-&gt;Run as-&gt;Run configurations… 在tomcat启动项添加VM参数​\t​\t-javaagent:D:&#x2F;springloaded-1.2.7.RELEASE.jar -noverify 其中参数中Springloaded的路径按实际填写 配置完成后可以启动项目了 测试 为了解Springloaded 适用于哪些更改，下面来作几个测试 我在上述启动的SSM项目中，在一个控制器里添加了以下方法 12345678/** * 测试SpringLoaded */@RequestMapping(&quot;/hello&quot;)@ResponseBody public String test()&#123; return &quot;Hello Spring Loaded!&quot;;&#125; 保存后通过浏览器访问失败，找不到&#x2F;hello 这个路径。通过重启Tomcat后可以正常访问test方法 继续添加test2()方法，不使用注解，为了在浏览器中方便测试，通过test()方法来访问test2()方法。（经过上面重启Tomcat后test方法可以访问） 123456789101112/** * 测试SpringLoaded */@RequestMapping(&quot;/hello&quot;)@ResponseBodypublic String test()&#123; return this.test2();&#125;public String test2()&#123; return &quot;Spring Loaded By Test2&quot;;&#125; 浏览器输出 ​\t“Spring Loaded By Test2” 说明我们添加的第二个方法test2()没有经过重启服务器就可以访问了，热部署生效 我们继续 新建一个类，并在test()方法中去调用 1234567891011/**新建类 * Created by JiangWei.Huang * 2017/8/22 0022. */@RestControllerpublic class TestCtrl &#123; @GetMapping(&quot;/hello3&quot;) public String test3()&#123; return &quot;TestCtrl-test3&quot;; &#125;&#125; 123456789/**修改test方法调用新建的类TestCtr中的test3方法 * 测试SpringLoaded */@RequestMapping(&quot;/hello&quot;)@ResponseBodypublic String test()&#123; TestCtrl testCtrl = new TestCtrl(); return testCtrl.test3();&#125; 浏览器输出 ​\t“TestCtrl-test3” 说明我们新建的类，在不用重启的情况下也能够被调用到了，热部署生效 但值得注意的是，我们新建的类中，在类上与方法上都写了Spring注解，但这里是也不生效的。@RestController这个注解没有生效，&#x2F;hello3这个路径也是访问不了的。需要重启服务器才生效 另外在Idea中修改后自动保存但不会自动重新编译，如果在Idea中修改后热部署没有生效，按ctrl+shift+f9重新编译。也可以设置Idea自动编译，设置如下图。 总结经过上面的测试我们可以得出一些结论。像官方所说，可以实现以下的热更新 Spring Loaded allows you to add&#x2F;modify&#x2F;delete methods&#x2F;fields&#x2F;constructors. The annotations on types&#x2F;methods&#x2F;fields&#x2F;constructors can also be modified and it is possible to add&#x2F;remove&#x2F;change values in enum types. Spring加载允许您添加&#x2F;修改&#x2F;删除&#x2F;字段&#x2F;方法构造函数。注释类型&#x2F;方法&#x2F;字段&#x2F;构造函数，并且还可以在枚举类型中添加&#x2F;删除&#x2F;更改值。 但是对于第三方像是Spring注解这些的修改，spring-loaded就无能为力了，必须求助于更加强大的，收费的JRebel了","categories":["Spring"]},{"title":"Groovy 入门学习小结","path":"/posts/250a2363/","content":"Groovy 是 JVM 的一个替代语言，替代 是指可以用 Groovy 在 Java 平台上进行 Java 编程，使用方式基本与使用 Java 代码的方式相同。在编写新应用程序时，Groovy 代码能够与 Java 代码很好地结合，也能用于扩展现有代码。目前的 Groovy 版本是 1.5.4，在 Java 1.4 和 Java 5 平台上都能使用，也能在 Java 6 上使用。Groovy 的一个好处是，它的语法与 Java 语言的语法很相似。可以将它想像成 Java 语言的一种更加简单、表达能力更强的变体。 认识 Groovy准备环境在Eclipse中安装Groovy 的插件。 安装的Url 为：http://dist.codehaus.org/groovy/distributions/update/ Hello World 在学习之前做一个简单的Hello World 在Eclipse 中新建项目。 new &gt; other &gt;Groovy &gt; Groovy Project ​\tEclipse 自动帮我们把Groovy需要的jar包添加上了。 在src 上建一个类 New &gt; other &gt; Groovy &gt; Groovy Class ​\t勾选添加 main 方法 在main 中打印hello world, 类如下： 12345678package groovyTestclass HelloWorld &#123;\tstatic main(args) &#123; System.out.print(&quot;Hello World&quot;);\t&#125;&#125; 运行 右击选择 run as &gt; Groovy Script 可以看到控制台打印了Hello World Groovy与Java 前面说到Groovy与java 很相似，有多相似可以从Hello World中看出，语法没有一点差别。但是Groovy也有它自己的特点。它可以基本兼容java的语法，也有自己的一套语法，写起来会比Java更加简洁方便。可以将上面的Hello World改成如下： 123456package groovyTestclass HelloWorld &#123;\tstatic main(args) &#123; println (&quot;Hello World&quot;);\t&#125;&#125; 输出语句变得更简洁了，运行起来与之前结果一样。甚至还能更简洁一点： println (&quot;Hello World&quot;) 只需要一句就可以实现一个Hello World 总结： Groovy 属于脚本语言。脚本语言的一个特点就是能够在运行时进行解释。 并且它支持松散的 Java 语法, 允许省略分号和修改符除非另行指定，Groovy 的所有内容都为 public。 Groovy 允许定义简单脚本，同时无需定义正规的 class 对象。 Groovy 语法还允许省略变量类型。 并且Groovy也可以在JVM中直接运行，是Java 强有力的补充。 语法简介定义方式变量Groovy 中定义变量不需要类型，Groovy 会根据对象的值来判断它的类型，这点与JavaScript 有点类似。也可以统一用 def 来代替类型。 例：def value = “Hello World” 方法定义方法时也不需要加 public 。 在Groovy中默认的修饰符就是public 。可以看到上面的Hello World程序上 main 方法前也没有Public 字符串字符串的使用也比较简单。可以这样定义：def str = “hello” 循环def var=&quot;hello &quot;+ &quot;world&quot;+ &quot;,groovy!&quot; def repeat(val)&#123; for(i = 0; i &lt; 5; i++)&#123; println val &#125; &#125; repeat(var) 上面这段代码是一个简单的循环。其中我们可以把For循环改得更简洁一些：for(i in 0..4){}效果与原先相同 Groovy 集合 Groovy支持很多集合， 并且都是标准的Java对象，每个集合都是java.util.Collection 或 java.util.Map 的实例。如： def coll &#x3D; [“Groovy”, “Java”, “Ruby”]这很像java中的数组，但实际上在Groovy中它一个Collection。向这个集合中添加元素有三种方式： coll.add(&quot;Python&quot;) coll &lt;&lt; &quot;Smalltalk&quot; coll[5] = &quot;Perl&quot; *. 标记的使用 1234coll = coll*.toUpperCase()println coll\t//输出：[GROOVY, JAVA, RUBY, PYTHON, SMALLTALK, PERL] 使用 *.标记。会对数组中的每一个值都调用toUpperCase()。方便的实现了对数组中的字符串转成大写 映射 Groovy中的映射写起来很简单，感觉有点像json。如下： Def hash = [name:&quot;Andy&quot;, &quot;VPN-#&quot;:45] Groovy 会自动处理成一个Map .上面定义中的键值不一定是String ，像上面的name 会自动转成一个StringGroovy 对 Map 的存取操作可以用 put 与 get但更具Groovy特色的Map 使用方式时直接用 . 号，如： Hash.name //直接取得name 的值 Hash.pwd = “123” //直接向hash 中添加一个键对值 闭包 闭包是用{符号括起来的代码块，它可以被单独运行或调用，也可以被命名。类似‘匿名类’或内联函数的概念。闭包中最常见的应用是对集合进行迭代，下面定义了3个闭包对map进行了迭代： map.each(&#123;key,value-&gt; //key,value两个参数用于接受每个元素的键/值 println &quot;$key:$value&quot;&#125;) map.each&#123;println it&#125; //it是一个关键字，代表map集合的每个元素 map.each(&#123; println it.getKey()+&quot;--&gt;&quot;+it.getValue()&#125;) 除了用于迭代之外，闭包也可以单独定义： def say=&#123;word-&gt; println &quot;Hi,$word!&quot; &#125; 调用： say(&#39;groovy&#39;) say.call(&#39;groovy&amp;grails&#39;) 输出： Hi,groovy! Hi,groovy&amp;grails! ##类定义 Groovy中的类与java中的类一样。但因为Groovy 的特色我们可以少写很多代码。 不需要public 修饰符 前面说到Groovy中默认的就是public 不需要变量类型说明 前面也说到了 不需要getter() 与setter()方法 Groovy默认生成了标准的getter ，setter方法 不需要构造函数 不在需要程序员声明任何构造函数，因为groovy自动提供了足够你使用的构造函数 不需要return 在 Groovy 中可以省略 return 语句。Groovy 默认返回方法的最后一行。 不需要括号 () Groovy中方法调用可以省略()号（构造函数除外） 感觉Groovy 和java差不多，比java写起来轻松一些，对于java开发来说，顺手学一门Groovy是很容易的事，对工作中也有很大的作用。","tags":["groovy"],"categories":["Groovy"]},{"title":"JS中如何用EL表达式","path":"/posts/b9025ac1/","content":"不积跬步，无以至千里。不积小流，无以成江海。 首先，我们有2种情况： 在JSP页面中写JS举例一看就知道： 12345678&lt;script type=&quot;text/javascript&quot;&gt;$(function()&#123;if(&quot;$&#123;msg&#125;&quot;.length&gt;0)&#123;alert(&quot;$&#123;msg&#125;&quot;);&#125;&#125;&lt;/script&gt; 只需要加双引号就行 外部的JS文件引用 因为EL表达式在JSP里才有效。所有可以先在JSP页面中写一个JS定义一个变量按上面的那种方法来接收EL表达式的值。然后在外部的JS中直接用这个变量 就行了","tags":["JSP"],"categories":["Java"]},{"title":"Spring Secutity 自定义权限配置","path":"/posts/197768c1/","content":"Srping Security网上也有很多例子，但基本都是所资源直接配置在XML文件里，限制太大，不够灵活。我们需要的是可以在后台修改资源访问权限，实时生效，才能符合现在大多数系统的需求。 需要引入的依赖123456789101112&lt;!-- Spring security --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-web&lt;/artifactId&gt; &lt;version&gt;4.2.2.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-config&lt;/artifactId&gt; &lt;version&gt;4.2.2.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!--Spring Security end--&gt; 用户身份认证 我们自定义一个实现类MUserDetailsService 来实现UserDetailsService接口。其中需要实现一个loadUserByUsername方法，用来读取用户的角色。在这里需要从数据库中通过用户名来查询用户的信息和用户所属的角色其中MGrantedAuthority实现了GrantedAuthority接口，用于构建用户权限。MUserDeatils实现了UserDeatils接口，用于存放用户信息与权限 UserDetailsService在身份认证中的作用 Spring Security中进行身份验证的是AuthenticationManager接口，ProviderManager是它的一个默认实现，但它并不用来处理身份认证，而是委托给配置好的AuthenticationProvider，每个AuthenticationProvider会轮流检查身份认证。检查后或者返回Authentication对象或者抛出异常。验证身份就是加载响应的UserDetails，看看是否和用户输入的账号、密码、权限等信息匹配。此步骤由实现AuthenticationProvider的DaoAuthenticationProvider（它利用UserDetailsService验证用户名、密码和授权）处理。包含 GrantedAuthority 的 UserDetails对象在构建 Authentication对象时填入数据。 MUserDetailsService.class中的loadUserByUsername方法12345678910111213141516171819202122232425262728293031/** * 根据用户名加载用户密码与权限信息 * @param username * @return * @throws UsernameNotFoundException */@Overridepublic UserDetails loadUserByUsername(String username) throws UsernameNotFoundException &#123; //查询用户信息 User user = userMapper.selectByName(username); List&lt;Role&gt; roleList = null; MUserDeatils userDeatils = null; if (user != null)&#123; //查询用户的角色 roleList = roleMapper.queryByUser(user.getId()); System.out.println(&quot;user&quot; + user.getUsername() + &quot;----&quot; + user.getPassword()); // 构建权限 Set&lt;MGrantedAuthority&gt; authorities = new HashSet&lt;MGrantedAuthority&gt;(); if (roleList.size() != 0)&#123; for (Role role: roleList)&#123; authorities.add(new MGrantedAuthority(role.getName())); System.out.println(role.getName()); &#125; userDeatils = new MUserDeatils(user.getUsername(),user.getPassword(),authorities); &#125; &#125; return userDeatils;&#125; MGrantedAuthority.class12345678910111213public class MGrantedAuthority implements GrantedAuthority &#123; private String authority; public MGrantedAuthority(String authority)&#123; this.authority = authority; &#125; @Override public String getAuthority() &#123; return authority; &#125;&#125; MUserDeatils.class 实现UserDetails接口定义好变量即可 读取资源与所属角色 需要自定义实现类实现FilterInvocationSecurityMetadataSource接口。通过loadResourceDefine方法可以实现资源与权限的对应关系。要使我们自定义的MFilterInvocationSecurityMetadataSource生效，我们还需要定义一个MyFilterSecurityInterceptor类。这里的数据需要从数据库中取得。另外自定义接口UrlMatcher，实现类为AntUrlPathMatcher。 坑 网上有教程是把loadResourceDefine方法放在了构造函数里。但我经过多次试验均出现service,mapper无法注入的问题，然后就会报一个空指针的导异常，经debug发现是service没有注入。经多次查询得知：原因是构造方法会先于注入执行，所以loadResourceDefine方法放入构造中执行时函数内的service与mapper还未执行注入，因此报 java.lang.NullPointerException的异常。解决方法是将loadResourceDefine方法放在getAttributes方法中执行。 MFilterInvocationSecurityMetadataSource.class123456789101112131415161718192021222324252627282930313233343536373839404142434445 @Component public class MFilterInvocationSecurityMetadataSource implements FilterInvocationSecurityMetadataSource &#123; @Autowired public IRescAndRoleService iRescAndRoleService ; @Autowired private IUserService iUserService ; private UrlMatcher urlMatcher = new AntUrlPathMatcher(); // 资源权限集合 private static Map&lt;String, Collection&lt;ConfigAttribute&gt;&gt; resourceMap = null; public void loadResourceDefine()&#123; resourceMap = new HashMap&lt;String, Collection&lt;ConfigAttribute&gt;&gt;(); //取得用户信息 List&lt;User&gt; userList = iUserService.query(); //取得资源与角色列表 List&lt;RescAndRole&gt; resourceList = iRescAndRoleService.query(); System.out.println(resourceList); for (RescAndRole resource : resourceList) &#123; Collection&lt;ConfigAttribute&gt; atts = new ArrayList&lt;ConfigAttribute&gt;(); atts.add(new SecurityConfig(resource.getRoleName() )); resourceMap.put(resource.getResString(), atts); &#125; &#125; @Override public Collection&lt;ConfigAttribute&gt; getAttributes(Object o) throws IllegalArgumentException &#123; loadResourceDefine();//防止无法注入问题 // guess object is a URL. String url = ((FilterInvocation) o).getRequestUrl(); Iterator&lt;String&gt; ite = resourceMap.keySet().iterator(); while (ite.hasNext()) &#123; String resURL = ite.next(); if (urlMatcher.pathMatchesUrl(resURL, url)) &#123; return resourceMap.get(resURL); &#125; &#125; return null; &#125; @Override public Collection&lt;ConfigAttribute&gt; getAllConfigAttributes() &#123; return null; &#125; @Override public boolean supports(Class&lt;?&gt; aClass) &#123; return true; &#125;&#125; AntUrlPathMatcher.class123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class AntUrlPathMatcher implements UrlMatcher &#123; private boolean requiresLowerCaseUrl; private PathMatcher pathMatcher; public AntUrlPathMatcher() &#123; this(true); &#125; public AntUrlPathMatcher(boolean requiresLowerCaseUrl) &#123; this.requiresLowerCaseUrl = true; this.pathMatcher = new AntPathMatcher(); this.requiresLowerCaseUrl = requiresLowerCaseUrl; &#125; public Object compile(String path) &#123; if (this.requiresLowerCaseUrl) &#123; return path.toLowerCase(); &#125; return path; &#125; public void setRequiresLowerCaseUrl(boolean requiresLowerCaseUrl) &#123; this.requiresLowerCaseUrl = requiresLowerCaseUrl; &#125; public boolean pathMatchesUrl(Object path, String url) &#123; if ((&quot;/**&quot;.equals(path)) || (&quot;**&quot;.equals(path))) &#123; return true; &#125; return this.pathMatcher.match((String) path, url); &#125; public String getUniversalMatchPattern() &#123; return &quot;/**&quot;; &#125; public boolean requiresLowerCaseUrl() &#123; return this.requiresLowerCaseUrl; &#125; public String toString() &#123; return super.getClass().getName() + &quot;[requiresLowerCase=&#x27;&quot; + this.requiresLowerCaseUrl + &quot;&#x27;]&quot;; &#125;&#125; MyFilterSecurityInterceptor.class123456789101112131415161718192021222324252627282930313233343536373839404142434445public class MyFilterSecurityInterceptor extends AbstractSecurityInterceptor implements Filter &#123; private FilterInvocationSecurityMetadataSource securityMetadataSource; public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; FilterInvocation fi = new FilterInvocation(request, response, chain); invoke(fi); &#125; public FilterInvocationSecurityMetadataSource getSecurityMetadataSource() &#123; return this.securityMetadataSource; &#125; public Class&lt;? extends Object&gt; getSecureObjectClass() &#123; return FilterInvocation.class; &#125; public void invoke(FilterInvocation fi) throws IOException, ServletException &#123; InterceptorStatusToken token = super.beforeInvocation(fi); try &#123; fi.getChain().doFilter(fi.getRequest(), fi.getResponse()); &#125; finally &#123; super.afterInvocation(token, null); &#125; &#125; public SecurityMetadataSource obtainSecurityMetadataSource() &#123; return this.securityMetadataSource; &#125; public void setSecurityMetadataSource( FilterInvocationSecurityMetadataSource newSource) &#123; this.securityMetadataSource = newSource; &#125; public void destroy() &#123; &#125; public void init(FilterConfig arg0) throws ServletException &#123; &#125;&#125; 决策管理器 自定义一个决策管理器MyAccessDecisionManager实现AccessDecisionManager接口。其中的decide方法，决定某一个用户是否有权限访问某个url 12345678910111213141516171819202122232425262728293031323334353637383940414243/* (non-Javadoc) * @see org.springframework.security.access.AccessDecisionManager#decide(org.springframework.security.core.Authentication, java.lang.Object, java.util.Collection) * 该方法决定该权限是否有权限访问该资源，其实object就是一个资源的地址，authentication是当前用户的 * 对应权限，如果没登陆就为游客，登陆了就是该用户对应的权限 */ @Override public void decide(Authentication authentication, Object object, Collection&lt;ConfigAttribute&gt; configAttributes) throws AccessDeniedException, InsufficientAuthenticationException &#123; if(configAttributes == null) &#123; return; &#125; System.out.println(object.toString()); // object is a URL. //所请求的资源拥有的权限(一个资源对多个权限) Iterator&lt;ConfigAttribute&gt; iterator = configAttributes.iterator(); while(iterator.hasNext()) &#123; ConfigAttribute configAttribute = iterator.next(); //访问所请求资源所需要的权限 String needPermission = configAttribute.getAttribute(); System.out.println(&quot;访问&quot;+object.toString()+&quot;需要的权限是：&quot; + needPermission); //用户所拥有的权限authentication Collection&lt;? extends GrantedAuthority&gt; authorities = authentication.getAuthorities(); for(GrantedAuthority ga : authorities) &#123; if(needPermission.equals(ga.getAuthority())) &#123; return; &#125; &#125; &#125; //没有权限 throw new AccessDeniedException(&quot; 没有权限访问！ &quot;); &#125; @Override public boolean supports(ConfigAttribute attribute) &#123; // TODO Auto-generated method stub return true; &#125; @Override public boolean supports(Class&lt;?&gt; clazz) &#123; // TODO Auto-generated method stub return true; &#125; 配置XMLweb.xml 添加Seucrity的过滤器，将拦截所有资源访问 注意 只能配置成 /* 123456789101112131415161718&lt;!--加载Security配置文件与mybatis配置文件--&gt;&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt; WEB-INF/config/security.xml WEB-INF/config/spring-mybatis.xml &lt;/param-value&gt;&lt;/context-param&gt;&lt;!-- spring security 的过滤器配置 --&gt;&lt;filter&gt; &lt;filter-name&gt;springSecurityFilterChain&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;springSecurityFilterChain&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; spring-security.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465&lt;b:beans xmlns=&quot;http://www.springframework.org/schema/security&quot; xmlns:b=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/security http://www.springframework.org/schema/security/spring-security.xsd&quot;&gt; &lt;!--登陆页面不验证--&gt; &lt;http pattern=&quot;/userLogin.html&quot; security=&quot;none&quot; /&gt; &lt;!--静态文件请求不验证--&gt; &lt;http pattern=&quot;/js/**&quot; security=&quot;none&quot; /&gt; &lt;http pattern=&quot;/css/**&quot; security=&quot;none&quot; /&gt; &lt;!--restful请求--&gt; &lt;http pattern=&quot;/login&quot; security=&quot;none&quot; /&gt; &lt;http pattern=&quot;/getGrid&quot; security=&quot;none&quot; /&gt; &lt;!--浏览器会自动请求网站图标：favicon.ico -不验证 --&gt; &lt;http pattern=&quot;/favicon.ico&quot; security=&quot;none&quot; /&gt; &lt;http &gt; &lt;!--自定义权限不足时显示的页面--&gt; &lt;access-denied-handler error-page=&quot;/accessHint.html&quot;&gt;&lt;/access-denied-handler&gt; &lt;!-- 自定义登录界面 --&gt; &lt;form-login authentication-failure-url=&quot;/userLogin.html?error=true&quot; login-page=&quot;/userLogin.html&quot; default-target-url=&quot;/index.html&quot; login-processing-url=&quot;/j_spring_security_check&quot; /&gt; &lt;logout invalidate-session=&quot;true&quot; logout-success-url=&quot;/userLogin.html&quot; logout-url=&quot;/j_spring_security_logout&quot;/&gt; &lt;!-- 通过配置custom-filter来增加过滤器，before=&quot;FILTER_SECURITY_INTERCEPTOR&quot;表示在SpringSecurity默认的过滤器之前执行。 --&gt; &lt;custom-filter ref=&quot;filterSecurityInterceptor&quot; before=&quot;FILTER_SECURITY_INTERCEPTOR&quot; /&gt; &lt;csrf disabled=&quot;true&quot; /&gt; &lt;/http&gt; &lt;!-- 认证过滤器 --&gt; &lt;b:bean id=&quot;filterSecurityInterceptor&quot; class=&quot;com.hand.security.utils.MyFilterSecurityInterceptor&quot;&gt; &lt;b:property name=&quot;rejectPublicInvocations&quot; value=&quot;true&quot;/&gt; &lt;!-- 用户拥有的权限 --&gt; &lt;b:property name=&quot;accessDecisionManager&quot; ref=&quot;accessDecisionManager&quot; /&gt; &lt;!-- 用户是否拥有所请求资源的权限 --&gt; &lt;b:property name=&quot;authenticationManager&quot; ref=&quot;authenticationManager&quot; /&gt; &lt;!-- 资源与权限对应关系 --&gt; &lt;b:property name=&quot;securityMetadataSource&quot; ref=&quot;securityMetadataSource&quot; /&gt; &lt;/b:bean&gt; &lt;!-- 2、更改验证信息加载方式 --&gt; &lt;authentication-manager alias=&quot;authenticationManager&quot;&gt; &lt;authentication-provider user-service-ref=&quot;mUserDetailsService&quot;&gt; &lt;!--如果用户的密码采用加密的话 &lt;password-encoder hash=&quot;md5&quot; /&gt; --&gt; &lt;/authentication-provider&gt; &lt;/authentication-manager&gt; &lt;!-- 1、配置自定义类MUserDetailsService --&gt; &lt;b:bean id=&quot;mUserDetailsService&quot; class=&quot;com.hand.security.service.impl.MUserDetailsService&quot; /&gt; &lt;!--访问决策器，决定某个用户具有的角色，是否有足够的权限去访问某个资源 --&gt; &lt;b:bean id=&quot;accessDecisionManager&quot; class=&quot;com.hand.security.utils.MyAccessDecisionManager&quot;&gt;&lt;/b:bean&gt; &lt;!--资源源数据定义，将所有的资源和权限对应关系建立起来，即定义某一资源可以被哪些角色访问 --&gt; &lt;b:bean id=&quot;securityMetadataSource&quot; class=&quot;com.hand.security.utils.MFilterInvocationSecurityMetadataSource&quot; &gt;&lt;/b:bean&gt;&lt;/b:beans&gt;","tags":["Spring","Security"],"categories":["Spring"]},{"title":"关于","path":"/about/index.html","content":"HJW Just Code . Hey there! Here are some links you might be interested in: &lt;code&gt;my [&lt;a href=&quot;https://github.com/hjwjw&quot;&gt;GitHub profile &lt;/a&gt;]&lt;/code&gt;"},{"title":"书目","path":"/books/index.html","content":"《Spring Cloud 与 Docker微服务架构实战》 作者：周立 购买时间：2018-05-14 这本书让我入门了Spring Cloud，一步步按书中的步骤自己搭建了一套微服务简单实例。作为一本入门的Spring Cloud书箱是很好的，书本有代码实例，还能通过作者的GitHub获取到书中的源码。我觉得这本书主要是讲解Spring Cloud ，Docker的部分只在书中作了少量介绍，主要是讲解Docker 在微服务架构当时如何使用。因此学完这本书后对微服务有了大概的了解，也能够搭建简单的微服务，但对Docker与微服务的结合还不是很熟悉。 《实战Java高并发程序设计》 作者：葛一鸣 &#x2F; 郭超 购买时间：2018-05-14 这本书买了一年了，我还没有怎么看，惭愧。 受公众号推荐买了这本书，想好好学习并发。但之后一直没怎么看，定了计划，也一直没有执行力。目前只看了前一章的内容还没有。 《深入分析Java Web技术内幕（修订版）》 作者：许令波 购买时间：2018-05-14 这本书把WEB的方方面面都讲解到了，内容我觉得比较杂，但都是工作中可能会使用到的技术。目前也只看了一小部分内容。 《码农翻身》 作者: 刘欣 购买时间：2019-06-22 关注作者的公众号很久了，刚开始是看到他的一篇文章《我的一个线程》，感觉这种方式很有趣，印象很深刻。后面关注公众号后，也经常看，对我等小白理解技术原理有很大的帮助。"},{"title":"friends 友链","path":"/friends/index.html","content":"某某某"}]